##W97-0805
<tag name="DOMAIN" value="start"/>Lexical Discrimination<tag name="DOMAIN" value="end"/> With The Italian Version Of <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>WordNet<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> .
We present a prototype of the Italian version of  <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>WORDNET<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> , a general computational lexical resource .
Some relevant extensions are discussed to make it usable for <tag name="DOMAIN" value="start"/>parsing<tag name="DOMAIN" value="end"/> : in particular we add <tag name="TECHNIQUE" value="start"/>verbal selectional restrictions<tag name="TECHNIQUE" value="end"/> to make <tag name="FOCUS" value="start"/>lexical discrimination<tag name="FOCUS" value="end"/> effective .
Italian <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>WORDNET<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> has been coupled with a <tag name="DOMAIN" value="start"/>parser<tag name="DOMAIN" value="end"/> and a number of experiments have been performed to individuate the methodology with the best trade-off between disambiguation rate and        precision .
Results confirm intuitive hypothesis on the role of <tag name="TECHNIQUE" value="start"/>selectional restrictions<tag name="TECHNIQUE" value="end"/> and show evidences for a WORDNET-Iike organization of lexical senses .


##W98-0301
A <tag name="TECHNIQUE" value="start"/>Surface-Based<tag name="TECHNIQUE" value="end"/> Approach To Identifying <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Discourse Markers And Elementary Textual Units<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> In Unrestricted Texts .
I present a <tag name="TECHNIQUE" value="start"/>surface-based<tag name="TECHNIQUE" value="end"/>  algorithm that employs knowledge of <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>cue phrase<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> usages in order to determine automatically <tag name="DOMAIN" value="start"/>clause boundaries and discourse markers<tag name="DOMAIN" value="end"/> in unrestricted natural language texts .
The knowledge was derived from a comprehensive corpus analysis .


##C04-1103
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Direct Orthographical Mapping For Machine Transliteration<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Machine transliteration\/back-transliteration<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> plays an important role in many multilingual speech and language applications .
In this paper , a novel framework for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>machine transliteration\/backtransliteration<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> that allows us to carry out direct <tag name="DOMAIN" value="start"/>orthographical mapping<tag name="DOMAIN" value="end"/> -LRB- DOM -RRB- between two different languages is presented .
Under this framework , a <tag name="TECHNIQUE" value="start"/>joint source-channel<tag name="TECHNIQUE" value="end"/> transliteration model , also called <tag name="TECHNIQUE" value="start"/>n-gram<tag name="TECHNIQUE" value="end"/> transliteration model -LRB- <tag name="TECHNIQUE" value="start"/>ngram<tag name="TECHNIQUE" value="end"/> TM -RRB- , is further proposed to model the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>transliteration<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/>       process .
We evaluate the proposed methods through several <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>transliteration\/backtransliteration<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> experiments for English\/Chinese and English\/Japanese language pairs .
Our study reveals that the proposed method not only reduces an extensive system development effort but also improves the <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>transliteration<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> accuracy significantly .


##W02-1032
Exploiting <tag name="TECHNIQUE" value="start"/>Headword Dependency<tag name="TECHNIQUE" value="end"/> And <tag name="TECHNIQUE" value="start"/>Predictive Clustering<tag name="TECHNIQUE" value="end"/> For <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/><tag name="FOCUS" value="start"/>Language Modeling<tag name="FOCUS" value="end"/><tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
This paper presents several practical ways of incorporating <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>linguistic structure<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> into <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>  language models<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
A <tag name="TECHNIQUE" value="start"/>headword detector<tag name="TECHNIQUE" value="end"/> is first applied to detect the headword of each phrase in a sentence .
A <tag name="TECHNIQUE" value="start"/>permuted headword trigram <tag name="TECHNIQUE" value="end"/> model -LRB- <tag name="TECHNIQUE" value="start"/>PHTM<tag name="TECHNIQUE" value="end"/> -RRB- is then generated from the annotated corpus .
Finally , <tag name="TECHNIQUE" value="start"/>PHTM<tag name="TECHNIQUE" value="end"/> is extended to a <tag name="TECHNIQUE" value="start"/>cluster PHTM <tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/>C-PHTM<tag name="TECHNIQUE" value="end"/> -RRB-  by defining clusters for similar words in the corpus .
We evaluated the proposed models on the realistic application of <tag name="DOMAIN" value="start"/>Japanese Kana-Kanji conversion<tag name="DOMAIN" value="end"/> .
Experiments show that <tag name="TECHNIQUE" value="start"/>C-PHTM<tag name="TECHNIQUE" value="end"/> achieves 15 % error rate reduction over the word trigram model .
This demonstrates that the use of simple methods such as the <tag name="TECHNIQUE" value="start"/>headword trigram<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>predictive clustering<tag name="TECHNIQUE" value="end"/> can effectively capture long distance word dependency , and substantially outperform a word trigram model .


##W01-1411
Towards A Simple And Accurate <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Statistical <tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> Approach To Learning  <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Translation Relationships Among Words<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We report on a project to derive <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>word translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/>  relationships automatically from parallel corpora .
Our effort is distinguished by the use of simpler , faster models than those used in previous high-accuracy approaches .
Our methods achieve accuracy on <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>singleword translations<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> that seems comparable to any work previously reported , up to nearly 60 % coverage of word types , and they perform particularly well on a class of multi-word compounds of special interest to our translation effort .


##N06-1053
Towards <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Spoken-Document Retrieval<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> For The Internet : <tag name="TECHNIQUE" value="start"/>Lattice Indexing<tag name="TECHNIQUE" value="end"/> For <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Large-Scale Web-Search Architectures<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Large-scale web-search engines<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> are generally designed for linear text .
The linear text representation is suboptimal for audio search , where accuracy can be significantly improved if the search includes alternate recognition candidates , commonly represented as word lattices .
This paper proposes a method for <tag name="TECHNIQUE" value="start"/>indexing word lattices<tag name="TECHNIQUE" value="end"/> that is suitable for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>large-scale web-search engines<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , requiring only limited code changes .
The proposed method , called <tag name="TECHNIQUE" value="start"/>Time-based Merging for Indexing <tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/>TMI<tag name="TECHNIQUE" value="end"/> -RRB- , first converts the word lattice to a posterior-probability representation and then merges word hypotheses with similar time boundaries to reduce the index size .
Four alternative approximations are presented , which differ in index size and the strictness of the phrase-matching constraints .
Results are presented for three types of typical web audio content , podcasts , video clips , and online lectures , for <tag name="DOMAIN" value="start"/>phrase spotting and relevance ranking<tag name="DOMAIN" value="end"/> .
Using <tag name="TECHNIQUE" value="start"/>TMI indexes<tag name="TECHNIQUE" value="end"/> that are only five times larger than corresponding lineartext indexes , <tag name="DOMAIN" value="start"/>phrase spotting<tag name="DOMAIN" value="end"/> was improved over searching top-1 transcripts by 25-35 % , and relevance ranking by 14 % , at only a small loss compared to unindexed lattice search .


##D08-1055
A Japanese  <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Predicate Argument Structure Analysis<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> using <tag name="TECHNIQUE" value="start"/>Decision Lists<tag name="TECHNIQUE" value="end"/> .
This paper describes a new automatic method for Japanese <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>predicate argument structure analysis<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
The method learns relevant features to assign <tag name="DOMAIN" value="start"/>case roles to the argument of the target predicate<tag name="DOMAIN" value="end"/> using the features of the words located closest to the target predicate under various constraints such as <tag name="TECHNIQUE" value="start"/>dependency types , words , semantic categories , parts of speech , functional words and predicate voices<tag name="TECHNIQUE" value="end"/> .
We constructed <tag name="TECHNIQUE" value="start"/>decision lists<tag name="TECHNIQUE" value="end"/> in which these featuresweresortedbytheirlearnedweights .
Using our method , we integrated the tasks of <tag name="DOMAIN" value="start"/>semantic role labeling<tag name="DOMAIN" value="end"/> and <tag name="DOMAIN" value="start"/>zero-pronoun identification<tag name="DOMAIN" value="end"/> , and achieved a 17 % improvement compared with a baseline method in a sentence level performance analysis .


##W03-1002
<tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Statistical <tag name="TECHNIQUE" value="end"/><tag name="DOMAIN" value="start"/>Machine Translation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> Using <tag name="TECHNIQUE" value="start"/>Coercive Two-Level Syntactic Transduction<tag name="TECHNIQUE" value="end"/> .
We define , implement and evaluate a novel model for <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>statistical <tag name="TECHNIQUE" value="end"/><tag name="DOMAIN" value="start"/> machine translation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> , which is based on <tag name="TECHNIQUE" value="start"/>shallow syntactic analysis -LRB- part-of-speech tagging and phrase chunking -RRB-<tag name="TECHNIQUE" value="end"/> in both the source and target languages .
It is able to model long-distance constituent motion and other syntactic phenomena without requiring a full parse in either language .
We also examine aspects of <tag name="TECHNIQUE" value="start"/>lexical transfer<tag name="TECHNIQUE" value="end"/> , suggesting and exploring a concept of translation coercion across <tag name="TECHNIQUE" value="start"/>parts of speech<tag name="TECHNIQUE" value="end"/> , as well as a transfer model based on <tag name="TECHNIQUE" value="start"/>lemma-to-lemma<tag name="TECHNIQUE" value="end"/> translation probabilities , which holds promise for improving <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>machine translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> of low-density languages .
Experiments are performed in both Arabic-to-English and French-to-English translation demonstrating the efficacy of the proposed techniques .
Performance is automatically evaluated via the Bleu score metric .


##N06-1034
Modelling <tag name="FOCUS" value="start"/>User Satisfaction<tag name="FOCUS" value="end"/> And <tag name="FOCUS" value="start"/>Student Learning<tag name="FOCUS" value="end"/> In A <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Spoken Dialogue Tutoring<tag name="FOCUS" value="end"/> <tag name="DOMAIN" value="end"/> System With Generic , Tutoring , And <tag name="TECHNIQUE" value="start"/>User Affect<tag name="TECHNIQUE" value="end"/> Parameters .
We investigate using the <tag name="FOCUS" value="start"/>PARADISE<tag name="FOCUS" value="end"/> framework to develop <tag name="TECHNIQUE" value="start"/>predictive<tag name="TECHNIQUE" value="end"/> models of system performance in our <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>spoken dialogue tutoring <tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> system .
We represent performance with two metrics : <tag name="FOCUS" value="start"/>user satisfaction<tag name="FOCUS" value="end"/> and <tag name="FOCUS" value="start"/>student learning<tag name="FOCUS" value="end"/> .
We train and test <tag name="TECHNIQUE" value="start"/>predictive<tag name="TECHNIQUE" value="end"/> models of these metrics in our tutoring system corpora .
We predict <tag name="FOCUS" value="start"/>user satisfaction<tag name="FOCUS" value="end"/> with 2 parameter types : 1 -RRB- system-generic , and 2 -RRB- tutoringspeci c. To predict <tag name="FOCUS" value="start"/>student learning<tag name="FOCUS" value="end"/> , we also use a third type : 3 -RRB-<tag name="TECHNIQUE" value="start"/> user affect<tag name="TECHNIQUE" value="end"/> .
Alhough generic parameters are useful predictors of <tag name="FOCUS" value="start"/>user satisfaction<tag name="FOCUS" value="end"/> in other PARADISE applications , overall our parameters produce less useful user satisfaction models in our system .
However , generic and tutoring-speci c parameters do produce useful models of <tag name="FOCUS" value="start"/>student learning<tag name="FOCUS" value="end"/> in our system .
<tag name="TECHNIQUE" value="start"/>User affect<tag name="TECHNIQUE" value="end"/> parameters can increase the usefulness of these models .


##W98-1428
<tag name="FOCUS" value="start"/>EXEMPLARS<tag name="FOCUS" value="end"/> : A Practical , Extensible Framework For <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Dynamic Text Generation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
In this paper , we present <tag name="FOCUS" value="start"/>EXEMPLARS<tag name="FOCUS" value="end"/> , an <tag name="TECHNIQUE" value="start"/>object-oriented , rule-based <tag name="TECHNIQUE" value="end"/> framework designed to support practical , <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>dynamic text generation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , emphasizing its novel features compared to .
existing hybrid systems that mix template-style and more sophisticated techniques .
These features - .
include an extensible <tag name="TECHNIQUE" value="start"/>classification-based text planning<tag name="TECHNIQUE" value="end"/>  mechanism , a definition language that is a superset of the Java language , and advanced support for <tag name="TECHNIQUE" value="start"/>HTMIdSGML templates<tag name="TECHNIQUE" value="end"/> .


##I08-1049
<tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Multi-View Co-Training<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Transliteration<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Model .
This paper discusses a new approach to training of <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>transliteration<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> model from unlabeled data for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>transliteration<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> extraction .
We start with an inquiry into the formulation of <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>transliteration<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> model by considering different <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>transliteration<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> strategies as a <tag name="TECHNIQUE" value="start"/>multi-view<tag name="TECHNIQUE" value="end"/> problem , where each view exploits a natural division of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>transliteration<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> features , such as <tag name="TECHNIQUE" value="start"/>phonemebased<tag name="TECHNIQUE" value="end"/> , <tag name="TECHNIQUE" value="start"/>grapheme-based<tag name="TECHNIQUE" value="end"/> or <tag name="TECHNIQUE" value="start"/>hybrid<tag name="TECHNIQUE" value="end"/> features .
Then we introduce a <tag name="TECHNIQUE" value="start"/>multi-view Cotraining<tag name="TECHNIQUE" value="end"/> algorithm , which leverages compatible and partially uncorrelated information across different views to effectively boost the model from unlabeled data .
Applying this algorithm to <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>transliteration<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> extraction , the results show that it not only circumvents the need of data labeling , but also achieves performance close to that of supervised learning , where manual labeling is required for all training samples .


##W01-0904
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Translating Treebank Annotation For Evaluation <tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
In this paper we discuss the need for corpora with a variety of annotations to provide suitable resources to <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>evaluate<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> different Natural Language Processing systems and to compare them .
A <tag name="TECHNIQUE" value="start"/>supervised machine learning <tag name="TECHNIQUE" value="end"/> technique is presented for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>translating corpora between syntactic formalisms<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> and is applied to the task of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>translating the Penn Treebank annotation into a Categorial Grammar annotation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
It is compared with a current alternative approach and results indicate annotation of broader coverage using a more compact grammar .


##J00-3004
A <tag name="TECHNIQUE" value="start"/>Compression-Based<tag name="TECHNIQUE" value="end"/> Algorithm For Chinese <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Word Segmentation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
Chinese is written without using spaces or other word delimiters .
Although a text may be thought of as a corresponding sequence of words , there is considerable ambiguity in the placement of boundaries .
Interpreting a text as a sequence of words is beneficial for some information retrieval and storage tasks : for example,full-text search , word-based compression , and keyphrase extraction .
We describe a scheme that infers appropriate positions for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>word boundaries<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> using an <tag name="TECHNIQUE" value="start"/>adaptive language model<tag name="TECHNIQUE" value="end"/> that is standard in text compression .
It is trained on a corpus of presegmented text , and when applied to new text , interpolates <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>word boundaries<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> so as to maximize the compression obtained .
This simple and general method performs well with respect to specialized schemes for Chinese  <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>language segmentation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .


##W09-2506
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Ranking Paraphrases<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> in <tag name="TECHNIQUE" value="start"/>Context<tag name="TECHNIQUE" value="end"/> .
We present a <tag name="TECHNIQUE" value="start"/>vector space model<tag name="TECHNIQUE" value="end"/> that supports the computation of appropriate <tag name="TECHNIQUE" value="start"/> vector representations<tag name="TECHNIQUE" value="end"/> for words in <tag name="TECHNIQUE" value="start"/>context<tag name="TECHNIQUE" value="end"/> , and apply it to a <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>paraphrase ranking<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> task .
An evaluation on the SemEval 2007 <tag name="DOMAIN" value="start"/>lexical substitution<tag name="DOMAIN" value="end"/> task data shows promising results : the model significantly outperforms a current state of the art model , and our treatment of context is effective .


##E87-1011
A Multi-Purpose Interface To An <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>On-Line Dictionary<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
We argue that there are two qualitatively different modes of using a <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>machine-readable dictionary<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> in the context of research in computational linguistics : batch processing of the source with the purpose of collating information for subsequent use by a natural language application , and placing the dictionary on-line in an environment which supports fast interactive access to data selected on the basis of a number of <tag name="TECHNIQUE" value="start"/>linguistic constraints<tag name="TECHNIQUE" value="end"/> .
While it is the former mode of dictionary use which is characteristic of most computational linguistics work to date , it is the latter which has the potential of making maximal use of the information typically found in a machine-readable dictionary .
We describe the mounting of the machine-readable source of the  <tag name="FOCUS" value="start"/>Longman Dictionary of Contemporary English<tag name="FOCUS" value="end"/> on a single user workstation to make it available as a development tool for a number of research projects .


##P00-1020
An Empirical Study Of The Influence Of <tag name="TECHNIQUE" value="start"/>Argument Conciseness<tag name="TECHNIQUE" value="end"/> On <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Argument Effectiveness<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We have developed a system that <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>generates evaluative arguments<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> that are tailored to the user , properly arranged and concise .
We have also developed an evaluation framework in which the <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>effectiveness of evaluative arguments<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> can be measured with real users .
This paper presents the results of a formal experiment we have performed in our framework to verify the influence of <tag name="TECHNIQUE" value="start"/>argument conciseness<tag name="TECHNIQUE" value="end"/> on <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>argument effectiveness<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/>


##H05-1095
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Translating<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> With <tag name="TECHNIQUE" value="start"/>Non-Contiguous Phrases<tag name="TECHNIQUE" value="end"/> .
This paper presents a <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>phrase-based statistical<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/> machine translation <tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> method , based on <tag name="TECHNIQUE" value="start"/>non-contiguous phrases<tag name="TECHNIQUE" value="end"/> , i.e. phrases with gaps .
A method for producing such phrases from a word-aligned corpora is proposed .
A <tag name="FOCUS" value="start"/>statistical  <tag name="DOMAIN" value="start"/> translation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> model is also presented that deals such phrases , as well as a <tag name="TECHNIQUE" value="start"/>training<tag name="TECHNIQUE" value="end"/> method based on the maximization of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> accuracy , as measured with the NIST evaluation metric .
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Translations<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> are produced by means of a <tag name="TECHNIQUE" value="start"/>beam-search decoder<tag name="TECHNIQUE" value="end"/> .
Experimental results are presented , that demonstrate how the proposed method allows to better generalize from the training data .


##P05-1039
What To Do When Lexicalization Fails : <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> German With <tag name="TECHNIQUE" value="start"/>Suffix Analysis And Smoothing<tag name="TECHNIQUE" value="end"/> .
In this paper , we present an <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>unlexicalized <tag name="TECHNIQUE" value="end"/><tag name="DOMAIN" value="start"/>parser<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> for German which employs <tag name="TECHNIQUE" value="start"/>smoothing and suffix analysis<tag name="TECHNIQUE" value="end"/> to achieve a labeled bracket F-score of 76.2 , higher than previously reported results on the NEGRA corpus .
In addition to the high accuracy of the model , the use of <tag name="TECHNIQUE" value="start"/>smoothing<tag name="TECHNIQUE" value="end"/> in an <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>unlexicalized<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>parser<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> allows us to better examine the interplay between <tag name="TECHNIQUE" value="start"/>smoothing<tag name="TECHNIQUE" value="end"/> and <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> results .


##A97-1022
A Prototype Of A <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Grammar Checker<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> For Czech .
This paper describes the implementation of a prototype of a <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>grammar based grammar checker<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> for Czech and the basic ideas behind this implementation .
The demo is implemented as an independent program cooperating with Microsoft Word .
The <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>grammar checker<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> uses <tag name="TECHNIQUE" value="start"/>specialized grammar formalism<tag name="TECHNIQUE" value="end"/> which generally enables to check errors in languages with a very high degree of word order freedom .


##C02-2009
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Machine Translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Based On <tag name="TECHNIQUE" value="start"/>NLG From XML-DB<tag name="TECHNIQUE" value="end"/> .
The purpose of this study is to propose a new method for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>machine translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
Wehave proceeded through with two projects for report generation -LRB- Kittredge and Polguere , 2000 -RRB- : Weather Forecast and Monthly Economic Report to be produced in four languages : English , Japanese , French , and German .
Their input data is stored in XML-DB .
We applied a three-stage pipelined architecture -LRB- Reiter and Dale , 2000 -RRB- , and each stage was implemented as <tag name="TECHNIQUE" value="start"/>XML transformation processes<tag name="TECHNIQUE" value="end"/> .
Weregard XML stored data as language-neutral intermediate form and employ the so-called ` <tag name="TECHNIQUE" value="start"/>sublanguage<tag name="TECHNIQUE" value="end"/>  approach  ' -LRB- Somers , 2000 -RRB- .
The <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>machine translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> process is implemented via <tag name="TECHNIQUE" value="start"/>XMLDB<tag name="TECHNIQUE" value="end"/> as a kind of interlingua approach instead of the conventional structure transfer approach .


##N09-2036
Faster <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>MT Decoding<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> Through <tag name="TECHNIQUE" value="start"/>Pervasive Laziness<tag name="TECHNIQUE" value="end"/> .
<tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Syntax-based<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/> MT<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> systems have proven effective -- the models are compelling and show good room for improvement .
However , decoding involves a slow search .
We present a new <tag name="TECHNIQUE" value="start"/>lazy-search <tag name="TECHNIQUE" value="end"/> method that obtains significant speedups over a strong baseline , with no loss in Bleu .


##J07-1005
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Answering Clinical Questions<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> with <tag name="TECHNIQUE" value="start"/>Knowledge-Based and Statistical <tag name="TECHNIQUE" value="end"/> Techniques .
The combination of recent developments in <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>question-answering<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> research and the availability of unparalleled resources developed specifically for automatic semantic processing of text in the medical domain provides a unique opportunity to explore complex question answering in the domain of clinical medicine .
This article presents a system designed to satisfy the information needs of physicians practicing <tag name="DOMAIN" value="start"/>evidence-based medicine<tag name="DOMAIN" value="end"/> .
We have developed a series of <tag name="TECHNIQUE" value="start"/>knowledge extractors<tag name="TECHNIQUE" value="end"/> , which employ a combination of <tag name="TECHNIQUE" value="start"/>knowledge-based and statistical <tag name="TECHNIQUE" value="end"/>  techniques , for automatically identifying <tag name="DOMAIN" value="start"/> clinically relevant aspects of MEDLINE abstracts<tag name="DOMAIN" value="end"/> .
These extracted elements serve as the input to an algorithm that scores the relevance of citations with respect to structured representations of information needs , in accordance with the principles of <tag name="DOMAIN" value="start"/>evidencebased medicine<tag name="DOMAIN" value="end"/> .
Starting with an initial list of citations retrieved by PubMed , our system can bring relevant abstracts into higher ranking positions , and from these abstracts generate responses that directly answer physicians ' questions .
We describe three separate evaluations : one focused on the accuracy of the knowledge extractors , one conceptualized as a document reranking task , and finally , an evaluation of answers by two physicians .
Experiments on a collection of real-world clinical questions show that our approach significantly outperforms the already competitive PubMed baseline .


##C08-1046
Japanese <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Dependency Parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Using a <tag name="TECHNIQUE" value="start"/>Tournament<tag name="TECHNIQUE" value="end"/> Model .
In Japanese <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>dependency parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , Kudo 's relative preference-based method -LRB- Kudo and Matsumoto , 2005 -RRB- outperforms both deterministic and probabilistic CKY-based parsing methods .
In Kudo 's method , for each dependent word -LRB- or chunk -RRB- a loglinear model estimates relative preference of all other candidate words -LRB- or chunks -RRB- for being as its head .
This can not be considered in the deterministic parsing methods .
We propose an algorithm based on a <tag name="TECHNIQUE" value="start"/>tournament <tag name="TECHNIQUE" value="end"/> model , in which the relative preferences are directly modeled by <tag name="TECHNIQUE" value="start"/>one-onone games in a step-ladder tournament<tag name="TECHNIQUE" value="end"/> .
In an evaluation experiment with Kyoto Text Corpus Version 4.0 , the proposed method outperforms previous approaches , including the relative preference-based method .


##W98-0303
Enriching Automated <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/> Essay Scoring<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Using <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Discourse Marking<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> .
<tag name="FOCUS" value="start"/>Electronic Essay Rater -LRB- e-rater -RRB-<tag name="FOCUS" value="end"/> is a prototype automated <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/> essay scoring<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> system built at Educational Testing Service -LRB- ETS -RRB- that uses <tag name="TECHNIQUE" value="start"/>discourse marking<tag name="TECHNIQUE" value="end"/> , in addition to <tag name="TECHNIQUE" value="start"/>syntactic information and topical content vector<tag name="TECHNIQUE" value="end"/> analyses to automatically assign <tag name="DOMAIN" value="start"/>essay scores<tag name="DOMAIN" value="end"/> .
This paper gives a general description ore-rater as a whole , but its emphasis is on the importance of<tag name="TECHNIQUE" value="start"/> <tag name="FOCUS" value="start"/>discourse marking and argument partitioning<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> for annotating the argument structure of an essay .
We show comparisons between two <tag name="TECHNIQUE" value="start"/>content vector analysis<tag name="TECHNIQUE" value="end"/> programs used to <tag name="DOMAIN" value="start"/>predict scores<tag name="DOMAIN" value="end"/> .
EsscQ \/ ` Content and ArgContent .
EsscnContent assigns scores to essays by using a standard cosine correlation that treats the essay like a '' ` bag of words . ''
in that it does not consider word order .
<tag name="TECHNIQUE" value="start"/>Ark , Content<tag name="TECHNIQUE" value="end"/> employs a novel <tag name="TECHNIQUE" value="start"/>content vector analysis<tag name="TECHNIQUE" value="end"/> approach for score assignment based on the individual arguments in an essay .
The average agreement between ArgContent scores and human rater scores is 82 % .
as compared to 69 % agreement between EssavContent and the human raters .
These results suggest that discourse marking enriches <tag name="FOCUS" value="start"/>e-rater<tag name="FOCUS" value="end"/> 's scoring capability .
When <tag name="FOCUS" value="start"/>e-rater<tag name="FOCUS" value="end"/> uses its whole set of predictive features , agreement with human rater scores ranges from 87 Â° , \/ o - 94 % across the 15 sets of essa5 responses used in this study


##W04-2213
Building Parallel Corpora For <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>EContent Professionals<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
This paper reports on completed work carried out in the framework of the <tag name="FOCUS" value="start"/>INTERA<tag name="FOCUS" value="end"/> project , and specifically , on the production of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>multilingual resources<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> -LRB- <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>LRs<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> -RRB- for <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>eContent<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> purposes .
The paper presents the methodology adopted for the development of the corpus -LRB- acquisition and processing of the textual data -RRB- , discusses the divergence of the initial assumptions from the actual situation met during this procedure , and concludes with a summarization of the problems attested which undermine the viability of <tag name="FOCUS" value="start"/>multilingual parallel corpora<tag name="FOCUS" value="end"/> construction .


##W09-2603
Mining of <tag name="TECHNIQUE" value="start"/>Parsed<tag name="TECHNIQUE" value="end"/> Data to Derive <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/> Deverbal Argument Structure<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
The availability of large parsed corpora and improved computing resources now make it possible to extract vast amounts of lexical data .
We describe the process of extracting structured data and several methods of deriving <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/> argument structure mappings for deverbal nouns<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> that significantly improves upon non-lexicalized rule-based methods .
For a typical model , the F-measure of performance improves from a baseline of about 0.72 to 0.81 .


##C04-1069
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Document Re-Ranking<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Based On Automatically Acquired <tag name="TECHNIQUE" value="start"/> Key Terms<tag name="TECHNIQUE" value="end"/> In Chinese  <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Information Retrieval<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
For <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Information Retrieval<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , users are more concerned about the precision of top ranking documents in most practical situations .
In this paper , we propose a method to improve the precision of top N <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/> ranking documents<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> by reordering the retrieved documents from the initial retrieval .
To reorder documents , we first automatically extract <tag name="TECHNIQUE" value="start"/>Global Key Terms<tag name="TECHNIQUE" value="end"/> from document set , then use extracted <tag name="TECHNIQUE" value="start"/>Global Key Terms<tag name="TECHNIQUE" value="end"/> to identify <tag name="TECHNIQUE" value="start"/>Local Key Terms<tag name="TECHNIQUE" value="end"/> in a single document or query topic , finally we make use of <tag name="TECHNIQUE" value="start"/>Local Key Terms<tag name="TECHNIQUE" value="end"/> in query and documents to reorder the initial ranking documents .
The experiment with NTCIR3 CLIR dataset shows that an average 10 % -11 % improvement and 2 % -5 % improvement in precision can be achieved at top 10 and 100 ranking documents level respectively .


##H92-1046
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Lexical Disambiguation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Using <tag name="TECHNIQUE" value="start"/>Simulated Annealing<tag name="TECHNIQUE" value="end"/> .
The <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>resolution of lexical ambiguity<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> is important for most natural language processing tasks , and a range of computational techniques have been proposed for its solution .
None of these has yet proven effective on a large scale .
In this paper , we describe a method for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>lexical disambiguation of text<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> using the definitions in a <tag name="TECHNIQUE" value="start"/>machine-readable dictionary<tag name="TECHNIQUE" value="end"/> together with the technique of <tag name="TECHNIQUE" value="start"/>simulated annealing<tag name="TECHNIQUE" value="end"/> .
The method operates on complete sentences and attempts to select the optimal combinations of word senses for all the words in the sentence simultaneously .
The words in the sentences may be any of the 28,000 headwords in Longman 's Dictionary of Contemporary English -LRB- LDOCE -RRB- and are disambiguated relative to the senses given in LDOCE .
Our initial results on a sample set of 50 sentences are comparable to those of other researchers , and the fully automatic method requires no hand coding of lexical entries , or hand tagging of text .


##D08-1082
A <tag name="TECHNIQUE" value="start"/>Generative <tag name="TECHNIQUE" value="end"/> Model for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Parsing Natural Language to Meaning Representations<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
In this paper , we present an algorithm for learning a <tag name="TECHNIQUE" value="start"/>generative<tag name="TECHNIQUE" value="end"/> model of natural language sentences together with their <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>formal meaning representations with hierarchical structures<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
The model is applied to the task of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>mapping sentences to hierarchical representations<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> of their underlying meaning .
We introduce <tag name="TECHNIQUE" value="start"/>dynamic programming<tag name="TECHNIQUE" value="end"/> techniques for efficient training and decoding .
In experiments , we demonstrate that the model , when coupled with a <tag name="TECHNIQUE" value="start"/>discriminative reranking<tag name="TECHNIQUE" value="end"/> technique , achieves state-of-the-art performance when tested on two publicly available corpora .
The generative model degrades robustly when presented with instances that are different from those seen in training .
This allows a notable improvement in recall compared to previous models .


##C90-2064
The Application Of <tag name="TECHNIQUE" value="start"/>Two-Level<tag name="TECHNIQUE" value="end"/> Morphology To <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Non-Concatenative German Morphology<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
Introduction In this paper 2 we describe a <tag name="TECHNIQUE" value="start"/>hybrid <tag name="TECHNIQUE" value="end"/> system for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>morphological analysis and synthesis<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We call it hybrid because it consists of two separate parts interacting with each other in a welldefined way .
The treatment of <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>morphonology and nonoconcatenative morphology<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> is based on the <tag name="TECHNIQUE" value="start"/>two-level<tag name="TECHNIQUE" value="end"/> approach originally proposed by Koskenniemi -LRB- 1983 -RRB- .
For the concatenative part of morphosyntax -LRB- i.e. affixation -RRB- we make use of a <tag name="TECHNIQUE" value="start"/>grammar based on feature-unification<tag name="TECHNIQUE" value="end"/> .
tloth parts rely on the same morph lexicon .
Combinations of <tag name="TECHNIQUE" value="start"/>two-level<tag name="TECHNIQUE" value="end"/> morphology with t ` eature-based morphosyntactic grammars have already been proposed by several authors -LRB- c.f. llear 1988a , Carson 1988 , G6rz & Paulus 1988 , Schiller & Steffens 1990 -RRB- to overcome the shortcomings of the continuation-classes originally proposed by Koskenniemi -LRB- 1983 -RRB- and Karttunen -LRB- 1983 -RRB- for the description of morphosyntax .
But up to now no linguistically ~ ; atisfying solution has been proposed for the treatment of non-concatenative morphology in : such a framework .
In this paper we describe an extension to the model which will allow for the description of such phenomena .
Namely we propose to restrict the applicability of two-level rules by providing them with filters in the form of feature structures .
We demonstrate how a well-known problem of German <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>morphology<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , so-called `` <tag name="DOMAIN" value="start"/>Umlautung<tag name="DOMAIN" value="end"/> '' , can be described in our approach in a linguistically motivated and efficient way .


##W98-0904
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Optimal Morphology<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
<tag name="DOMAIN" value="start"/>Optimal morphology <tag name="DOMAIN" value="end"/> -LRB- OM -RRB- is a <tag name="TECHNIQUE" value="start"/>finite state<tag name="TECHNIQUE" value="end"/> formalism that unifies concepts from Optimality Theory -LRB- OT , Prince ~ : Smolensky , 1993 -RRB- and Declarative Phonology -LRB- DP , Scobbie , Coleman Bird , 1996 -RRB- to describe <tag name="DOMAIN" value="start"/>morphophonological alternations in inflectional morphology<tag name="DOMAIN" value="end"/> .
Candidate sets are formalized by <tag name="TECHNIQUE" value="start"/>inviolable lexical constraints<tag name="TECHNIQUE" value="end"/> which map abstract morpheme signatures to allomorphs .
Phonology is implemented as <tag name="TECHNIQUE" value="start"/>violable rankable constraints<tag name="TECHNIQUE" value="end"/> selecting optimal candidates from these .
Both types of constraints are realized by <tag name="TECHNIQUE" value="start"/>finite state transducers<tag name="TECHNIQUE" value="end"/> .
Using phonological data from Albanian it is shown that given a finite state lexicalization of candidate outputs for word forms OM allows more natural analyses than unviolable finite state constraints do .
Two possible evaluation strategies for OM grammars are considered : the global evaluation procedure from E1lisou -LRB- 1994 -RRB- and a simple strategy of local constraint evaluation .
While the OM-specific lexicalization of candidate sets allows straightforward generation and a simple method of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>morphological parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> even under global evaluation , local constraint evaluation is shown to be preferable empirically and to be formally more restrictive .
The first point is illustrated by an account of directionality effects in some classical Mende data .
A procedure is given that generates a finite state transducer simulating the effects of local constraint evaluation .
Thus local as opposed to global evaluation -LRB- Frank & Satta , 1998 -RRB- seems to guarantee the finite-stateness of the input-output-mapping .


##W07-0723
Getting to Know Moses : Initial Experiments on German-English <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Factored <tag name="TECHNIQUE" value="end"/><tag name="DOMAIN" value="start"/>Translation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
We present results and experiences from our experiments with <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>phrase-based statistical<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/> machine translation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> using <tag name="TECHNIQUE" value="start"/>Moses<tag name="TECHNIQUE" value="end"/> .
The paper is based on the idea of using an offthe-shelf <tag name="TECHNIQUE" value="start"/>parser<tag name="TECHNIQUE" value="end"/> to supply linguistic information to a <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/> factored<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>translation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> model and compare the results of German ?
English translation to the shared task baseline system based on word form .
We report partial results for this model and results for two simplified setups .
Our best setup takes advantage of the parser ?
s <tag name="TECHNIQUE" value="start"/>lemmatization and decompounding<tag name="TECHNIQUE" value="end"/> .
A qualitative analysis of compound translation shows that decompounding improves <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>translation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> quality .


##W07-2044
<tag name="TECHNIQUE" value="start"/>KU<tag name="TECHNIQUE" value="end"/> : <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Word Sense Disambiguation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> by <tag name="TECHNIQUE" value="start"/>Substitution<tag name="TECHNIQUE" value="end"/> .
Data sparsity is one of the main factors that make <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>word sense disambiguation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> -LRB- <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>WSD<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> -RRB- difficult .
To overcome this problem we need to find effective ways to use resources other than sense labeled data .
In this paper I describe a <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>WSD<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> system that uses a <tag name="TECHNIQUE" value="start"/>statistical language model <tag name="TECHNIQUE" value="end"/> based on a large unannotated corpus .
The model is used to evaluate the likelihood of various substitutes for a word in a given context .
These likelihoods are then used to determine the best sense for the word in novel contexts .
The resulting system participated in three tasks in the SemEval 2007 workshop .
The WSD of prepositions task proved to be challenging for the system , possibly illustrating some of its limitations : e.g. not all words have good substitutes .
The system achieved promising results for the English lexical sample and English lexical substitution tasks .


##W04-2003
A Robust And <tag name="TECHNIQUE" value="start"/>Hybrid Deep-Linguistic Theory<tag name="TECHNIQUE" value="end"/> Applied To <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Large-Scale Parsing<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
Modern statistical parsers are robust and quite fast , but their output is relatively shallow when compared to formal grammar parsers .
We suggest to extend <tag name="TECHNIQUE" value="start"/>statistical <tag name="TECHNIQUE" value="end"/> approaches to a more deep-linguistic analysis while at the same time keeping the speed and low complexity of a <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>statistical <tag name="TECHNIQUE" value="end"/><tag name="DOMAIN" value="start"/>parser<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
The resulting parsing architecture suggested , implemented and evaluated here ishighlyrobustandhybridonanumberof levels , combining <tag name="TECHNIQUE" value="start"/>statistical and rule-based<tag name="TECHNIQUE" value="end"/> approaches ,<tag name="TECHNIQUE" value="start"/> constituency and dependency grammar , shallow and deep processing<tag name="TECHNIQUE" value="end"/> , full and nearfull parsing .
With its parsing speed of about 300,000 words per hour and state-of-the-art performance the parser is reliable for a number of large-scale applications discussed in the article .


##W04-3249
<tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Unsupervised Domain Relevance Estimation<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> For <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Word Sense Disambiguation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
This paper presents <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Domain Relevance Estimation -LRB- DRE -RRB-<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> , a fully <tag name="TECHNIQUE" value="start"/>unsupervised text categorization<tag name="TECHNIQUE" value="end"/> technique based on the statistical estimation of the relevance of a text with respect to a certain category .
We use a <tag name="TECHNIQUE" value="start"/>pre-de ned set of categories<tag name="TECHNIQUE" value="end"/> -LRB- we call them domains -RRB- which have been previously associated to <tag name="TECHNIQUE" value="start"/>WORDNET <tag name="TECHNIQUE" value="end"/> word senses .
Given a certain domain , DRE distinguishes between relevant and non-relevant texts by means of a <tag name="TECHNIQUE" value="start"/>Gaussian Mixture <tag name="TECHNIQUE" value="end"/> model that describes the frequency distribution of domain words inside a large-scale corpus .
Then , an <tag name="TECHNIQUE" value="start"/>Expectation Maximization<tag name="TECHNIQUE" value="end"/> algorithm computes the parameters that maximize the likelihood of the model on the empirical data .
The correct identi cation of the domain of the text is a crucial point for <tag name="DOMAIN" value="start"/>Domain Driven Disambiguation<tag name="DOMAIN" value="end"/> , an <tag name="TECHNIQUE" value="start"/>unsupervised<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/> Word Sense Disambiguation -LRB- WSD -RRB-<tag name="DOMAIN" value="end"/> methodology that makes use of only domain information .
Therefore , DRE has been exploited and evaluated in the context of a <tag name="DOMAIN" value="start"/>WSD task<tag name="DOMAIN" value="end"/> .
Results are comparable to those of state-ofthe-art unsupervised WSD systems and show that DRE provides an important contribution .


##W06-1631
Capturing <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Out-Of-Vocabulary Words<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> In Arabic Text .
The increasing flow of information between languages has led to a rise in the frequency of non-native or loan words , where terms of one language appear transliterated in another .
Dealing with such out of vocabulary words is essential for successful cross-lingual information retrieval .
For example , techniques such as stemming should not be applied indiscriminately to all words in a collection , and so before any stemming , foreign words need to be identified .
In this paper , we investigate three approaches for the identification of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/> foreign words<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> in Arabic text :<tag name="TECHNIQUE" value="start"/> lexicons , language patterns , and n-grams<tag name="TECHNIQUE" value="end"/> and present that results show that <tag name="TECHNIQUE" value="start"/>lexicon-based<tag name="TECHNIQUE" value="end"/> approaches outperform the other techniques .


##W98-1427
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Generation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> As A Solution To Its Own Problem .
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Natural language generation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> technology is now ripe for commercial exploitation , but one of the remaining bottlenecks is that of providing NLG â¢ systems with user-friendly interfaces for Specifying the content of documents to be generated .
We present here a new technique we have developed for providing such interfaces : <tag name="TECHNIQUE" value="start"/>WYSIWYM editing<tag name="TECHNIQUE" value="end"/> .
<tag name="TECHNIQUE" value="start"/>WYSIWYM -LRB- What You See Is What You Meant -RRB-<tag name="TECHNIQUE" value="end"/> makes novel use of the system 's generator to provide a natural language input device which requires no NL interpretation ... - only NL generation .


##P97-1005
Automatic <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Detection Of Text Genre<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
As the text databases available to users become larger and more heterogeneous , genre becomes increasingly important for computational linguistics as a complement to topical and structural principles of classification .
We propose a theory of genres as bundles of facets , which correlate with various <tag name="TECHNIQUE" value="start"/>surface cues<tag name="TECHNIQUE" value="end"/> , and argue that <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>genre detection<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> based on <tag name="TECHNIQUE" value="start"/>surface cues<tag name="TECHNIQUE" value="end"/> is as successful as detection based on deeper structural properties .


##N09-1063
<tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Hierarchical Search<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
Both<tag name="FOCUS" value="start"/> <tag name="TECHNIQUE" value="start"/>coarse-to-fine<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>A â<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>parsing<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> use simple grammars to guide search in complex ones .
We compare the two approaches in a common , <tag name="TECHNIQUE" value="start"/>agenda-based <tag name="TECHNIQUE" value="end"/> framework , demonstrating the tradeoffs and relative strengths of each method .
Overall , <tag name="TECHNIQUE" value="start"/>coarse-to-fine<tag name="TECHNIQUE" value="end"/> is much faster for moderate levels of search errors , but below a certain threshold <tag name="TECHNIQUE" value="start"/>A â<tag name="TECHNIQUE" value="end"/> is superior .
In addition , we present the first experiments on <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>hierarchical A â<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , in which computation of heuristics is itself guided by meta-heuristics .
<tag name="TECHNIQUE" value="start"/>Multi-level hierarchies<tag name="TECHNIQUE" value="end"/> are helpful in both approaches , but are more effective in the coarseto-fine case because of accumulated slack in A â heuristics .


##C00-1075
Application Of <tag name="TECHNIQUE" value="start"/>Analogical <tag name="TECHNIQUE" value="end"/> Modelling To <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Example Based<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/> Machine Translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
This paper describes a <tag name="TECHNIQUE" value="start"/>self-modelling , incremental <tag name="TECHNIQUE" value="end"/> algorithm for learning <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/> translation rules <tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> from existing bilingual corpora .
The notions of <tag name="TECHNIQUE" value="start"/>supracontext<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>subcontext<tag name="TECHNIQUE" value="end"/> are extended to encompass bilingual information through simultaneous analogy on both source and target sentences and juxtaposition of corresponding results .
<tag name="TECHNIQUE" value="start"/>Analogical <tag name="TECHNIQUE" value="end"/>  modelling is performed during the learning phase and translation patterns are projected in a <tag name="TECHNIQUE" value="start"/>multi-dimensional analogical network<tag name="TECHNIQUE" value="end"/> .
The proposed fi'amework was evaluated on a small training corpus providing promising results .
Suggestions to improve system performance are


##H01-1045
Large Scale Testing Of A <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Descriptive Phrase Finder<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
This paper describes an evaluation of an existing technique that locates <tag name="DOMAIN" value="start"/>sentences containing descriptions of a query word or phrase<tag name="DOMAIN" value="end"/> .
The experiments expand on previous tests by exploring the effectiveness of the system when searching from a much larger document collection .
The results showed the system working significantly better than when searching over smaller collections .
The improvement was such , that a more stringent definition of what constituted a correct description was devised to better measure effectiveness .
The results also pointed to potentially new forms of evidence that might be used in improving the location process .
Keywords Information retrieval , descriptive phrases , WWW .


##D09-1071
The <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>infinite HMM<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> for <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>unsupervised<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/> PoS tagging<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We extend previous work on fully <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>unsupervised<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>part-of-speech tagging<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
Using a <tag name="TECHNIQUE" value="start"/>non-parametric version of the HMM<tag name="TECHNIQUE" value="end"/> , called the <tag name="TECHNIQUE" value="start"/>infinite HMM -LRB- iHMM -RRB- <tag name="TECHNIQUE" value="end"/>, we address the problem of choosing the number of hidden states in <tag name="TECHNIQUE" value="start"/>unsupervised Markov models<tag name="TECHNIQUE" value="end"/> for <tag name="DOMAIN" value="start"/>PoS tagging<tag name="DOMAIN" value="end"/> .
We experiment with two non-parametric priors , the <tag name="TECHNIQUE" value="start"/>Dirichlet and Pitman-Yor processes<tag name="TECHNIQUE" value="end"/> , on the Wall Street Journal dataset using a parallelized implementation of an <tag name="TECHNIQUE" value="start"/>iHMM inference<tag name="TECHNIQUE" value="end"/>  algorithm .
We evaluate the results with a variety of clustering evaluation metrics and achieve equivalent or better performances than previously reported .
Building on this promising result we evaluate the output of the <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>unsupervised<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>PoS tagger<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> as a direct replacement for the output of a fully supervised PoS tagger for the task of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>shallow parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> and compare the two evaluations .


##P05-2018
<tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Centrality Measures<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> In Text Mining : Prediction Of  <tag name="DOMAIN" value="start"/>Noun Phrases<tag name="DOMAIN" value="end"/> That Appear In Abstracts .
In this paper , we study different<tag name="TECHNIQUE" value="start"/> <tag name="FOCUS" value="start"/>centrality measures<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> being used in predicting <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/> noun phrases <tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> appearing in the abstracts of scientific articles .
Our experimental results show that centrality measures improve the accuracy of the prediction in terms of both precision and recall .
We also found that the method of constructing Noun Phrase Network significantly influences the accuracy when using the centrality heuristics itself , but is negligible when it is used together with other text features in decision trees .


##D08-1096
A <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Graph-theoretic <tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> Model of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Lexical Syntactic Acquisition<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
This paper presents a <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>graph-theoretic <tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> model of the acquisition of  <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>lexical syntactic representations<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
The representations the model learns are non-categorical or graded .
We propose a new evaluation methodology of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>syntactic acquisition<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> in the framework of exemplar theory .
When applied to the CHILDES corpus , the evaluation shows that the model 's graded syntactic representations perform better than previously proposed categorical representations .


##W97-1311
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Event Coreference<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> For <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Information Extraction<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We propose a general approach for performing <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>event coreference<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> and for constructing <tag name="DOMAIN" value="start"/>complex event representations<tag name="DOMAIN" value="end"/> , such as those required for <tag name="DOMAIN" value="start"/>information extraction<tag name="DOMAIN" value="end"/> tasks .
Our approach is based on a representation which allows a tight coupling between <tag name="TECHNIQUE" value="start"/> world or conceptual modelling and discourse modelling<tag name="TECHNIQUE" value="end"/> .
The representation and the coreference mechanism are fully implemented within the LaSIE information extraction system where the mechanism is used for both <tag name="DOMAIN" value="start"/>object -LRB- noun phrase -RRB- and event coreference resolution .<tag name="DOMAIN" value="end"/>
Indirect evaluation of the approach shows small , but significant benefit , for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>information extraction<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> tasks .


##P05-3018
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Word Alignment And Cross-Lingual Resource Acquisition<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
Annotated corpora are valuable resources for developing Natural Language Processing applications .
This work focuses on <tag name="FOCUS" value="start"/>acquiring annotated data for <tag name="DOMAIN" value="start"/>multilingual processing <tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> applications .
We present an annotation environment that supports a web-based user-interface for <tag name="DOMAIN" value="start"/>acquiring word alignments <tag name="DOMAIN" value="end"/> between English and Chinese as well as a visualization tool for researchers to explore the annotated data .


##A00-1046
The Efficiency Of <tag name="TECHNIQUE" value="start"/>Multimodal<tag name="TECHNIQUE" value="end"/> Interaction For A <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Map-Based <tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Task .
This paper compares the efficiency of using a standard direct-manipulation graphical user interface -LRB- GUI -RRB- with that of using the <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>QuickSet pen\/voice multimodal interface<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> for supporting a <tag name="DOMAIN" value="start"/>military task<tag name="DOMAIN" value="end"/> .
In this task , a user places military units and control measures -LRB- e.g. , various types of lines , obstacles , objectives -RRB- on a map .
Four military personnel designed and entered their own simulation scenarios via both interfaces .
Analyses revealed that the <tag name="TECHNIQUE" value="start"/>multimodal <tag name="TECHNIQUE" value="end"/> interface led to an average 3.5-fold speed improvement in the average entity creation time , including all error handling .
The mean time to repair errors also was 4.3 times faster when interacting multimodally .
Finally , all subjects reported a strong preference for <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>multimodal<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> interaction .
These results indicate a substantial efficiency advantage for multimodal over GUI-based interaction during <tag name="DOMAIN" value="start"/>map-based<tag name="DOMAIN" value="end"/> tasks .


##W98-0319
<tag name="TECHNIQUE" value="start"/>Lexical , Prosodic , And Syntactic Cues<tag name="TECHNIQUE" value="end"/> For <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Dialog Acts<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
The structure of a discourse is reflected in many aspects of its linguistic realization , including its lexical , prosodic , syntactic , and semantic nature .
Multiparty dialog contains a particular kind of discourse structure , the <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>dialog act -LRB- DA -RRB-<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
Like other types of structure , the dialog act sequence of a conversation is also reflected in its lexical , prosodic , and syntactic realization .
This paper presents a preliminary investigation into the realization of a particular class of  <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/> dialog acts<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> which play an essential structuring role in dialog , the backchannels or acknowledgements tokens .
We discuss the <tag name="TECHNIQUE" value="start"/>lexical , prosodic , and syntactic realization<tag name="TECHNIQUE" value="end"/> of these and subsumed or related <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>dialog acts<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> like continuers , assessments , yesanswers , agreements , and incipient-speakership .
We show that <tag name="TECHNIQUE" value="start"/>lexical knowledge<tag name="TECHNIQUE" value="end"/> plays a role in distinguishing these <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>dialog acts<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , despite the widespread ambiguity of words such as yeah , and that <tag name="TECHNIQUE" value="start"/>prosodic knowledge<tag name="TECHNIQUE" value="end"/> plays a role in DA identification for certain DA types , while <tag name="TECHNIQUE" value="start"/>lexical cues<tag name="TECHNIQUE" value="end"/> may be sufficient for the remainder .
Finally , our investigation of the syntax of assessments suggests that at least some dialog acts have a very constrained syntactic realization , a per-dialog act ` microsyntax ' .


##P03-1044
<tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Counter-Training<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> In Discovery Of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Semantic Patterns<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
This paper presents a method for <tag name="TECHNIQUE" value="start"/>unsupervised<tag name="TECHNIQUE" value="end"/> discovery of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/> semantic patterns<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Semantic patterns<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> are useful for a variety of text understanding tasks , in particular for locating events in text for information extraction  .
The method builds upon previously described approaches to <tag name="TECHNIQUE" value="start"/>iterative unsupervised<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>pattern acquisition<tag name="DOMAIN" value="end"/> .
One common characteristic of prior approaches is that the output of the algorithm is a continuous stream of patterns , with gradually degrading precision .
Our method differs from the previous pattern acquisition algorithms in that it introduces competition among several scenarios simultaneously .
This provides natural stopping criteria for the <tag name="TECHNIQUE" value="start"/>unsupervised learners<tag name="TECHNIQUE" value="end"/> , while maintaining good precision levels at termination .
We discuss the results of experiments with several scenarios , and examine different aspects of the new procedure .


##N06-3006
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Detecting Emotion In Speech<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> : Experiments In Three Domains .
The goal of my proposed dissertation work is to help answer two fundamental questions : -LRB- 1 -RRB- How is emotion communicated in speech ?
and -LRB- 2 -RRB- Does emotion modeling improve spoken dialogue applications ?
In this paper I describe <tag name="TECHNIQUE" value="start"/>feature extraction and emotion classi cation<tag name="TECHNIQUE" value="end"/> experiments I have conducted and plan to conduct on three different domains : <tag name="DOMAIN" value="start"/>EPSaT , HMIHY , and ITSpoke<tag name="DOMAIN" value="end"/> .
In addition , I plan to implement emotion modeling capabilities into ITSpoke and evaluate the effectiveness of doing so .


##E09-1056
Improvements in <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Analogical Learning<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> : Application to <tag name="DOMAIN" value="start"/>Translating Multi-Terms of the Medical Domain<tag name="DOMAIN" value="end"/> .
Handling terminology is an important matter in a translation workflow .
However , current Machine Translation -LRB- MT -RRB- systems do not yet propose anything proactive upon tools which assist in managing terminological databases .
In this work , we investigate several enhancements to <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>analogical learning<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> and test our implementation on <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>translating medical terms<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We show that the analogical engine works equally well when translating from and into a morphologically rich language , or when dealing with language pairs written in different scripts .
Combining it with a <tag name="TECHNIQUE" value="start"/>phrasebased statistical<tag name="TECHNIQUE" value="end"/> engine leads to significant improvements .


##W06-1112
A <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Structural Similarity<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/>  Measure .
This paper outlines a measure of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>language similarity<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> based on <tag name="TECHNIQUE" value="start"/>structural similarity of surface syntactic dependency trees<tag name="TECHNIQUE" value="end"/> .
Unlike the more traditional string-based measures , this measure tries to reflect `` deeper '' correspondences among languages .
The development of this measure has been inspired by the experience from MT of syntactically similar languages .
This experience shows that the lexical similarity is less important than syntactic similarity .
This claim is supported by a number of examples illustrating the problems which may arise when a measure of language similarity relies too much on a simple similarity of texts in different languages .


##C04-1159
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Dependency Structure Analysis<tag name="DOMAIN" value="end"/> And <tag name="DOMAIN" value="start"/>Sentence Boundary Detection<tag name="DOMAIN" value="end"/> In <tag name="DOMAIN" value="start"/>Spontaneous Japanese<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
This paper describes a project to detect <tag name="DOMAIN" value="start"/>dependencies <tag name="DOMAIN" value="end"/> between Japanese phrasal units called bunsetsus , and <tag name="DOMAIN" value="start"/>sentence boundaries in a spontaneous speech corpus<tag name="DOMAIN" value="end"/> .
In monologues , the biggest problem with dependency structure analysis is that sentence boundaries are ambiguous .
In this paper , we propose two methods for improving the accuracy of <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>sentence boundary detection<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> in <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>spontaneous Japanese speech<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> : One is based on <tag name="TECHNIQUE" value="start"/>statistical machine translation<tag name="TECHNIQUE" value="end"/> using <tag name="TECHNIQUE" value="start"/>dependency information<tag name="TECHNIQUE" value="end"/> and the other is based on <tag name="TECHNIQUE" value="start"/>text chunking using SVM<tag name="TECHNIQUE" value="end"/> .
An F-measure of 84.9 was achieved for the accuracy of sentence boundary detection by using the proposed methods .
The accuracy of dependency structure analysis was also improved from 75.2 % to 77.2 % by using automatically detected sentence boundaries .
The accuracy of dependency structure analysis and that of sentence boundary detection were also improved by interactively using both automatically detected dependency structures and sentence boundaries .


##P07-1085
<tag name="TECHNIQUE" value="start"/>Unsupervised<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/> Language Model Adaptation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Incorporating <tag name="TECHNIQUE" value="start"/>Named Entity Information<tag name="TECHNIQUE" value="end"/> .
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Language model -LRB- LM -RRB- adaptation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/>  is important for both speech and language processing .
It is often achieved by combining a generic LM with a topic-specific model that is more relevant to the target document .
Unlike previous work on unsupervised LM adaptation , this paper investigates how effectively using <tag name="TECHNIQUE" value="start"/>named entity <tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/>NE<tag name="TECHNIQUE" value="end"/> -RRB- information , instead of considering all the words , helps <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>LM adaptation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We evaluate two <tag name="TECHNIQUE" value="start"/>latent topic analysis<tag name="TECHNIQUE" value="end"/> approaches in this paper , namely , <tag name="TECHNIQUE" value="start"/>clustering<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>Latent Dirichlet Allocation<tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/>LDA<tag name="TECHNIQUE" value="end"/> -RRB- .
In addition , a new dynamically adapted weighting scheme for  <tag name="TECHNIQUE" value="start"/> topic mixture models<tag name="TECHNIQUE" value="end"/> is proposed based on <tag name="TECHNIQUE" value="start"/>LDA<tag name="TECHNIQUE" value="end"/> topic analysis .
Our experimental results show that the NE-driven LM adaptation framework outperforms the baseline generic LM .
The best result is obtained using the <tag name="TECHNIQUE" value="start"/>LDA-based <tag name="TECHNIQUE" value="end"/> approach by expanding the <tag name="TECHNIQUE" value="start"/>named entities <tag name="TECHNIQUE" value="end"/> with syntactically filtered words , together with using a large number of topics , which yields a perplexity reduction of 14.23 % compared to the baseline generic LM .


##W99-0908
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Text Classification<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> By <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Bootstrapping With Keywords , EM And Shrinkage<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> .
When applying <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>text classification<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> to complex tasks , it is tedious and expensive to hand-label the large amounts of training data necessary for good performance .
This paper presents an alternative approach to <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>text classification<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> that requires no labeled documentsi instead , it uses a small set of keywords per class , a class hierarchy and a large quantity of easilyobtained unlabeled documents .
The keywords are used to assign approximate labels to the unlabeled documents by termmatching .
These preliminary labels become the starting point for a <tag name="TECHNIQUE" value="start"/>bootstrapping<tag name="TECHNIQUE" value="end"/> process that learns a <tag name="TECHNIQUE" value="start"/>naive Bayes classifier<tag name="TECHNIQUE" value="end"/> using <tag name="TECHNIQUE" value="start"/>Expectation-Maximization and hierarchical shrinkage<tag name="TECHNIQUE" value="end"/> .
When classifying a complex data set of computer science research papers into a 70-leaf topic hierarchy , the keywords alone provide 45 % accuracy .
The classifier learned by bootstrapping reaches 66 % accuracy , a level close to human agreement .


##N03-2028
<tag name="TECHNIQUE" value="start"/>LM<tag name="TECHNIQUE" value="end"/> Studies On <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Filled Pauses In Spontaneous Medical Dictation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We investigate the optimal <tag name="TECHNIQUE" value="start"/>LM<tag name="TECHNIQUE" value="end"/> treatment of abundant <tag name="DOMAIN" value="start"/>filled pauses -LRB- FP -RRB- in spontaneous monologues of a professional dictation <tag name="DOMAIN" value="end"/> task .
Questions addressed here are -LRB- 1 -RRB- how to deal with <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>FP<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> in the LM history and -LRB- 2 -RRB- to which extent can the <tag name="TECHNIQUE" value="start"/>LM<tag name="TECHNIQUE" value="end"/> distinguish between positions with high and low <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>FP<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> likelihood .
Our results differ partly from observations reported on dialogues .
Discarding <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>FP<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> from all <tag name="TECHNIQUE" value="start"/>LM<tag name="TECHNIQUE" value="end"/> histories clearly improves the performance .
<tag name="TECHNIQUE" value="start"/>Local perplexities<tag name="TECHNIQUE" value="end"/> , <tag name="TECHNIQUE" value="start"/>entropies<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>word rankings<tag name="TECHNIQUE" value="end"/> at positions following <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>FP<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> suggest that most <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>FP<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> indicate hesitations rather than restarts .
Proper prediction of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>FP<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> allows to distinguish <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>FP<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> from word positions by a doubled FP probability .
Recognition experiments confirm the improvements found in our perplexity studies .


##P06-1092
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Phoneme-To-Text Transcription<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> System With An Infinite Vocabulary .
The <tag name="TECHNIQUE" value="start"/>noisy channel model<tag name="TECHNIQUE" value="end"/> approach is successfully applied to various natural language processing tasks .
Currently the main research focus of this approach is <tag name="TECHNIQUE" value="start"/>adaptation <tag name="TECHNIQUE" value="end"/> methods , how to capture characteristics of words and expressions in a target domain given example sentences in that domain .
As a solution we describe a method enlarging the vocabulary of a <tag name="TECHNIQUE" value="start"/>language model<tag name="TECHNIQUE" value="end"/> to an almost infinite size and capturing their context information .
Especially the new method is suitable for languages in which words are not delimited by whitespace .
We applied our method to a <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>phoneme-to-text transcription<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> task in Japanese and reduced about 10 % of the errors in the results of an existing method .


##C08-1131
Measuring and Predicting <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Orthographic Associations<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> : Modelling the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Similarity of Japanese Kanji<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
As human beings , our mental processes for recognizing linguistic symbols generate perceptual neighborhoods around such symbols where confusion errors occur .
Such neighborhoods also provide us with conscious mental associations between symbols .
This paper formalises <tag name="TECHNIQUE" value="start"/>orthographic <tag name="TECHNIQUE" value="end"/> models for <tag name="DOMAIN" value="start"/>similarity of Japanese kanji<tag name="DOMAIN" value="end"/> , and provides a proofof-concept dictionary extension leveraging the mental associations provided by <tag name="TECHNIQUE" value="start"/>orthographic proximity<tag name="TECHNIQUE" value="end"/> .


##P06-1044
Automatic Classification Of <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Verbs In Biomedical Texts<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
Lexical classes , when tailored to the application and domain in question , can provide an effective means to deal with a number of natural language processing -LRB- NLP -RRB- tasks .
While manual construction of such classes is difficult , recent research shows that it is possible to automatically induce verb classes from <tag name="TECHNIQUE" value="start"/>cross-domain corpora<tag name="TECHNIQUE" value="end"/> with promising accuracy .
We report a novel experiment where similar technology is applied to the important , challenging domain of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>biomedicine<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We show that the resulting classification , acquired from a corpus of biomedical journal articles , is highly accurate and strongly domainspecific .
It can be used to aid <tag name="DOMAIN" value="start"/>BIO-NLP<tag name="DOMAIN" value="end"/> directly or as useful material for investigating the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>syntax and semantics of verbs in biomedical texts<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .


##P04-1026
<tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Linguistic Profiling<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> For <tag name="DOMAIN" value="start"/>Authorship Recognition And Verification<tag name="DOMAIN" value="end"/> .
A new technique is introduced , <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>linguistic profiling<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> , in which large numbers of counts of <tag name="TECHNIQUE" value="start"/> linguistic features<tag name="TECHNIQUE" value="end"/> are used as a text profile , which can then be compared to average profiles for groups of texts .
The technique proves to be quite effective for <tag name="DOMAIN" value="start"/>authorship verification and recognition<tag name="DOMAIN" value="end"/> .
The best parameter settings yield a False Accept Rate of 8.1 % at a False Reject Rate equal to zero for the verification task on a test corpus of student essays , and a 99.4 % 2-way recognition accuracy on the same corpus .


##I05-1054
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Machine Translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Based on <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Constraint-Based Synchronous Grammar<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> .
This paper proposes a variation of <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>synchronous grammar<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> based on the formalism of <tag name="TECHNIQUE" value="start"/>context-free grammar<tag name="TECHNIQUE" value="end"/> by generalizing the first component of productions that models the source text , named <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Constraint-based Synchronous Grammar -LRB- CSG -RRB-<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> .
Unlike other synchronous grammars , <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>CSG<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> allows multiple target productions to be associated to a single source production rule , which can be used to guide a parser to infer different possible translational equivalences for a recognized input string according to the feature constraints of symbols in the pattern .
Furthermore , <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>CSG<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> is augmented with independent rewriting that allows expressing discontinuous constituents in the inference rules .
It turns out that such grammar is more expressive to model the translational equivalences of parallel texts for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>machine translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , and in this paper , we propose the use of <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>CSG<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> as a basis for building a <tag name="DOMAIN" value="start"/>machine translation <tag name="DOMAIN" value="end"/> -LRB- <tag name="DOMAIN" value="start"/>MT<tag name="DOMAIN" value="end"/> -RRB- system for Portuguese to Chinese translation .


##P05-1032
Scaling <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Phrase-Based Statistical<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>Machine Translation<tag name="DOMAIN" value="end"/> <tag name="FOCUS" value="end"/>To Larger Corpora And Longer Phrases .
In this paper we describe a novel data structure for <tag name="FOCUS" value="start"/> <tag name="TECHNIQUE" value="start"/>phrase-based statistical<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/> machine translation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> which allows for the retrieval of arbitrarily long phrases while simultaneously using less memory than is required by current decoder implementations .
We detail the computational complexity and average retrieval times for looking up phrase translations in our <tag name="TECHNIQUE" value="start"/>suffix array-based data structure<tag name="TECHNIQUE" value="end"/> .
We show how <tag name="TECHNIQUE" value="start"/>sampling<tag name="TECHNIQUE" value="end"/> can be used to reduce the retrieval time by orders of magnitude with no loss in <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> quality .


##J91-4002
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Systemic Classification<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> And Its Efficiency .
This paper examines the problem of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>classifying linguistic objects<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> on the basis of information encoded in the <tag name="TECHNIQUE" value="start"/> system network formalism<tag name="TECHNIQUE" value="end"/> developed by Halliday .
It is shown that this problem is NP-hard , and a restriction to the formalism , which renders the classification problem soluble in polynomial time , is suggested .
An algorithm for the <tag name="DOMAIN" value="start"/>unrestricted classification <tag name="DOMAIN" value="end"/> problem , which separates a potentially expensive second stage from a more tractable first stage , is then presented .


##I05-1088
Extracting <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Terminologically Relevant Collocations<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> in the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Translation of Chinese Monograph<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
This paper suggests a methodology which is aimed to extract the <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>terminologically relevant collocations<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> purposes .
Our basic idea is to use a <tag name="TECHNIQUE" value="start"/>hybrid <tag name="TECHNIQUE" value="end"/> method which combines the <tag name="TECHNIQUE" value="start"/>statistical method and linguistic rules<tag name="TECHNIQUE" value="end"/> .
The extraction system used in our work operated at three steps : -LRB- 1 -RRB- <tag name="TECHNIQUE" value="start"/>Tokenization and POS tagging<tag name="TECHNIQUE" value="end"/> of the corpus ; -LRB- 2 -RRB- <tag name="TECHNIQUE" value="start"/>Extraction of multi-word units using statistical measure<tag name="TECHNIQUE" value="end"/> ; -LRB- 3 -RRB- Linguistic filtering to make use of <tag name="TECHNIQUE" value="start"/> syntactic patterns and stop-word list<tag name="TECHNIQUE" value="end"/> .
As a result , <tag name="TECHNIQUE" value="start"/>hybrid<tag name="TECHNIQUE" value="end"/> method using <tag name="TECHNIQUE" value="start"/>linguistic filters<tag name="TECHNIQUE" value="end"/> proved to be a suitable method for selecting terminological collocations , it has considerably improved the precision of the extraction which is much higher than that of purely statistical method .
In our test , hybrid method combining `` <tag name="TECHNIQUE" value="start"/>Log-likelihood ratio<tag name="TECHNIQUE" value="end"/> '' and `` <tag name="TECHNIQUE" value="start"/>linguistic rules<tag name="TECHNIQUE" value="end"/> '' had the best performance in the extraction .
We believe that <tag name="TECHNIQUE" value="start"/>terminological collocations<tag name="TECHNIQUE" value="end"/> and phrases extracted in this way , could be used effectively either to supplement existing terminological collections or to be used in addition to traditional reference works .


##W97-0906
Practical Considerations In Building A <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Multi-Lingual Authoring System<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> For Business Letters .
The paper describes the experiences of a multi-national consortium in an on-going project to construct a <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>multilingual authoring<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> tool for business letters .
The consortium consists of two universities -LRB- both with significant experience in language engineering -RRB- , three software companies , and various potential commercial users with the organizations being located in a total of four countries .
The paper covers the history of the development of the project from an academic idea but focuses on the implications of the user-requirements orientated outlook of the commercial developers and the implications of this view for the system architecture , user requirements , delivery platforms and so on .
Particularly interesting consequences of the user requirements are the <tag name="TECHNIQUE" value="start"/>database centred architecture<tag name="TECHNIQUE" value="end"/> , and the constraints and opportunities this presents for development of <tag name="TECHNIQUE" value="start"/>grammatical components<tag name="TECHNIQUE" value="end"/> at both the text and sentence level .


##W97-1006
Method For Improving Automatic <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Word Categorization<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
This paper presents a new approach to automatic  <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>word categorization<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> which improves both the efficiency of the algorithm and the quality of the formed clusters .
The <tag name="TECHNIQUE" value="start"/>unigram and the bigram<tag name="TECHNIQUE" value="end"/> statistics of a corpus of about two million words are used with an efficient <tag name="TECHNIQUE" value="start"/>distance function<tag name="TECHNIQUE" value="end"/> to measure the similarities of words , and a <tag name="TECHNIQUE" value="start"/>greedy algorithm<tag name="TECHNIQUE" value="end"/> to put the words into clusters .
The notions of <tag name="TECHNIQUE" value="start"/>fuzzy clustering<tag name="TECHNIQUE" value="end"/> like <tag name="TECHNIQUE" value="start"/>cluster prototypes , degree of membership<tag name="TECHNIQUE" value="end"/> are used to form up the clusters .
The algorithm is of <tag name="TECHNIQUE" value="start"/>unsupervised<tag name="TECHNIQUE" value="end"/> type and the number of clusters are determined at run-time .


##P05-3008
A <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Voice Enabled Procedure Browser<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> For The International Space Station .
Clarissa , an experimental <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>voice enabled procedure browser<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> that has recently been deployed on the International Space Station -LRB- ISS -RRB- , is to the best of our knowledge the first <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>spoken dialog<tag name="FOCUS" value="end"/> <tag name="DOMAIN" value="end"/> system in space .
This paper gives background on the system and the ISS procedures , then discusses the research developed to address three key problems : <tag name="DOMAIN" value="start"/>grammarbased speech recognition<tag name="DOMAIN" value="end"/> using the <tag name="TECHNIQUE" value="start"/>Regulus toolkit<tag name="TECHNIQUE" value="end"/> ; <tag name="TECHNIQUE" value="start"/>SVM <tag name="TECHNIQUE" value="end"/> based methods for <tag name="DOMAIN" value="start"/>open microphone speech recognition<tag name="DOMAIN" value="end"/> ; and robust <tag name="TECHNIQUE" value="start"/>side-effect free dialogue management<tag name="TECHNIQUE" value="end"/> for handling undos , corrections and confirmations .


##W07-1026
Automatic <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Indexing of Specialized Documents<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> : Using <tag name="TECHNIQUE" value="start"/>Generic vs. Domain-Specific Document Representations<tag name="TECHNIQUE" value="end"/> .
The shift from paper to electronic documents has caused the curation of information sources in large electronic databases to become more generalized .
In the biomedical domain , continuing efforts aim at refining indexing tools to assist with the update and maintenance of databases such as MEDLINE Â® .
In this paper , we evaluate two <tag name="TECHNIQUE" value="start"/>statistical <tag name="TECHNIQUE" value="end"/> methods of producing <tag name="DOMAIN" value="start"/> MeSH Â® indexing recommendations for the genetics literature<tag name="DOMAIN" value="end"/> , including <tag name="DOMAIN" value="start"/>recommendations involving subheadings<tag name="DOMAIN" value="end"/> , which is a novel application for the methods .
We show that a <tag name="TECHNIQUE" value="start"/>generic representation of the documents<tag name="TECHNIQUE" value="end"/> yields both better precision and recall .
We also find that a <tag name="TECHNIQUE" value="start"/>domainspecific representation<tag name="TECHNIQUE" value="end"/> of the documents can contribute to enhancing recall .


##C08-1042
Evaluating <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Unsupervised Part-of-Speech Tagging<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Grammar Induction<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
This paper explores the relationship between various measures of <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>unsupervised part-of-speech tag <tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> induction and the performance of both <tag name="TECHNIQUE" value="start"/>supervised and unsupervised<tag name="TECHNIQUE" value="end"/> <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>parsing<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> models trained on induced <tag name="TECHNIQUE" value="start"/>tags<tag name="TECHNIQUE" value="end"/> .
We find that no standard tagging metrics correlate well with <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>unsupervised<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>parsing<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> performance , and several metrics grounded in information theory have no strong relationship with even supervised parsing performance .


##C96-2104
A Portable And Quick Japanese <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Parser<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> : <tag name="FOCUS" value="start"/>QJP<tag name="FOCUS" value="end"/> .
<tag name="FOCUS" value="start"/>QJP<tag name="FOCUS" value="end"/> is a portable and quick softwaxe module for Japanese processing .
<tag name="FOCUS" value="start"/>QJP<tag name="FOCUS" value="end"/> analyzes a Japanese sentence into <tag name="DOMAIN" value="start"/>segmented morphemes\/words with tags<tag name="DOMAIN" value="end"/> and a syntactic bunsetsu kakari-uke structure based on the two strategies , a -RRB- <tag name="TECHNIQUE" value="start"/>Morphological<tag name="TECHNIQUE" value="end"/> analysis based on <tag name="TECHNIQUE" value="start"/>character-types and functional-words<tag name="TECHNIQUE" value="end"/> and b -RRB- <tag name="TECHNIQUE" value="start"/>Syntactic<tag name="TECHNIQUE" value="end"/> analysis  by simple treatment of <tag name="TECHNIQUE" value="start"/> structural ambiguities<tag name="TECHNIQUE" value="end"/> and ignoring semantic information .
<tag name="FOCUS" value="start"/>QJP<tag name="FOCUS" value="end"/> is small , fast and robust , because 1 -RRB- dictionary size -LRB- less than1 100KB -RRB- and required memory size -LRB- 260KB -RRB- are very small , 2 -RRB- analysis speed is fast -LRB- more than 100 words\/see on 80486-PC -RRB- , and 3 -RRB- even a 100-word long sentence containing unknown words is easily processed .
Using <tag name="FOCUS" value="start"/>QJP<tag name="FOCUS" value="end"/> and its ana -RRB- ysis results as a base and adding other functions for processing Japanese documents , a valqety of applications can be developed on UNIX workstations or even on PCs .


##J94-4001
A <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Syntactic Analysis<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/>  Method Of Long Japanese Sentences Based On The Detection Of <tag name="TECHNIQUE" value="start"/>Conjunctive Structures<tag name="TECHNIQUE" value="end"/> .
This paper presents a <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>syntactic analysis<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> method that first detects <tag name="TECHNIQUE" value="start"/>conjunctive structures<tag name="TECHNIQUE" value="end"/> in a sentence by checking parallelism of two series of words and then analyzes the <tag name="DOMAIN" value="start"/>dependency structure<tag name="DOMAIN" value="end"/> of the sentence with the help of the information about the <tag name="TECHNIQUE" value="start"/>conjunctive structures<tag name="TECHNIQUE" value="end"/> .
Analysis of long sentences is one of the most difficult problems in natural language processing .
The main reason for this difficulty is the structural ambiguity that is common for <tag name="TECHNIQUE" value="start"/>conjunctive structures<tag name="TECHNIQUE" value="end"/> that appear in long sentences .
Human beings can recognize <tag name="TECHNIQUE" value="start"/>conjunctive structures<tag name="TECHNIQUE" value="end"/> because of a certain , but sometimes subtle , similarity that exists between conjuncts .
Therefore , we have developed an algorithm for calculating a similarity measure between two arbitrary series of words from the left and the right of a conjunction and selecting the two most similar series of words that can reasonably be considered as composing a <tag name="TECHNIQUE" value="start"/>conjunctive structure<tag name="TECHNIQUE" value="end"/> .
This is realized using a <tag name="TECHNIQUE" value="start"/>dynamic programming <tag name="TECHNIQUE" value="end"/> technique .
A long sentence can be reduced into a shorter form by recognizing <tag name="TECHNIQUE" value="start"/>conjunctive structures<tag name="TECHNIQUE" value="end"/> .
Consequently , the total <tag name="DOMAIN" value="start"/>dependency structure<tag name="DOMAIN" value="end"/>  of a sentence can be obtained by relatively simple <tag name="TECHNIQUE" value="start"/>head-dependent rules<tag name="TECHNIQUE" value="end"/> .
A serious problem concerning <tag name="TECHNIQUE" value="start"/>conjunctive structures<tag name="TECHNIQUE" value="end"/> , besides the ambiguity of their scopes , is the ellipsis of some of their components .
Through our <tag name="DOMAIN" value="start"/>dependency analysis<tag name="DOMAIN" value="end"/> process , we can find the ellipses and recover the omitted components .
We report the results of analyzing 150Japanese sentences to illustrate the effectiveness of this method .


##W97-0803
Extending A <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Thesaurus<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> By <tag name="TECHNIQUE" value="start"/>Classifying Words<tag name="TECHNIQUE" value="end"/> .
This paper proposes a method for extending an existing <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>thesaurus<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> through <tag name="TECHNIQUE" value="start"/>classification of new words<tag name="TECHNIQUE" value="end"/>  in terms of that <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>thesaurus<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
New words are <tag name="TECHNIQUE" value="start"/>classified<tag name="TECHNIQUE" value="end"/> on the basis of relative probabilities of .
a word belonging to a given word class , with the probabilities calculated using <tag name="TECHNIQUE" value="start"/>nounverb co-occurrence pairs<tag name="TECHNIQUE" value="end"/> .
Experiments using the Japanese Bunruigoihy5 thesaurus on about 420,000 co-occurrences showed that new words can be classified correctly with a maximum accuracy of more than 80 % .


##H05-1059
<tag name="TECHNIQUE" value="start"/>Bidirectional Inference With The Easiest-First Strategy<tag name="TECHNIQUE" value="end"/> For <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Tagging Sequence Data<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
This paper presents a <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>bidirectional inference <tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> algorithm for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>sequence labeling<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> problems such as <tag name="DOMAIN" value="start"/>part-of-speech tagging<tag name="DOMAIN" value="end"/> , <tag name="DOMAIN" value="start"/>named entity recognition<tag name="DOMAIN" value="end"/> and <tag name="DOMAIN" value="start"/>text chunking<tag name="DOMAIN" value="end"/> .
The algorithm can enumerate all possible decomposition structures and find the highest probability sequence together with the corresponding decomposition structure in polynomial time .
We also present an efficient <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>decoding<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> algorithm based on the <tag name="TECHNIQUE" value="start"/>easiest-first strategy<tag name="TECHNIQUE" value="end"/> , which gives comparably good performance to full <tag name="TECHNIQUE" value="start"/>bidirectional inference<tag name="TECHNIQUE" value="end"/> with significantly lower computational cost .
Experimental results of <tag name="DOMAIN" value="start"/>part-of-speech tagging<tag name="DOMAIN" value="end"/> and <tag name="DOMAIN" value="start"/>text chunking<tag name="DOMAIN" value="end"/> show that the proposed <tag name="TECHNIQUE" value="start"/>bidirectional inference<tag name="TECHNIQUE" value="end"/> methods consistently outperform unidirectional inference methods and bidirectional MEMMs give comparable performance to that achieved by state-of-the-art learning algorithms including kernel support vector machines .


##W02-2001
Extracting The Unextractable : A Case Study On <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Verb-Particles<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
This paper proposes a series of techniques for extracting <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/> English verb -LCB- particle constructions <tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> from raw text corpora .
We initially propose three basic methods , based on <tag name="TECHNIQUE" value="start"/>tagger<tag name="TECHNIQUE" value="end"/> output , <tag name="TECHNIQUE" value="start"/>chunker output and a chunk grammar<tag name="TECHNIQUE" value="end"/> , respectively , with the <tag name="TECHNIQUE" value="start"/>chunk grammar<tag name="TECHNIQUE" value="end"/> method optionally combining with an <tag name="TECHNIQUE" value="start"/>attachment resolution<tag name="TECHNIQUE" value="end"/> module to determine the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>syntactic structure of verb -LCB- preposition pairs<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> in ambiguous constructs .
We then combine the three methods together into a single <tag name="TECHNIQUE" value="start"/>classifler<tag name="TECHNIQUE" value="end"/> , and add in a number of extra <tag name="TECHNIQUE" value="start"/>lexical and frequentistic features<tag name="TECHNIQUE" value="end"/> , producing a flnal F-score of 0.865 over the WSJ .


##N07-1038
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Multiple Aspect Ranking<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Using the <tag name="TECHNIQUE" value="start"/>Good Grief Algorithm<tag name="TECHNIQUE" value="end"/> .
We address the problem of analyzing <tag name="DOMAIN" value="start"/>multiple related opinions<tag name="DOMAIN" value="end"/> in a text .
For instance , in a restaurant review such opinions may include food , ambience and service .
We formulate this task as a <tag name="DOMAIN" value="start"/>multiple aspect ranking<tag name="DOMAIN" value="end"/> problem , where the goal is to produce a set of numerical scores , one for each aspect .
We present an algorithm that<tag name="TECHNIQUE" value="start"/> jointly learns<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>ranking<tag name="DOMAIN" value="end"/> models for individual aspects by modeling the <tag name="TECHNIQUE" value="start"/>dependencies<tag name="TECHNIQUE" value="end"/> between assigned ranks .
This algorithm guides the prediction of individual rankers by analyzing <tag name="TECHNIQUE" value="start"/>meta-relations <tag name="TECHNIQUE" value="end"/>between opinions , such as agreement and contrast .
We prove that our <tag name="TECHNIQUE" value="start"/>agreementbased joint<tag name="TECHNIQUE" value="end"/> model is more expressive than individual <tag name="DOMAIN" value="start"/>ranking<tag name="DOMAIN" value="end"/> models .
Our empirical results further con rm the strength of the model : the algorithm provides signi cant improvement over both individual rankers and a state-of-the-art joint <tag name="DOMAIN" value="start"/>ranking<tag name="DOMAIN" value="end"/> model .


##W09-0412
NUS at WMT09 : <tag name="TECHNIQUE" value="start"/>Domain Adaptation<tag name="TECHNIQUE" value="end"/> Experiments for <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>English-Spanish Machine Translation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> of News Commentary Text .
We describe the system developed by the team of the National University of Singapore for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>English to Spanish machine translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> of News Commentary text for the WMT09 Shared Translation Task .
Our approach is based on <tag name="TECHNIQUE" value="start"/>domain adaptation<tag name="TECHNIQUE" value="end"/> , combining a small in-domain News Commentary bi-text and a large out-of-domain one from the Europarl corpus , from which we built and combined two separate phrase tables .
We further combined two <tag name="TECHNIQUE" value="start"/>language models <tag name="TECHNIQUE" value="end"/> -LRB- in-domain and out-of-domain -RRB- , and we experimented with <tag name="TECHNIQUE" value="start"/>cognates<tag name="TECHNIQUE" value="end"/> , improved <tag name="TECHNIQUE" value="start"/>tokenization<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>recasing<tag name="TECHNIQUE" value="end"/> , achieving the highest lowercased NIST score of 6.963 and the second best lowercased Bleu score of 24.91 % for training without using additional external data for English-toSpanish translation at the shared task .


##W07-2094
<tag name="TECHNIQUE" value="start"/>UPAR7<tag name="TECHNIQUE" value="end"/> : A <tag name="TECHNIQUE" value="start"/>knowledge-based<tag name="TECHNIQUE" value="end"/> system for headline <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/> sentiment tagging<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
For the Affective Text task at SemEval2007 , University Paris 7 's system first evaluates <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>emotion and valence on all words<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> of a news headline -LRB- using enriched versions of <tag name="TECHNIQUE" value="start"/>SentiWordNet<tag name="TECHNIQUE" value="end"/> and a subset of <tag name="TECHNIQUE" value="start"/>WordNetAffect<tag name="TECHNIQUE" value="end"/> -RRB- .
We use a <tag name="TECHNIQUE" value="start"/>parser<tag name="TECHNIQUE" value="end"/> to find the <tag name="TECHNIQUE" value="start"/>head word<tag name="TECHNIQUE" value="end"/> , considering that it has a major importance .
We also detect contrasts -LRB- between <tag name="TECHNIQUE" value="start"/>positive and negative words<tag name="TECHNIQUE" value="end"/> -RRB- that shift valence .
Our <tag name="TECHNIQUE" value="start"/>knowledge-based<tag name="TECHNIQUE" value="end"/> system achieves high accuracy on <tag name="DOMAIN" value="start"/>emotion and valence annotation<tag name="DOMAIN" value="end"/> .
These results show that working with <tag name="TECHNIQUE" value="start"/>linguistic<tag name="TECHNIQUE" value="end"/> techniques and a broad-coverage <tag name="TECHNIQUE" value="start"/>lexicon<tag name="TECHNIQUE" value="end"/> is a viable approach to <tag name="DOMAIN" value="start"/>sentiment analysis<tag name="DOMAIN" value="end"/> of headlines .
1 Introduction 1.1 Objectives The detection of emotional connotations in texts is a recent task in computational linguistics .
Its economic stakes are promising ; for example , a company could detect , by analyzing the blogosphere , people 's opinion on its products .
The goal of the SemEval task is to annotate news headlines for emotions -LRB- using a predefined list : anger , disgust , fear , joy , sadness & surprise -RRB- , and for valence -LRB- positive or negative -RRB- .
A specific difficulty here is related to the small number of words available for the analysis .


##P07-1024
Optimizing <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Grammars<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> for Minimum <tag name="FOCUS" value="start"/>Dependency Length<tag name="FOCUS" value="end"/> .
We examine the problem of choosing word order for a set of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>dependency trees<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> so as to minimize total <tag name="FOCUS" value="start"/>dependency length<tag name="FOCUS" value="end"/> .
We present an algorithm for computing the optimal layout of a single tree as well as a <tag name="TECHNIQUE" value="start"/>numerical<tag name="TECHNIQUE" value="end"/> method for optimizing a <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>grammar<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> of orderings over a set of dependency types .
A <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>grammar<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> generated by minimizing <tag name="FOCUS" value="start"/>dependency length<tag name="FOCUS" value="end"/> in unordered trees from the Penn Treebank is found to agree surprisingly well with English word order , suggesting that <tag name="FOCUS" value="start"/>dependency length<tag name="FOCUS" value="end"/> minimization has influenced the evolution of English .


##D07-1071
<tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Online Learning of Relaxed CCG Grammars<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Parsing to Logical Form<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We consider the problem of learning to <tag name="DOMAIN" value="start"/>parse sentences<tag name="DOMAIN" value="end"/> to <tag name="DOMAIN" value="start"/>lambda-calculus representations<tag name="DOMAIN" value="end"/> of their underlying semantics and present an algorithm that learns a <tag name="TECHNIQUE" value="start"/>weighted combinatory categorial grammar -LRB- CCG -RRB-<tag name="TECHNIQUE" value="end"/> .
A key idea is to introduce <tag name="TECHNIQUE" value="start"/>non-standard CCG combinators<tag name="TECHNIQUE" value="end"/> that relax certain parts of the grammar -- for example allowing flexible word order , or insertion of lexical items -- with learned costs .
We also present a new , <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>online algorithm for inducing a weighted CCG<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> .
Results for the approach on ATIS data show 86 % F-measure in recovering fully correct semantic analyses and 95.9 % F-measure by a partial-match criterion , a more than 5 % improvement over the 90.3 % partial-match figure reported by He and Young -LRB- 2006 -RRB- .


##H05-1013
A Large-Scale Exploration Of Effective <tag name="TECHNIQUE" value="start"/>Global Features<tag name="TECHNIQUE" value="end"/> For A <tag name="FOCUS" value="start"/> Joint <tag name="DOMAIN" value="start"/> Entity Detection And Tracking <tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> Model .
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Entity detection and tracking -LRB- EDT -RRB-<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> is the task of identifying textual mentions of real-world entities in documents , extending the <tag name="DOMAIN" value="start"/>named entity detection<tag name="DOMAIN" value="end"/> and <tag name="DOMAIN" value="start"/>coreference resolution<tag name="DOMAIN" value="end"/> task by considering mentions other than names -LRB- pronouns , de nite descriptions , etc. -RRB- .
Like NE tagging and coreference resolution , most solutions to the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>EDT<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> task separate out the mention detection aspect from the coreference aspect .
By doing so , these solutions are limited to using only local features for learning .
In contrast , by modeling both aspects of the EDT task simultaneously , we are able to learn using highly <tag name="TECHNIQUE" value="start"/>complex , non-local features<tag name="TECHNIQUE" value="end"/> .
We develop a new <tag name="FOCUS" value="start"/> joint <tag name="DOMAIN" value="start"/>EDT<tag name="DOMAIN" value="end"/> <tag name="FOCUS" value="end"/> model and explore the utility of many features , demonstrating their effectiveness on this task .


##W06-2109
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>German Particle Verbs And Pleonastic Prepositions<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
This paper discusses the behavior of <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/> German particle verbs<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> formed by two-way prepositions in combination with <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>pleonastic PPs<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> including the verb particle as a preposition .
These <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>particle verbs<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> have a characteristic feature : some of them license directional prepositional phrases in the accusative , some only allow for locative PPs in the dative , and some particle verbs can occur with PPs in the accusative and in the dative .
Directional particle verbs together with directional PPs present an additional problem : the particle and the preposition in the PP seem to provide redundant information .
The paper gives an overview of the <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>semantic verb classes<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> in uencing this phenomenon , based on corpus data , and explains the underlying reasons for the behavior of the particle verbs .
We also show how the restrictions on <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>particle verbs and pleonastic PPs<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> can be expressed in a grammar theory like <tag name="TECHNIQUE" value="start"/>Lexical Functional Grammar -LRB- LFG -RRB-<tag name="TECHNIQUE" value="end"/> .


##P04-3023
On The <tag name="FOCUS" value="start"/>Equivalence Of <tag name="DOMAIN" value="start"/>Weighted Finite-State Transducers<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
Although they can be topologically different , two distinct transducers may actually recognize the same rational relation .
Being able to test the equivalence of transducers allows to implement such operations as incremental minimization and iterative composition .
This paper presents an algorithm for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>testing the equivalence of deterministic weighted finite-state transducers<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , and outlines an implementation of its applications in a prototype weighted finite-state <tag name="TECHNIQUE" value="start"/>calculus tool<tag name="TECHNIQUE" value="end"/> .


##E06-1006
<tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Phrase-Based Backoff <tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> Models For <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Machine Translation Of Highly Inflected Languages<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We propose a <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>backoff <tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> model for <tag name="FOCUS" value="start"/> <tag name="TECHNIQUE" value="start"/>phrasebased<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/> machine translation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> that translates unseen word forms in foreign-language text by <tag name="TECHNIQUE" value="start"/>hierarchical morphological abstractions<tag name="TECHNIQUE" value="end"/> at the word and the phrase level .
The model is evaluated on the Europarl corpus for German-English and FinnishEnglish translation and shows improvements over state-of-the-art phrase-based models .


##P98-1075
Growing <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Semantic Grammars<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
A critical path in the development of <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>natural language understanding -LRB- NLU -RRB-<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> modules lies in the difficulty of defining a mapping from words to semantics : Usually it takes in the order of years of highly-skilled labor to develop a <tag name="DOMAIN" value="start"/>semantic mapping<tag name="DOMAIN" value="end"/> , e.g. , in the form of a <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>semantic grammar<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , that is comprehensive enough for a given domain .
Yet , due to the very nature of human language , such mappings invariably fail to achieve full coverage on unseen data .
Acknowledging the impossibility of stating a priori all the surface forms by which a concept can be expressed , we present <tag name="FOCUS" value="start"/>GsG<tag name="FOCUS" value="end"/> : an empathic computer system for the rapid deployment of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>NLU front-ends<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> and their dynamic customization by non-expert end-users .
Given a new domain for which an <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>NLU front-end<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> is to be developed , two stages are involved .
In the authoring stage , <tag name="FOCUS" value="start"/>GSQ<tag name="FOCUS" value="end"/> aids the developer in the construction of a simple <tag name="TECHNIQUE" value="start"/>domain model<tag name="TECHNIQUE" value="end"/> and a <tag name="DOMAIN" value="start"/>kernel analysis grammar<tag name="DOMAIN" value="end"/> .
Then , in the run-time stage , GSG provides the enduser with an interactive environment in which the kernel grammar is dynamically extended .
Three <tag name="TECHNIQUE" value="start"/>learning <tag name="TECHNIQUE" value="end"/> methods are employed in the acquisition of <tag name="DOMAIN" value="start"/>semantic mappings<tag name="DOMAIN" value="end"/> from unseen data : -LRB- i -RRB- <tag name="TECHNIQUE" value="start"/>parser predictions<tag name="TECHNIQUE" value="end"/> , -LRB- ii -RRB- <tag name="TECHNIQUE" value="start"/>hidden understanding model<tag name="TECHNIQUE" value="end"/> , and -LRB- iii -RRB- <tag name="TECHNIQUE" value="start"/>end-user paraphrases<tag name="TECHNIQUE" value="end"/> .
A baseline version of <tag name="FOCUS" value="start"/>GsG<tag name="FOCUS" value="end"/> has been implemented and prellminary experiments show promising results .


##A97-1011
A <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Non-Projective Dependency Parser<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
We describe a practical <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>parser for unrestricted dependencies<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
The parser creates links between words and names the links according to their <tag name="TECHNIQUE" value="start"/>syntactic functions<tag name="TECHNIQUE" value="end"/> .
We first describe the older <tag name="TECHNIQUE" value="start"/>Constraint Grammar<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>parser<tag name="DOMAIN" value="end"/> where many of the ideas come from .
Then we proceed to describe the central ideas of our new parser .
Finally , the parser is evaluated .


##P97-1040
Efficient <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Generation<tag name="DOMAIN" value="end"/> In Primitive Optimality Theory<tag name="FOCUS" value="end"/> .
This paper introduces <tag name="FOCUS" value="start"/>primitive Optimality Theory -LRB- OTP -RRB-<tag name="FOCUS" value="end"/> , a linguistically motivated <tag name="FOCUS" value="start"/>formalization of OT<tag name="FOCUS" value="end"/> .
<tag name="FOCUS" value="start"/>OTP<tag name="FOCUS" value="end"/> specifies the class of <tag name="DOMAIN" value="start"/>autosegmental representations<tag name="DOMAIN" value="end"/> , the <tag name="DOMAIN" value="start"/>universal generator Gen<tag name="DOMAIN" value="end"/> , and the two simple families of permissible constraints .
In contrast to less restricted theories using Generalized Alignment , <tag name="FOCUS" value="start"/>OTP<tag name="FOCUS" value="end"/> 's optimal surface forms can be generated with <tag name="TECHNIQUE" value="start"/>finite-state methods<tag name="TECHNIQUE" value="end"/> adapted from -LRB- Ellison , 1994 -RRB- .
Unfortunately these methods take time exponential on the size of the grammar .
Indeed the <tag name="DOMAIN" value="start"/>generation<tag name="DOMAIN" value="end"/> problem is shown NP-complete in this sense .
However , techniques are discussed for making Ellison 's approach fast in the typical case , including a simple trick that alone provides a 100-fold speedup on a grammar fragment of moderate size .
One avenue for future improvements is a new <tag name="TECHNIQUE" value="start"/>finite-state<tag name="TECHNIQUE" value="end"/> notion , `` <tag name="TECHNIQUE" value="start"/>factored automata<tag name="TECHNIQUE" value="end"/> , '' where regular languages are represented compactly via formal intersections N ~ = IAi of FSAs .


##P08-1086
<tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Distributed Word Clustering<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> for <tag name="FOCUS" value="start"/>Large Scale Class-Based Language Modeling in <tag name="DOMAIN" value="start"/>Machine Translation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
In <tag name="FOCUS" value="start"/>statistical language modeling<tag name="FOCUS" value="end"/> , one technique to reduce the problematic effects of data sparsity is to partition the vocabulary into equivalence classes .
In this paper we investigate the effects of applying such a technique to higherorder <tag name="TECHNIQUE" value="start"/> n-gram <tag name="TECHNIQUE" value="end"/>  models trained on large corpora .
We introduce a modification of the <tag name="TECHNIQUE" value="start"/>exchange clustering <tag name="TECHNIQUE" value="end"/> algorithm with improved efficiency for certain <tag name="TECHNIQUE" value="start"/>partially class-based<tag name="TECHNIQUE" value="end"/> models and a <tag name="TECHNIQUE" value="start"/>distributed<tag name="TECHNIQUE" value="end"/> version of this algorithm to efficiently obtain automatic <tag name="TECHNIQUE" value="start"/> word classifications<tag name="TECHNIQUE" value="end"/> for large vocabularies -LRB- -RRB- 1 million words -RRB- using such large training corpora -LRB- -RRB- 30 billion tokens -RRB- .
The resulting clusterings are then used in <tag name="TECHNIQUE" value="start"/> training partially class-based language <tag name="TECHNIQUE" value="end"/> models .
We show that combining them with <tag name="TECHNIQUE" value="start"/>wordbased n-gram <tag name="TECHNIQUE" value="end"/> models in the <tag name="TECHNIQUE" value="start"/>log-linear <tag name="TECHNIQUE" value="end"/> model of a state-of-the-art <tag name="DOMAIN" value="start"/>statistical machine translation<tag name="DOMAIN" value="end"/> system leads to improvements in translation quality as indicated by the BLEU score .


##W01-1413
Using The <tag name="TECHNIQUE" value="start"/> Web<tag name="TECHNIQUE" value="end"/> As A <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Bilingual Dictionary<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We present a system for extracting an English <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> of a given Japanese technical term by collecting and scoring <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> candidates from the <tag name="TECHNIQUE" value="start"/> web<tag name="TECHNIQUE" value="end"/> .
We first show that there are a lot of partially bilingual documents in the web that could be useful for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>term translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , discovered by using a commercial technical term <tag name="TECHNIQUE" value="start"/>dictionary<tag name="TECHNIQUE" value="end"/> and an <tag name="TECHNIQUE" value="start"/>Internet search engine<tag name="TECHNIQUE" value="end"/> .
We then present an algorithm for obtaining <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/> translation<tag name="FOCUS" value="end"/> <tag name="DOMAIN" value="end"/> candidates based on the distance of Japanese and English terms in <tag name="TECHNIQUE" value="start"/>web documents<tag name="TECHNIQUE" value="end"/> , and report the results of a preliminary experiment .


##N03-2022
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Semantic Extraction<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> With <tag name="TECHNIQUE" value="start"/>Wide-Coverage Lexical Resources<tag name="TECHNIQUE" value="end"/> .
We report on results of <tag name="TECHNIQUE" value="start"/>combining graphical modeling techniques<tag name="TECHNIQUE" value="end"/> with Information Extraction resources -LRB- <tag name="TECHNIQUE" value="start"/>Pattern Dictionary and Lexicon<tag name="TECHNIQUE" value="end"/> -RRB- for both <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>frame and semantic role assignment<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
Our approach demonstrates the use of two human built knowledge bases -LRB- <tag name="TECHNIQUE" value="start"/>WordNet and FrameNet<tag name="TECHNIQUE" value="end"/> -RRB- for the task of <tag name="DOMAIN" value="start"/>semantic extraction<tag name="DOMAIN" value="end"/> .


##D09-1053
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Model Adaptation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> via <tag name="TECHNIQUE" value="start"/>Model Interpolation<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>Boosting<tag name="TECHNIQUE" value="end"/> for <tag name="DOMAIN" value="start"/>Web Search Ranking<tag name="DOMAIN" value="end"/> .
This paper explores two classes of <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>model adaptation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> methods for <tag name="DOMAIN" value="start"/>Web search ranking<tag name="DOMAIN" value="end"/> : <tag name="TECHNIQUE" value="start"/>Model Interpolation and error-driven learning <tag name="TECHNIQUE" value="end"/> approaches based on a <tag name="TECHNIQUE" value="start"/>boosting<tag name="TECHNIQUE" value="end"/> algorithm .
The results show that <tag name="TECHNIQUE" value="start"/>model interpolation<tag name="TECHNIQUE" value="end"/> , though simple , achieves the best results on all the open test sets where the test data is very different from the training data .
The <tag name="TECHNIQUE" value="start"/>tree-based boosting <tag name="TECHNIQUE" value="end"/> algorithm achieves the best performance on most of the closed test sets where the test data and the training data are similar , but its performance drops significantly on the open test sets due to the instability of trees .
Several methods are explored to improve the robustness of the algorithm , with limited success .


##J03-1006
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Weighted Deductive Parsing<tag name="DOMAIN" value="end"/> And <tag name="TECHNIQUE" value="start"/>Knuth 's Algorithm<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> .
We discuss <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>weighted deductive parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> and consider the problem of finding the derivation with the lowest weight .
We show that <tag name="TECHNIQUE" value="start"/>Knuth 's generalization of Dijkstra 's <tag name="TECHNIQUE" value="end"/> algorithm for the shortestpath problem offers a general method to solve this problem .
Our approach is modular in the sense that <tag name="TECHNIQUE" value="start"/>Knuth 's algorithm<tag name="TECHNIQUE" value="end"/> is formulated independently from the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>weighted deduction<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> system .


##W06-1646
<tag name="TECHNIQUE" value="start"/>Corrective <tag name="TECHNIQUE" value="end"/> Models For <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Speech Recognition Of Inflected Languages<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
This paper presents a corrective model for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>speech recognition of inflected languages<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
The model , based on a <tag name="TECHNIQUE" value="start"/>discriminative<tag name="TECHNIQUE" value="end"/>  framework , incorporates <tag name="TECHNIQUE" value="start"/>word ngrams<tag name="TECHNIQUE" value="end"/> features as well as <tag name="TECHNIQUE" value="start"/>factored morphological<tag name="TECHNIQUE" value="end"/> features , providing error reduction over the model based solely on word n-gram features .
Experiments on a large vocabulary task , namely the Czech portion of the MALACH corpus , demonstrate performance gain of about 1.1 -- 1.5 % absolute in word error rate , wherein morphological features contribute about a third of the improvement .
A simple feature selection mechanism based on <tag name="TECHNIQUE" value="start"/>Ï2 statistics<tag name="TECHNIQUE" value="end"/> is shown to be effective in reducing the number of features by about 70 % without any loss in performance , making it feasible to explore yet larger feature spaces .


##P98-1032
Automated <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Scoring<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Using A <tag name="TECHNIQUE" value="start"/>Hybrid Feature Identification<tag name="TECHNIQUE" value="end"/> Technique .
This study exploits <tag name="TECHNIQUE" value="start"/>statistical redundancy<tag name="TECHNIQUE" value="end"/> inherent in natural language to automatically predict <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>scores for essays<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We use a <tag name="TECHNIQUE" value="start"/>hybrid feature identification<tag name="TECHNIQUE" value="end"/> method , including <tag name="TECHNIQUE" value="start"/>syntactic structure analysis<tag name="TECHNIQUE" value="end"/> , <tag name="TECHNIQUE" value="start"/>rhetorical structure analysis<tag name="TECHNIQUE" value="end"/> , and <tag name="TECHNIQUE" value="start"/>topical analysis<tag name="TECHNIQUE" value="end"/> , to <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>score essay<tag name="FOCUS" value="end"/> <tag name="DOMAIN" value="end"/> responses from test-takers of the Graduate Management Admissions Test -LRB- GMAT -RRB- and the Test of Written English -LRB- TWE -RRB- .
For each essay question , a <tag name="TECHNIQUE" value="start"/>stepwise linear regression analysis<tag name="TECHNIQUE" value="end"/> is run on a training set -LRB- sample of human scored essay responses -RRB- to extract a weighted set of predictive features for each test question .
Score prediction for cross-validation sets is calculated from the set of predictive features .
Exact or adjacent agreement between the Electronic Essay Rater -LRB- e-rater -RRB- score predictions and human rater scores ranged from 87 % to 94 % across the 15 test questions .


##W09-2010
An <tag name="TECHNIQUE" value="start"/>Unsupervised <tag name="TECHNIQUE" value="end"/>  Model for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Text Message Normalization<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
Cell phone text messaging users express themselves briefly and colloquially using a variety of creative forms .
We analyze a sample of creative , non-standard text message word forms to determine frequent word formation processes in texting language .
Drawing on these observations , we construct an <tag name="TECHNIQUE" value="start"/>unsupervised noisy-channel <tag name="TECHNIQUE" value="end"/> model for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>text message normalization<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
On a test set of 303 text message forms that differ from their standard form , our model achieves 59 % accuracy , which is on par with the best supervised results reported on this dataset .


##P09-1077
Automatic <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/> sense prediction for implicit discourse relations <tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> in text .
We present a series of experiments on automatically identifying the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/> sense of implicit discourse relations<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , i.e. relations that are not marked with a discourse connective such as `` but '' or `` because '' .
We work with a corpus of implicit relations present in newspaper text and report results on a test set that is representative of the naturally occurring distribution of senses .
We use several <tag name="TECHNIQUE" value="start"/>linguistically informed features<tag name="TECHNIQUE" value="end"/> , including  <tag name="TECHNIQUE" value="start"/> polarity tags , Levin verb classes , length of verb phrases , modality , context , and lexical features<tag name="TECHNIQUE" value="end"/> .
In addition , we revisit past approaches using lexical pairs from unannotated text as features , explain some of their shortcomings and propose modifications .
Our best combination of features outperforms the baseline from data intensive approaches by 4 % for comparison and 16 % for contingency .


##W98-0602
A <tag name="TECHNIQUE" value="start"/>Dynamic Temporal Logic Of Events , Intervals And States<tag name="TECHNIQUE" value="end"/> For <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Nominalization<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> In Natural Language .
The interpretation of <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>nominalized expressions<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> in English poses several problems .
First , it must be explained how their meanings are derived from the meanings of the underlying verbs .
Second , different forms of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>nominalizations<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> differ in their semantic behavior .
Finally , aspectual restrictions which exist for ingofnominals must be explained .
The solution to be proposed is based on the assumption that <tag name="TECHNIQUE" value="start"/>non-stative verbs<tag name="TECHNIQUE" value="end"/> denote changes .
Changes can be conceived of in two different ways , either as objects which bring about a particular result or as relations between states .
A <tag name="TECHNIQUE" value="start"/>dynamic structure of events , intervals and states<tag name="TECHNIQUE" value="end"/> is defined in which both perspectives can be expressed by means of sorting the universe D. The basic idea is to augment a transition system for <tag name="TECHNIQUE" value="start"/>Dynamic Logic<tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/>DL<tag name="TECHNIQUE" value="end"/> -RRB- by a further domain of events such that programs from DL can be described either as objects or relations between states .
The interpretation of verbs is based on the second perspective : they denote -LRB- generalized -RRB- relations between states .
The interpretation of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>nominalized expressions<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> uses the first perspective : they denote changes as objects .
Different forms of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>nominalizations<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> denote different sorts of objects which are systematically related to the denotation of the underlying verb .


##I08-2088
Method of <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Selecting Training Data<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> to Build a Compact and Efficient <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Translation <tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> Model .
Target task matched parallel corpora are required for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>statistical translation<tag name="FOCUS" value="end"/> <tag name="DOMAIN" value="end"/> model training .
However , training corpora sometimes include both target task matched and unmatched sentences .
In such a case , training set selection can reduce the size of the translation model .
In this paper , we propose a <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>training set selection<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> method for <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>translation <tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> model training using <tag name="TECHNIQUE" value="start"/>linear<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> model interpolation and a <tag name="TECHNIQUE" value="start"/>language model<tag name="TECHNIQUE" value="end"/> technique .
According to the experimental results , the proposed method reduces the translation model size by 50 % and improves BLEU score by 1.76 % in comparison with a baseline training corpus usage .


##C04-1153
<tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Learning<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>Greek Verb Complements<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> : Addressing The <tag name="FOCUS" value="start"/>Class Imbalance<tag name="FOCUS" value="end"/> .
Imbalanced training sets , where one class is heavily underrepresented compared to the others , have a bad effect on the classification of rare class instances .
We apply <tag name="TECHNIQUE" value="start"/>One-sided Sampling<tag name="TECHNIQUE" value="end"/> for the first time to a <tag name="DOMAIN" value="start"/>lexical acquisition task<tag name="DOMAIN" value="end"/> -LRB- learning verb complements from Modern Greek corpora -RRB- to remove redundant and misleading training examples of verb nondependents and thereby balance our training set .
We experiment with well-known <tag name="TECHNIQUE" value="start"/>learning <tag name="TECHNIQUE" value="end"/> algorithms to classify new examples .
Performance improves up to 22 % in recall and 15 % in precision after balancing the dataset 1 .


##H93-1034
Efficient <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/> Collaborative Discourse<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> : A Theory And Its Implementation .
An architecture for <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>voice dialogue machines<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> is described with emphasis on the problem solving and high level decision making mechanisms .
The architecture provides facilities for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>generating voice interactions<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> aimed at cooperative human-machine problem solving .
It assumes that the dialogue will consist of a series of local selfconsistent subdialogues each aimed at subgoals related to the overall task .
The discourse may consist of a set of such subdiaiogues with jumps from one subdialogue to the other in a search for a successful conclusion .
The architecture maintains a <tag name="TECHNIQUE" value="start"/>user model<tag name="TECHNIQUE" value="end"/> to assure that interactions properly account for the level of competence of the user , and it includes an ability for the machine to take the initiative or yield the initiative to the user .
It uses <tag name="TECHNIQUE" value="start"/>expectation <tag name="TECHNIQUE" value="end"/> from the dialogue processor to aid in the correction of errors from the speech recognizer .


##W00-0702
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Corpus-Based Grammar Specialization<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
Broad-coverage grammars tend to be highly ambiguous .
When such grammars are used in a restricted domain , it may be desirable to specialize them , in effect trading some coverage for a reduction in ambiguity .
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Grammar specialization<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> is here given a novel formulation as an <tag name="TECHNIQUE" value="start"/>optimization <tag name="TECHNIQUE" value="end"/> problem , in which the search is guided by a global measure combining coverage , ambiguity and grammar size .
The method , applicable to any <tag name="DOMAIN" value="start"/>unification grammar<tag name="DOMAIN" value="end"/> with a phrasestructure backbone , is shown to be effective in specializing a broad-coverage <tag name="DOMAIN" value="start"/> LFG for French<tag name="DOMAIN" value="end"/> .


##W08-0303
<tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Discriminative<tag name="TECHNIQUE" value="end"/><tag name="DOMAIN" value="start"/> Word Alignment<tag name="DOMAIN" value="end"/> via <tag name="TECHNIQUE" value="start"/>Alignment Matrix <tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> Modeling .
In this paper a new <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>discriminative<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>  word alignment<tag name="DOMAIN" value="end"/> <tag name="FOCUS" value="end"/> method is presented .
This approach models directly the <tag name="TECHNIQUE" value="start"/> alignment matrix by a conditional random field -LRB- CRF -RRB-<tag name="TECHNIQUE" value="end"/> and so no restrictions to the alignments have to be made .
Furthermore , it is easy to add features and so all available information can be used .
Since the structure of the <tag name="TECHNIQUE" value="start"/>CRFs<tag name="TECHNIQUE" value="end"/> can get complex , the inference can only be done approximately and the standard algorithms had to be adapted .
In addition , different methods to train the model have been developed .
Using this approach the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>alignment<tag name="FOCUS" value="end"/> <tag name="DOMAIN" value="end"/> quality could be improved by up to 23 percent for 3 different language pairs compared to a combination of both IBM4alignments .
Furthermore the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>word alignment<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> was used to generate new phrase tables .
These could improve the translation quality significantly .


##N03-3006
A Low-Complexity , Broad-Coverage <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Probabilistic<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>Dependency Parser<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> For English .
Large-scale <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> is still a complex and timeconsuming process , often so much that it is infeasible in real-world applications .
The <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>parsing<tag name="FOCUS" value="end"/> <tag name="DOMAIN" value="end"/> system described here addresses this problem by combining <tag name="TECHNIQUE" value="start"/>finite-state<tag name="TECHNIQUE" value="end"/> approaches , <tag name="TECHNIQUE" value="start"/>statistical<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>parsing<tag name="DOMAIN" value="end"/> techniques and engineering knowledge , thus keeping parsing complexity as low as possible at the cost of a slight decrease in performance .
The parser is robust and fast and at the same time based on strong linguistic foundations .


##P84-1024
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Semantic Interpretation<tag name="DOMAIN" value="end"/> Using <tag name="TECHNIQUE" value="start"/>KL-One<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> .
This paper presents extensions to the work of Bobrow and Webber -LRB- Bobrow & Webber 80a , Bobrow & Webber 80b -RRB- on <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>semantic interpretation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> using <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>KL-ONE<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> to represent knowledge .
The approach is based on an <tag name="TECHNIQUE" value="start"/>extended case frame <tag name="TECHNIQUE" value="end"/> formalism applicable to all types of phrases , not just clauses .
The frames are used to recognize semantically acceptable phrases , identify their structure , and , relate them to their meaning representation through <tag name="TECHNIQUE" value="start"/>translation rules<tag name="TECHNIQUE" value="end"/> .
Approaches are presented for generating <tag name="TECHNIQUE" value="start"/>KL-ONE structures<tag name="TECHNIQUE" value="end"/> as the meaning of a sentence , for capturing <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>semantic generalizations<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> through abstract <tag name="TECHNIQUE" value="start"/>case frames<tag name="TECHNIQUE" value="end"/> , and for handling pronouns and relative clauses .


##W06-1322
A Computational Model Of <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Multi-Modal Grounding<tag name="TECHNIQUE" value="end"/> For <tag name="DOMAIN" value="start"/>Human Robot Interaction<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Dialog systems for mobile robots<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> operating in the real world should enable mixedinitiative dialog style , handle <tag name="TECHNIQUE" value="start"/>multi-modal<tag name="TECHNIQUE" value="end"/> information involved in the communication and be relatively independent of the domain knowledge .
Most dialog systems developed for mobile robots today , however , are often system-oriented and have limited capabilities .
We present an <tag name="TECHNIQUE" value="start"/>agentbased <tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>dialog<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> model that are specially designed for <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>human-robot interaction<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> and provide evidence for its efficiency with our implemented system .


##H91-1051
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Context Dependent Modeling Of Phones In Continuous Speech<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> Using <tag name="TECHNIQUE" value="start"/>Decision Trees<tag name="TECHNIQUE" value="end"/> .
In a <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>continuous speech recognition<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> system it is important to model the context dependent variations in the pronunciations of words .
In this paper we present an automatic method for <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>modeling phonological variation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> using <tag name="TECHNIQUE" value="start"/>decision trees<tag name="TECHNIQUE" value="end"/> .
For each phone we construct a <tag name="TECHNIQUE" value="start"/>decision tree<tag name="TECHNIQUE" value="end"/> that specifies the acoustic realization of the phone as a function of the context in which it appears .
Several thousand sentences from a natural language corpus spoken by several talkers are used to construct these <tag name="TECHNIQUE" value="start"/>decision trees<tag name="TECHNIQUE" value="end"/> .
Experimental results on a 5000-word vocabulary natural language <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>speech recognition<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> task are presented .


##P06-3012
<tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Focus<tag name="TECHNIQUE" value="end"/> To Emphasize <tag name="TECHNIQUE" value="start"/>Tone Structures<tag name="TECHNIQUE" value="end"/> For <tag name="DOMAIN" value="start"/>Prosodic Analysis In Spoken Language Generation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
We analyze the concept of <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>focus<tag name="TECHNIQUE" value="end"/> in speech<tag name="FOCUS" value="end"/> and the relationship between <tag name="FOCUS" value="start"/>focus and speech acts for <tag name="DOMAIN" value="start"/>prosodic generation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
We determinehowthespeaker 's utterancesare influenced by speaker 's intention .
The relationship between speech acts and focus information is used to define which parts of the sentence serve as the focus parts .
We propose the <tag name="FOCUS" value="start"/>Focus to Emphasize <tag name="TECHNIQUE" value="start"/>Tones<tag name="TECHNIQUE" value="end"/> -LRB- FET -RRB- structure<tag name="FOCUS" value="end"/> to analyze the focus components .
We also design the <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>FET grammar<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> to analyze the intonation patterns and produce tone marks as a result of our analysis .
We present a proof-of-the-concept working example to validate our proposal .
More comprehensive evaluations are part of our current work .


##C90-2003
Finding <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Equivalents : An Application Of <tag name="TECHNIQUE" value="start"/>Grammatical Metaphor<tag name="TECHNIQUE" value="end"/> .
In this paper I describe how a significant class of cases that would involve -LRB- possibly complex -RRB- structural transfer in nmchine <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>translation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> can be handled avoiding transfer .
This is achieved by applying a <tag name="TECHNIQUE" value="start"/>semantic organization<tag name="TECHNIQUE" value="end"/> developed for monolingual text generation that is sufficiently abstract to remain invariant , within theoretically specifiable limits , across different languages .
The further application of a mechanism motivated from within monolingual text generation , ` <tag name="TECHNIQUE" value="start"/>grammatical metaphor<tag name="TECHNIQUE" value="end"/> ' , then allows candidate appropriate translations to be isolated .
The incorporation of these essentially monolingual mechanisms within the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>machine translation<tag name="FOCUS" value="end"/> <tag name="DOMAIN" value="end"/> process promises to significantly improve <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>translational<tag name="FOCUS" value="end"/> <tag name="DOMAIN" value="end"/> capabilities ; examples of this are presented for English and German .


##P96-1022
<tag name="FOCUS" value="start"/>SEMHE<tag name="FOCUS" value="end"/> A Generalised <tag name="FOCUS" value="start"/>Two-Level System<tag name="FOCUS" value="end"/> .
This paper presents a generalised <tag name="FOCUS" value="start"/> twolevel<tag name="FOCUS" value="end"/> implementation which can handle <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>linear and non-linear morphological operations<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
An algorithm for the interpretation of <tag name="FOCUS" value="start"/>multi-tape two-level rules<tag name="FOCUS" value="end"/> is described .
In addition , a number of issues which arise when developing non-linear grammars are discussed with examples from Syriac .


##A00-1020
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Multilingual Coreference Resolution<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
In this paper we present a new , multilingual <tag name="TECHNIQUE" value="start"/>data-driven<tag name="TECHNIQUE" value="end"/> method for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>coreference resolution<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> as implemented in the SWIZZLE system .
The results obtained after training this system on a bilingual corpus of English and Romanian tagged texts , outperformed <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>coreference resolution<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> in each of the individual languages .


##P97-1036
<tag name="FOCUS" value="start"/> <tag name="TECHNIQUE" value="start"/>Unification-Based<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/> Multimodal Integration<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
Recent empirical research has shown conclusive advantages of <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>multimodal interaction<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> over speech-only interaction for mapbased tasks .
This paper describes a <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>multimodal language processing<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> architecture which supports interfaces allowing simultaneous input from <tag name="DOMAIN" value="start"/> speech and gesture recognition<tag name="DOMAIN" value="end"/> .
Integration of spoken and gestural input is driven by <tag name="TECHNIQUE" value="start"/>unification of typed feature structures <tag name="TECHNIQUE" value="end"/> representing the semantic contributions of the different modes .
This integration method allows the component modalities to mutually compensate for each others ' errors .
It is implemented in QuickSet , a multimodal -LRB- pen\/voice -RRB- system that enables users to set up and control distributed interactive simulations .


##P02-1025
A Study On <tag name="TECHNIQUE" value="start"/>Richer Syntactic Dependencies<tag name="TECHNIQUE" value="end"/> For <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Structured Language Modeling<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
We study the impact of <tag name="TECHNIQUE" value="start"/>richer syntactic dependencies<tag name="TECHNIQUE" value="end"/> on the performance of the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>structured language model<tag name="FOCUS" value="end"/> <tag name="DOMAIN" value="end"/> -LRB- <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>SLM<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> -RRB- along three dimensions : <tag name="DOMAIN" value="start"/>parsing<tag name="DOMAIN" value="end"/> accuracy -LRB- LP\/LR -RRB- , perplexity -LRB- PPL -RRB- and worderror-rate -LRB- WER , N-best re-scoring -RRB- .
We show that our models achieve an improvement in LP\/LR , PPL and\/or WER over the reported baseline results using the SLM on the UPenn Treebank and Wall Street Journal -LRB- WSJ -RRB- corpora , respectively .
Analysis of <tag name="DOMAIN" value="start"/>parsing<tag name="DOMAIN" value="end"/> performance shows correlation between the quality of the parser -LRB- as measured by precision\/recall -RRB- and the <tag name="DOMAIN" value="start"/>language model<tag name="DOMAIN" value="end"/> performance -LRB- PPL and WER -RRB- .
A remarkable fact is that the enriched <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>SLM<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> outperforms the baseline 3-gram model in terms of WER by 10 % when used in isolation as a second pass -LRB- N-best re-scoring -RRB- language model .


##W05-0710
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Classifying Amharic News Text<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> Using <tag name="TECHNIQUE" value="start"/>Self-Organizing Maps<tag name="TECHNIQUE" value="end"/> .
The paper addresses using <tag name="TECHNIQUE" value="start"/>artificial neural networks<tag name="TECHNIQUE" value="end"/> for <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>classification of Amharic news <tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> items .
Amharic is the language for countrywide communication in Ethiopia and has its own writing system containing extensive systematic redundancy .
It is quite dialectally diversified and probably representative of the languages of a continent that so far has received little attention within the language processing field .
The experiments investigated <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>document clustering<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> around user queries using <tag name="TECHNIQUE" value="start"/>SelfOrganizing Maps , an unsupervised learning neural network strategy<tag name="TECHNIQUE" value="end"/> .
The best ANN model showed a precision of 60.0 % when trying to cluster unseen data , and a 69.5 % precision when trying to classify it .


##H94-1083
Advanced <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Human-Computer Interface And Voice Processing Applications In Space<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
Much interest already exists in the electronics research community for developing and integrating speech technology to a variety of applications , ranging from voice-activated systems to automatic telephone transactions .
This interest is particularly true in the field of aerospace where the training and operational demands on the crew have significantly increased with the proliferation of technology .
Indeed , with advances in vehicule and robot automation , the role of the human operator has evolved from that of pilot\/driver and manual controller to supervisor and decision maker .
Lately , some effort has been expended to implement alternative modes of system control , but automatic speech recognition -LRB- ASR -RRB- and human-computer interaction -LRB- HCI -RRB- research have only recently extended to civilian aviation and space applications .
The purpose of this paper is to present the particularities of <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>operator-computer interaction <tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> in the unique conditions found in space .
The potential for <tag name="DOMAIN" value="start"/>voice control applications inside spacecraft<tag name="DOMAIN" value="end"/> is outlined and methods of integrating spoken-language interfaces onto operational space systems are suggested .


##W98-1412
<tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Abductive Reasoning<tag name="TECHNIQUE" value="end"/> For <tag name="DOMAIN" value="start"/>Syntactic Realization<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
<tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Abductive reasoning<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> is â¢ used in a bidirectional framework for <tag name="DOMAIN" value="start"/>syntactic realization and semantic interpretation<tag name="DOMAIN" value="end"/> .
The use of the framework is illustrated in a case study of <tag name="DOMAIN" value="start"/>sentence generation<tag name="DOMAIN" value="end"/> , where different syntactic forms are generated depending on the status of discourse information .
Examples are given involving three differen t syntactic constructions in German root clauses .


##P98-1010
A <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Memory-Based<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> Approach to Learning <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/> Shallow Natural Language Patterns<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
Recognizing <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>shallow linguistic patterns<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , such as basic syntactic relationships between words , is a common task in applied natural language and text processing .
The common practice for approaching this task is by tedious manual definition of possible pattern structures , often in the form of regular expressions or finite automata .
This paper presents a novel <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>memory-based learning<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> method that recognizes <tag name="DOMAIN" value="start"/> <tag name="FOCUS" value="start"/>shallow patterns<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> in new text based on a <tag name="TECHNIQUE" value="start"/>bracketed training corpus<tag name="TECHNIQUE" value="end"/> .
The training data are stored as-is , in efficient suffix-tree data structures .
Generalization is performed on-line at recognition time by comparing subsequences of the new text to positive and negative evidence in the corpus .
This way , no information in the training is lost , as can happen in other learning systems that construct a single generalized model at the time of training .
The paper presents experimental results for <tag name="DOMAIN" value="start"/>recognizing noun phrase , subject-verb and verb-object patterns in English<tag name="DOMAIN" value="end"/> .
Since the learning approach enables easy porting to new domains , we plan to apply it to syntactic patterns in other languages and to sub-language patterns for <tag name="DOMAIN" value="start"/>information extraction<tag name="DOMAIN" value="end"/> .


##W06-1630
<tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Unsupervised<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/> Named Entity Transliteration<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> Using <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Temporal And Phonetic Correlation<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> .
In this paper we investigate <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>unsupervised<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/> name transliteration<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> using comparable corpora , corpora where texts in the two languages deal in some of the same topics -- and therefore share references to named entities -- but are not translations of each other .
We present two distinct methods for <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>transliteration<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> , one approach using an <tag name="TECHNIQUE" value="start"/>unsupervised phonetic <tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>transliteration<tag name="DOMAIN" value="end"/> method , and the other using the <tag name="TECHNIQUE" value="start"/>temporal distribution<tag name="TECHNIQUE" value="end"/> of candidate pairs .
Each of these approaches works quite well , but by combining the approaches one can achieve even better results .
We believe that the novelty of our approach lies in the <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>phonetic-based scoring <tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> method , which is based on a combination of carefully crafted phonetic features , and empirical results from the pronunciation errors of second-language learners of English .
Unlike previous approaches to transliteration , this method can in principle work with any pair of languages in the absence of a training dictionary , provided one has an estimate of the pronunciation of words in text .


##J98-3005
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Generating Natural Language Summaries<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> From <tag name="TECHNIQUE" value="start"/>Multiple On-Line Sources<tag name="TECHNIQUE" value="end"/> .
We present a methodology for <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>summarization of news <tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> about current events in the form of briefings that include appropriate background -LRB- historical -RRB- information .
The system that we developed , <tag name="FOCUS" value="start"/>SUMMONS<tag name="FOCUS" value="end"/> , uses the output of systems developed for the DARPA Message Understanding Conferences to <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>generate summaries<tag name="FOCUS" value="end"/> <tag name="DOMAIN" value="end"/> of multiple documents  on the same or related events , presenting similarities and differences , contradictions , and generalizations among sources of information .
We describe the various components of the system , showing how information from multiple articles is combined , organized into a paragraph , and finally , realized as English sentences .
A feature of our work is the <tag name="TECHNIQUE" value="start"/>extraction of descriptions of entities <tag name="TECHNIQUE" value="end"/> such as people and places for reuse to enhance a briefing .


##W04-1801
A <tag name="TECHNIQUE" value="start"/>Lexico-Semantic <tag name="TECHNIQUE" value="end"/> Approach To The <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Structuring Of Terminology<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
This paper discusses a number of implications of using either a <tag name="TECHNIQUE" value="start"/>conceptual<tag name="TECHNIQUE" value="end"/> approach or a <tag name="TECHNIQUE" value="start"/>lexico-semantic<tag name="TECHNIQUE" value="end"/> approach to <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>terminology structuring<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , especially for interpreting data supplied by corpora for the purpose of building  <tag name="DOMAIN" value="start"/>specialized dictionaries<tag name="DOMAIN" value="end"/> .
A simple example , i.e. , program , will serve as a basis for showing how relationships between terms are captured in both approaches .
My aim is to demonstrate that truly conceptual approaches do not allow a flexible integration of terms and relationships between terms and that <tag name="TECHNIQUE" value="start"/>lexico-semantic approaches<tag name="TECHNIQUE" value="end"/> are more compatible with data gathered from corpora .
I will also discuss some of the implications these approaches have for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>computational terminology<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> and other corpus-based terminological endeavors .


##W07-2102
UTD-SRL : A Pipeline Architecture for Extracting <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/> Frame Semantic Structures<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
This paper describes our system for the task of extracting <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/> frame semantic structures <tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/>  in SemEval 2007 .
The system architecture uses two types of <tag name="TECHNIQUE" value="start"/>learning <tag name="TECHNIQUE" value="end"/> models in each part of the task : <tag name="TECHNIQUE" value="start"/>Support Vector Machines -LRB- SVM -RRB- and Maximum Entropy -LRB- ME -RRB-<tag name="TECHNIQUE" value="end"/> .
Designed as a pipeline of classi ers , the <tag name="DOMAIN" value="start"/>semantic parsing<tag name="DOMAIN" value="end"/> system obtained competitive precision scores on the test data .


##A92-1029
<tag name="FOCUS" value="start"/>Compound Nouns<tag name="FOCUS" value="end"/> In A <tag name="TECHNIQUE" value="start"/>Unification-Based<tag name="TECHNIQUE" value="end"/> <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/> MT System<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
This paper describes an approach to the treatment of <tag name="FOCUS" value="start"/>nominal compounds in a <tag name="DOMAIN" value="start"/>machine translation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> project employing a modern <tag name="TECHNIQUE" value="start"/>unification-based<tag name="TECHNIQUE" value="end"/> system  .
General problems connected with the analysis of compounds are briefly reviewed , and the project , for the automatic <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> of Swiss avalanche bulletins , is introduced .
Avalanche bulletins deal with a limited semantic domain and employ a sublanguage in which nominal compounds occur frequently .
These and other properties of the texts affect the treatment of compounds , permitting certain simplifications , while leaving a number of possible alternative analyses .
We discuss the different problems involving the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> of compounds between German and French , and show how the computational environment in use permits two different approaches to the problem : an <tag name="TECHNIQUE" value="start"/>interlingua-based<tag name="TECHNIQUE" value="end"/> approach and a <tag name="TECHNIQUE" value="start"/>transfer-based<tag name="TECHNIQUE" value="end"/> approach .
Finally , wc evaluate these approaches with respect to linguistic and computational considerations applicable in a <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>MT-system<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> dealing with a limited semantic domain and describe the solution that has actually been implemented .


##J93-2006
Coping With Ambiguity And Unknown Words Through <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Probabilistic <tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> Models .
From spring 1990 through fall 1991 , we performed a battery of small experiments to test the effectiveness of supplementing knowledge-based techniques with <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>probabilistic<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> models .
This paper reports our experiments in predicting <tag name="DOMAIN" value="start"/>parts of speech <tag name="DOMAIN" value="end"/> of highly ambiguous words , predicting the intended interpretation of an utterance when more than one interpretation satisfies all known syntactic and semantic constraints , and learning <tag name="DOMAIN" value="start"/>case frame information for verbs<tag name="DOMAIN" value="end"/> from example uses .
From these experiments , we are convinced that <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>probabilistic<tag name="FOCUS" value="end"/> <tag name="TECHNIQUE" value="end"/> models based on annotated corpora can effectively reduce the ambiguity in processing text and can be used to acquire lexical information from a corpu> , by supplementing knowledge-based techniques .
Based on the results of those experiments , we have constructed a new natural language system -LRB- PLUM -RRB- for extracting data from text , e.g. , newswire text .


##N04-1007
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Answering Definition Questions<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> With Multiple Knowledge Sources .
Definition questions represent a largely unexplored area of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>question answering<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> -- they are different from factoid questions in that the goal is to return as many relevant `` nuggets '' of information about a concept as possible .
We describe a <tag name="TECHNIQUE" value="start"/>multi-strategy <tag name="TECHNIQUE" value="end"/> approach to <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>answering such questions<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> using a database constructed offline with <tag name="TECHNIQUE" value="start"/>surface patterns<tag name="TECHNIQUE" value="end"/> , a Webbased <tag name="TECHNIQUE" value="start"/>dictionary <tag name="TECHNIQUE" value="end"/>, and an off-the-shelf <tag name="TECHNIQUE" value="start"/>document retriever<tag name="TECHNIQUE" value="end"/> .
Results are presented from component-level evaluation and from an endto-end evaluation of our implemented system at the TREC 2003 <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Question Answering<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Track .


##D09-1093
Using the <tag name="TECHNIQUE" value="start"/>Web<tag name="TECHNIQUE" value="end"/> for <tag name="FOCUS" value="start"/> Language Independent <tag name="DOMAIN" value="start"/>Spellchecking and Autocorrection<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
We have designed , implemented and evaluated an end-to-end system <tag name="DOMAIN" value="start"/> <tag name="FOCUS" value="start"/>spellchecking and autocorrection<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> system that does not require any manually annotated training data .
The <tag name="TECHNIQUE" value="start"/>World Wide Web<tag name="TECHNIQUE" value="end"/> is used as a large noisy corpus from which we infer knowledge about misspellings and word usage .
This is used to build an <tag name="TECHNIQUE" value="start"/>error model<tag name="TECHNIQUE" value="end"/> and an <tag name="TECHNIQUE" value="start"/>n-gram language model<tag name="TECHNIQUE" value="end"/> .
A small secondary set of news texts with artificially inserted misspellings are used to tune confidence classifiers .
Because no manual annotation is required , our system can easily be instantiated for new languages .
When evaluated on human typed data with real misspellings in English and German , our web-based systems outperform baselines which use candidate corrections based on hand-curated dictionaries .
Our system achieves 3.8 % total error rate in English .
We show similar improvements in preliminary results on artificial data for Russian and Arabic .


##C92-2070
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Word-Sense Disambiguation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Using <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Statistical Models Of Roget 's Categories <tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> Trained On Large Corpora .
This paper describes a program that <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>disambignates English word senses<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> in unrestricted text using <tag name="TECHNIQUE" value="start"/>statistical<tag name="TECHNIQUE" value="end"/> models of the major <tag name="TECHNIQUE" value="start"/>Roget 's Thesaurus<tag name="TECHNIQUE" value="end"/> categories .
Roget 's categories serve as approximations of conceptual classes .
The categories listed for a word in Roger 's index tend to correspond to sense distinctions ; thus selecting the most likely category provides a useful level of sense disambiguatiou .
The selection of categories is accomplished by identifying and weighting words that are indicative of each category when seen in context , using a <tag name="TECHNIQUE" value="start"/>Bayesian theoretical <tag name="TECHNIQUE" value="end"/> framework .
Other statistical approaches have required special corpora or hand-labeled training examples for much of the lexicon .
Our use of class models overcomes this knowledge acquisition bottleneck , enabling training on unresUicted monolingual text without human intervention .
Applied to the 10 million word Grolier 's Encyclopedia , the system correctly disambiguated 92 % of the instances of 12 polysemous words that have been previously studied in the literature .


##W04-2403
A <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Semantic Kernel<tag name="TECHNIQUE" value="end"/> For <tag name="DOMAIN" value="start"/>Predicate Argument Classification<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
Automatically deriving semantic structures from text is a challenging task for machine learning .
The flat feature representations , usually used in learning models , can only partially describe structured data .
This makes difficult the processing of the semantic information that is embedded into parse-trees .
In this paper a new <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/> kernel<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/>  for automatic <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>classification of predicate arguments<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/>has been designed and experimented .
It is based on subparse-trees annotated with predicate argument information from PropBank corpus .
This <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>kernel<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> , exploiting the convolution properties of the <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>parse-tree kernel<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> , enables us to learn which syntactic structures can be associated with the arguments defined in PropBank .
<tag name="TECHNIQUE" value="start"/>Support Vector Machines<tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/>SVMs<tag name="TECHNIQUE" value="end"/> -RRB- using such a <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>kernel<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>classify arguments<tag name="DOMAIN" value="end"/> with a better accuracy than SVMs based on linear kernel .


##W06-3918
The<tag name="FOCUS" value="start"/> Alligator theorem prover for <tag name="TECHNIQUE" value="start"/>dependent type systems<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> : Description and proof samples .
This paper introduces the <tag name="FOCUS" value="start"/>Alligator theorem prover for <tag name="TECHNIQUE" value="start"/>Dependent Type Systems<tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/>dts<tag name="TECHNIQUE" value="end"/> -RRB-<tag name="FOCUS" value="end"/> .
We start with highlighting a number of properties of <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>dts<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> that make them specifically suited for computational semantics .
We then briefly introduce <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>dts<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> and our implementation .
The paper concludes with an example of a dts proof that illustrates the suitability of dts for modelling <tag name="DOMAIN" value="start"/> anaphora resolution<tag name="DOMAIN" value="end"/> .


##W00-1413
The Hyperonym Problem Revisited : <tag name="TECHNIQUE" value="start"/>Conceptual And Lexical Hierarchies<tag name="TECHNIQUE" value="end"/> In <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Language Generation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
When a lexical item is selected in the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>language production<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> process , it needs to be explained why none of its superordinates gets selected instead , since their applicability conditions are fulfilled all the same .
This question has received much attention in cognitive modelling and not as much in other branches of NLG .
This paper describes the various approaches taken , discusses the reasons why they are so different , and argues that <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>production<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> models using  <tag name="TECHNIQUE" value="start"/>symbolic representations<tag name="TECHNIQUE" value="end"/> should make a distinction between <tag name="TECHNIQUE" value="start"/>conceptual and lexical hierarchies<tag name="TECHNIQUE" value="end"/> , which can be organized along fixed levels as studied in -LRB- some branches of -RRB- lexical semantics .


##J04-3002
Learning <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Subjective Language<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Subjectivity<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> in natural language refers to aspects of language used to express opinions , evaluations , and speculations .
There are numerous natural language processing applications for which <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>subjectivity<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> analysis is relevant , including information extraction and text categorization .
The goal of this work is learning <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>subjective language<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> from corpora .
Clues of subjectivity are generated and tested , including low-frequency words , <tag name="TECHNIQUE" value="start"/>collocations<tag name="TECHNIQUE" value="end"/> , and adjectives and verbs identified using  <tag name="TECHNIQUE" value="start"/> distributional similarity<tag name="TECHNIQUE" value="end"/> .
The features are also examined working together in concert .
The features , generated from different data sets using different procedures , exhibit consistency in performance in that they all do better and worse on the same data sets .
In addition , this article shows that the density of subjectivity clues in the surrounding context strongly affects how likely it is that a word is subjective , and it provides the results of an annotation study assessing the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>subjectivity<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> of sentences with <tag name="TECHNIQUE" value="start"/>high-density features<tag name="TECHNIQUE" value="end"/> .
Finally , the clues are used to perform <tag name="DOMAIN" value="start"/>opinion piece recognition <tag name="DOMAIN" value="end"/> -LRB- a type of <tag name="DOMAIN" value="start"/>text categorization<tag name="DOMAIN" value="end"/> and <tag name="DOMAIN" value="start"/>genre detection<tag name="DOMAIN" value="end"/> -RRB- to demonstrate the utility of the knowledge acquired in this article .


##E89-1034
French <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Order<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> Without Order .
To account for the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>semi-free word order<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> of French , <tag name="TECHNIQUE" value="start"/>Unification Categorial Grammar<tag name="TECHNIQUE" value="end"/> is extended in two ways .
First , verbal valencies are contained in a set rather than in a list .
Second , type-raised NP 's are described as two-sided functors .
The new framework does not overgenerate i.e. , it accepts all and only the sentences which are grammatical .
This follows partly from the elimination of false lexical ambiguities - i.e. , ambiguities introduced in order to account for all the possible positions a word can be in within a sentence - and partly from a system of features constraining the possible combinations .


##E93-1013
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>LFG Semantics<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Via <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Constraints<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> .
Semantic theories of natural language associate meanings with utterances by providing meanings for lexical items and rules for determining the meaning of larger units given the meanings of their parts .
Traditionally , meanings are combined via function composition , which works well when constituent structure trees are used to guide semantic composition .
More recently , the functional structure of <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>LFG<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> has been used to provide the syntactic information necessary for constraining derivations of meaning in a cross-linguistically uniform format .
It has been difficult , however , to reconcile this approach with the combination of meanings by function composition .
In contrast to compositional approaches , we present a <tag name="TECHNIQUE" value="start"/>deductive <tag name="TECHNIQUE" value="end"/> approach to <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>assembling meanings<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , based on reasoning with <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>constraints<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> , which meshes well with the unordered nature of information in the functional structure .
Our use of <tag name="TECHNIQUE" value="start"/>linear logic<tag name="TECHNIQUE" value="end"/> as a ` glue ' for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>assembling meanings<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> also allows for a coherent treatment of modification as well as of the LFG requirements of completeness and coherence .


##C00-2087
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Interaction Grammars<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Interaction Grammars<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> -LRB- <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>IG<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> -RRB- are a new linguistic formalism which is based on descriptions of under ~ specified trees in the fl ` amework of <tag name="TECHNIQUE" value="start"/>intuitionistic linear logic<tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/>ILL<tag name="TECHNIQUE" value="end"/> -RRB- .
Syntactic composition , which is expressed by deduction in <tag name="TECHNIQUE" value="start"/> linear logic<tag name="TECHNIQUE" value="end"/> , is controlled by a system of polarized features .
In this way , <tag name="DOMAIN" value="start"/>parsing<tag name="DOMAIN" value="end"/> amounts to generating models of tree descriptions and it is implemented as a <tag name="TECHNIQUE" value="start"/>constraint satisfaction problem<tag name="TECHNIQUE" value="end"/> .


##N07-3001
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Query Expansion<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> Using Domain Information in <tag name="TECHNIQUE" value="start"/>Compounds<tag name="TECHNIQUE" value="end"/> .
This paper describes a <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>query expansion<tag name="DOMAIN" value="end"/> strategy for <tag name="DOMAIN" value="start"/>domain specific information retrieval<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
Components of <tag name="TECHNIQUE" value="start"/>compounds<tag name="TECHNIQUE" value="end"/> are used selectively .
Only parts belonging to the same domain as the <tag name="TECHNIQUE" value="start"/>compound<tag name="TECHNIQUE" value="end"/> itself will be used in expanded queries .


##P05-3014
<tag name="FOCUS" value="start"/>SenseLearner : <tag name="DOMAIN" value="start"/>Word Sense Disambiguation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> For All Words In Unrestricted Text .
This paper describes <tag name="FOCUS" value="start"/>SENSELEARNER<tag name="FOCUS" value="end"/> -- a <tag name="TECHNIQUE" value="start"/>minimally supervised<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>word sense disambiguation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> system that attempts to disambiguate all content words in a text using <tag name="TECHNIQUE" value="start"/>WordNet senses<tag name="TECHNIQUE" value="end"/> .
We evaluate the accuracy of <tag name="FOCUS" value="start"/>SENSELEARNER<tag name="FOCUS" value="end"/> on several standard sense-annotated data sets , and show that it compares favorably with the best results reported during the recent SENSEVAL evaluations .


##W93-0227
<tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Rhetoric<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> As Knowledge .
A proper assessment of the relation between discourse structure and speaker 's communicative intentions requires a better understanding of communicative intentions .
This contribution proposes that there is a crucial difference between intending the hearer to entertain a certain belief -LRB- or desire , or intention -RRB- , and intending to affect the strength with which the hearer entertains the belief -LRB- or desire , or intention -RRB- .
<tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Rhetoric<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> , if defined as a body of knowledge about how discourse structure affects the strength with which a discourse participant entertains beliefs , desires , and intentions , can be seen to play a precise and crucial role in the planning of  <tag name="DOMAIN" value="start"/> discourse<tag name="DOMAIN" value="end"/> .


##P94-1016
Interleaving  Syntax And Semantics In An Efficient <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Bottom-Up Parser<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> .
We describe an efficient <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>bottom-up<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>parser<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> that interleaves syntactic and semantic structure building .
Two techniques are presented for reducing search by reducing local ambiguity : Limited <tag name="TECHNIQUE" value="start"/>leftcontext constraints<tag name="TECHNIQUE" value="end"/> are used to reduce local syntactic ambiguity , and <tag name="TECHNIQUE" value="start"/>deferred sortal-constraint<tag name="TECHNIQUE" value="end"/> application is used to reduce local semantic ambiguity .
We experimentally evaluate these techniques , and show dramatic reductions in both number of chart edges and total <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> time .
The robust processing capabilities of the <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>parser<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> are demonstrated in its use in improving the accuracy of a <tag name="DOMAIN" value="start"/>speech recognizer<tag name="DOMAIN" value="end"/> .


##W04-0849
<tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Class-Based Collocations<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> For <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Word Sense Disambiguation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
This paper describes the <tag name="FOCUS" value="start"/>NMSU-Pitt-UNCA <tag name="DOMAIN" value="start"/>word-sense disambiguation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> system participating in the Senseval-3 English lexical sample  task .
The focus of the work is on using <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>semantic class-based collocations<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> to augment traditional word-based collocations .
Three separate sources of word relatedness are used for these collocations : 1 -RRB- <tag name="TECHNIQUE" value="start"/>WordNet hypernym relations<tag name="TECHNIQUE" value="end"/> ; 2 -RRB- <tag name="TECHNIQUE" value="start"/>cluster-based word similarity classes<tag name="TECHNIQUE" value="end"/> ; and 3 -RRB- <tag name="TECHNIQUE" value="start"/>dictionary definition analysis<tag name="TECHNIQUE" value="end"/> .


##I08-4015
The <tag name="TECHNIQUE" value="start"/>Character-based CRF<tag name="TECHNIQUE" value="end"/> <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Segmenter<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> of MSRA&NEU for the 4th Bakeoff .
This paper describes the <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Chinese Word Segmenter<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> for the fourth International Chinese Language Processing Bakeoff .
Base on <tag name="TECHNIQUE" value="start"/>Conditional Random Field <tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/>CRF<tag name="TECHNIQUE" value="end"/> -RRB- model , a basic segmenter is designed as a problem of <tag name="DOMAIN" value="start"/>character-based tagging<tag name="DOMAIN" value="end"/> .
To further improve the performance of our segmenter , we employ a <tag name="TECHNIQUE" value="start"/>word-based<tag name="TECHNIQUE" value="end"/> approach to increase the in-vocabulary -LRB- IV -RRB- word recall and a post-processing to increase the out-of-vocabulary -LRB- OOV -RRB- word recall .
We participate in the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>word segmentation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> closed test on all five corpora and our system achieved four second best and one the fifth in all the five corpora .


##N09-1018
<tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Jointly<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> Identifying <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Predicates , Arguments and Senses<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> using <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Markov Logic<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> .
In this paper we present a <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Markov Logic Network<tag name="TECHNIQUE" value="end"/> for <tag name="DOMAIN" value="start"/>Semantic Role Labelling<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> that jointly performs <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>predicate identification , frame disambiguation , argument identification and argument classification<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> for all predicates in a sentence .
Empirically we find that our approach is competitive : our best model would appear on par with the best entry in the CoNLL 2008 shared task open track , and at the 4th place of the closed track -- right behind the systems that use significantly better parsers to generate their input features .
Moreover , we observe that by fully capturing the complete <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>SRL<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> pipeline in a single <tag name="TECHNIQUE" value="start"/> probabilistic <tag name="TECHNIQUE" value="end"/> model we can achieve significant improvements over more isolated systems , in particular for out-of-domain data .
Finally , we show that despite the joint approach , our system is still efficient .


##D09-1068
Improving <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Web Search Relevance<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> with <tag name="TECHNIQUE" value="start"/>Semantic Features<tag name="TECHNIQUE" value="end"/> .
Most existing <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>information retrieval<tag name="FOCUS" value="end"/> <tag name="DOMAIN" value="end"/> -LRB- <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>IR<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> -RRB- systems do not take much advantage of natural language processing -LRB- NLP -RRB- techniques due to the complexity and limited observed effectiveness of applying NLP to IR .
In this paper , we demonstrate that substantial gains can be obtained over a strong baseline using <tag name="TECHNIQUE" value="start"/>NLP<tag name="TECHNIQUE" value="end"/>  techniques , if properly handled .
We propose a framework for deriving <tag name="TECHNIQUE" value="start"/>semantic text matching<tag name="TECHNIQUE" value="end"/> features from <tag name="TECHNIQUE" value="start"/>named entities<tag name="TECHNIQUE" value="end"/> identified in Web queries ; we then utilize these features in a <tag name="TECHNIQUE" value="start"/>supervised machine-learned<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>ranking<tag name="DOMAIN" value="end"/>  approach , applying a set of emerging <tag name="TECHNIQUE" value="start"/>machine learning<tag name="TECHNIQUE" value="end"/> techniques .
Our approach is especially useful for queries that contain multiple types of concepts .
Comparing to a major commercial Web search engine , we observe a substantial 4 % DCG5 gain over the affected queries .


##P09-4005
A <tag name="TECHNIQUE" value="start"/>Web-Based<tag name="TECHNIQUE" value="end"/>  <tag name="FOCUS" value="start"/> Interactive Computer Aided <tag name="DOMAIN" value="start"/>Translation <tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> Tool .
We developed <tag name="FOCUS" value="start"/>caitra<tag name="FOCUS" value="end"/> , a novel tool that aids human translators by -LRB- a -RRB- making suggestions for sentence completion in an<tag name="FOCUS" value="start"/> interactive <tag name="DOMAIN" value="start"/> machine translation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> setting , -LRB- b -RRB- providing alternative word and phrase translations , and -LRB- c -RRB- allowing them to postedit machine translation output .
The tool uses the <tag name="TECHNIQUE" value="start"/>Moses decoder<tag name="TECHNIQUE" value="end"/> , is implemented in Ruby on Rails and C + + and delivered over the web .


##W03-1901
Outline Of The <tag name="FOCUS" value="start"/>International Standard Linguistic Annotation <tag name="FOCUS" value="end"/> Framework .
This paper describes the outline of a <tag name="FOCUS" value="start"/>linguistic annotation <tag name="FOCUS" value="end"/> framework under development by ISO TC37 SC WG1-1 .
This international standard provides an architecture for the  creation , annotation , and manipulation of linguistic resources and processing software .
The goal is to provide maximum flexibility for encoders and annotators , while at the same time enabling interchange and re-use of annotated linguistic resources .
We describe here the outline of the standard for the purposes of enabling annotators to begin to explore how their schemes may map into the framework .


##N07-1035
Estimating the Reliability of  <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>MDP Policies<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> : a <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Confidence Interval<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> Approach .
Past approaches for using <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>reinforcement learning<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> to derive <tag name="DOMAIN" value="start"/>dialog<tag name="DOMAIN" value="end"/> control policies have assumed that there was enough collected data to derive a reliable policy .
In this paper we present a methodology for numerically constructing con dence intervals for the expected cumulative reward for a learned policy .
These intervals are used to -LRB- 1 -RRB-  better assess the reliability of the expected cumulative reward , and -LRB- 2 -RRB- perform a re ned comparison between policies derived from different <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Markov Decision Processes<tag name="FOCUS" value="end"/> <tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>MDP<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> -RRB- models .
We applied this methodology to a prior experiment where the goal was to select the best features to include in the MDP statespace .
Our results show that while some of the policies developed in the prior work exhibited very large con dence intervals , the policy developed from the best feature set had a much smaller con dence interval and thus showed very high reliability .


##W03-1010
A Plethora Of Methods For Learning <tag name="FOCUS" value="start"/> English Countability<tag name="FOCUS" value="end"/> .
This paper compares a range of methods for <tag name="DOMAIN" value="start"/>classifying words<tag name="DOMAIN" value="end"/> based on <tag name="TECHNIQUE" value="start"/>linguistic diagnostics<tag name="TECHNIQUE" value="end"/> , focusing on the task of <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>learning countabilities<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> for English nouns .
We propose two basic approaches to feature representation : <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>distribution-based representation<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> , which simply looks at the distribution of features in the corpus data , and <tag name="TECHNIQUE" value="start"/>agreement-based representation<tag name="TECHNIQUE" value="end"/> which analyses the level of tokenwise agreement between multiple preprocessor systems .
We additionally compare a single multiclass classifier architecture with a suite of binary classifiers , and combine analyses from multiple preprocessors .
Finally , we present and evaluate a <tag name="TECHNIQUE" value="start"/>feature selection<tag name="TECHNIQUE" value="end"/> method .


##H05-1113
Measuring The <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Relative Compositionality Of Verb-Noun -LRB- V-N -RRB-<tag name="DOMAIN" value="end"/> <tag name="TECHNIQUE" value="start"/>Collocations<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> By Integrating Features .
Measuring the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>relative compositionality of Multi-word Expressions -LRB- MWEs -RRB-<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> is crucial to Natural Language Processing .
Various <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>collocation<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> based measures have been proposed to compute the <tag name="DOMAIN" value="start"/>relative compositionality of MWEs<tag name="DOMAIN" value="end"/> .
In this paper , we define novel measures -LRB- both  <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>collocation<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> based and <tag name="TECHNIQUE" value="start"/>context<tag name="TECHNIQUE" value="end"/> based measures -RRB- to measure the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>relative compositionality of MWEs of V-N type<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We show that the correlation of these features with the human ranking is much superior to the correlation of the traditional features with the human ranking .
We then integrate the proposed features and the traditional features using a <tag name="TECHNIQUE" value="start"/>SVM<tag name="TECHNIQUE" value="end"/> based ranking function to rank the <tag name="TECHNIQUE" value="start"/>collocations<tag name="TECHNIQUE" value="end"/> of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>V-N type<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> based on their <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>relative compositionality<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We then show that the correlation between the ranks computed by the <tag name="TECHNIQUE" value="start"/>SVM<tag name="TECHNIQUE" value="end"/> based ranking function and human ranking is significantly better than the correlation between ranking of individual features and human ranking .


##W05-1518
Improving <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Accuracy By  Combining Diverse <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Dependency Parsers<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
This paper explores the possibilities of improving <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>parsing<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> results by combining outputs of several <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>parsers<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
To some extent , we are porting the ideas of Henderson and Brill -LRB- 1999 -RRB- to the world of dependency structures .
We differ from them in  exploring <tag name="TECHNIQUE" value="start"/>context features<tag name="TECHNIQUE" value="end"/>  more deeply .
All our experiments were conducted on Czech but the method is language-independent .
We were able to significantly improve over the best parsing result for the given setting , known so far .
Moreover , our experiments show that even parsers far below the state of the art can contribute to the total improvement .


##P01-1012
Detecting Problematic Turns In <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/> Human-Machine Interactions<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> : <tag name="TECHNIQUE" value="start"/>Rule-Induction<tag name="TECHNIQUE" value="end"/> Versus <tag name="TECHNIQUE" value="start"/>Memory-Based Learning<tag name="TECHNIQUE" value="end"/> Approaches .
We address the issue of on-line detection of communication problems in  <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/> spoken dialogue<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> systems .
The usefulness is investigated of the sequence of system question types and the <tag name="TECHNIQUE" value="start"/>word graphs<tag name="TECHNIQUE" value="end"/> corresponding to the respective user utterances .
By applying both <tag name="TECHNIQUE" value="start"/>ruleinduction<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>memory-based learning<tag name="TECHNIQUE" value="end"/> techniques to data obtained with a Dutch train time-table information system , the current paper demonstrates that the aforementioned features indeed lead to a method for problem detection that performs significantly above baseline .
The results are interesting from a <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>dialogue<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> perspective since they employ features that are present in the majority of <tag name="DOMAIN" value="start"/>spoken dialogue<tag name="DOMAIN" value="end"/> systems and can be obtained with little or no computational overhead .
The results are interesting from a <tag name="TECHNIQUE" value="start"/>machine learning<tag name="TECHNIQUE" value="end"/> perspective , since they show that the <tag name="TECHNIQUE" value="start"/>rule-based<tag name="TECHNIQUE" value="end"/> method performs significantly better than the <tag name="TECHNIQUE" value="start"/>memory-based<tag name="TECHNIQUE" value="end"/> method , because the former is better capable of representing interactions between features .


##E87-1020
<tag name="FOCUS" value="start"/>REFTEX<tag name="FOCUS" value="end"/> - A <tag name="TECHNIQUE" value="start"/>Context-Based<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Translation Aid<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
The system presented in this paper produces <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>bilingual passages of text<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> from an original -LRB- source -RRB- text and one -LRB- or more -RRB- of its translated versions .
The source text passage includes words or word compounds which a translator wants to retrieve for the current translating of another text .
The target text passage is the equivalent version of the source text passage .
On the basis of a comparison of the <tag name="TECHNIQUE" value="start"/>contexts<tag name="TECHNIQUE" value="end"/> of these words in the concorded passage and his own text , the translator has to decide on the utility of the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> proposed in the target text passage .
The program might become a component of translator 's work bench .


##W07-0204
<tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Timestamped Graphs : Evolutionary<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> Models of Text for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Multi-Document Summarization<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
Current graph-based approaches to automatic <tag name="DOMAIN" value="start"/> <tag name="FOCUS" value="start"/>text summarization<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , such as LexRank and TextRank , assume a static graph which does not model how the input texts emerge .
A suitable <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>evolutionary text graph<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> model may impart a better understanding of the texts and improve the summarization process .
We propose a <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>timestamped graph -LRB- TSG -RRB- <tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> model that is motivated by human writing and reading processes , and show how text units in this model emerge over time .
In our model , the graphs used by LexRank and TextRank are specific instances of our <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>timestamped graph<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> with particular parameter settings .
We apply <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>timestamped graphs<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> on the standard <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>DUC multi-document text summarization<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> task and achieve comparable results to the state of the art .


##N04-1002
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Cross-Document Coreference<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> On A Large Scale Corpus .
In this paper , we will compare and evaluate the effectiveness of different <tag name="TECHNIQUE" value="start"/>statistical<tag name="TECHNIQUE" value="end"/> methods in the task of <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>cross-document coreference resolution<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
We created <tag name="TECHNIQUE" value="start"/>entity<tag name="TECHNIQUE" value="end"/> models for different test sets and compare the following <tag name="TECHNIQUE" value="start"/>disambiguation and clustering<tag name="TECHNIQUE" value="end"/> techniques to <tag name="TECHNIQUE" value="start"/>cluster<tag name="TECHNIQUE" value="end"/> the <tag name="TECHNIQUE" value="start"/>entity<tag name="TECHNIQUE" value="end"/> models in order to create  <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>coreference chains<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> : <tag name="TECHNIQUE" value="start"/>Incremental Vector Space KL-Divergence Agglomerative Vector Space <tag name="TECHNIQUE" value="end"/>

##D07-1107
<tag name="TECHNIQUE" value="start"/>Learning<tag name="TECHNIQUE" value="end"/> to <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Merge Word Senses<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
It has been widely observed that different NLP applications require different sense granularities in order to best exploit <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>word sense<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> distinctions , and that for many applications WordNet senses are too fine-grained .
In contrast to previously proposed automatic methods for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>sense clustering<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , we formulate <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>sense merging<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> as a <tag name="TECHNIQUE" value="start"/>supervised learning<tag name="TECHNIQUE" value="end"/> problem , exploiting human-labeled <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>sense clusterings<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> as training data .
We train a <tag name="TECHNIQUE" value="start"/>discriminative classifier<tag name="TECHNIQUE" value="end"/> over a wide variety of features derived from <tag name="TECHNIQUE" value="start"/>WordNet structure<tag name="TECHNIQUE" value="end"/> , corpus-based evidence , and evidence from other lexical resources .
Our learned similarity measure outperforms previously proposed automatic methods for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>sense clustering<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> on the task of predicting human sense merging judgments , yielding an absolute F-score improvement of 4.1 % on nouns , 13.6 % on verbs , and 4.0 % on adjectives .
Finally , we propose a model for <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>clustering sense taxonomies<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> using the outputs of our classifier , and we make available several automatically sense-clustered WordNets of various sense granularities .


##W04-2705
The <tag name="FOCUS" value="start"/>NomBank Project<tag name="FOCUS" value="end"/> : An Interim Report .
This paper describes <tag name="FOCUS" value="start"/>NomBank <tag name="FOCUS" value="end"/> , a project that will provide <tag name="DOMAIN" value="start"/>argument structure<tag name="DOMAIN" value="end"/> for instances of common nouns in the Penn Treebank II corpus .
<tag name="FOCUS" value="start"/>NomBank<tag name="FOCUS" value="end"/> is part of a larger effort to add additional layers of annotation to the Penn Treebank II corpus .
The University of Pennsylvania 's PropBank , NomBank and other annotation projects taken together should lead to the creation of better tools for the automatic analysis of text .
This paper describes the <tag name="FOCUS" value="start"/>NomBank<tag name="FOCUS" value="end"/> project in detail including its speci cations and the process involved in creating the resource .


##P99-1010
<tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Supervised<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>Grammar Induction<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> Using Training Data With Limited Constituent Information .
Corpus-based <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>grammar induction<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> generally relies on hand-parsed training data to learn the structure of the language .
Unfortunately , the cost of building large annotated corpora is prohibitively expensive .
This work aims to improve the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>induction<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> strategy when there are few labels in the training data .
We show that the most informative linguistic constituents are the higher nodes in the parse trees , typically denoting complex noun phrases and sentential clauses .
They account for only 20 % of all constituents .
For <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>inducing grammars<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> from sparsely labeled training data -LRB- e.g. , only higher-level constituent labels -RRB- , we propose an <tag name="TECHNIQUE" value="start"/>adaptation<tag name="TECHNIQUE" value="end"/> strategy , which produces grammars that parse almost as well as grammars induced from fully labeled corpora .
Our results suggest that for a partial parser to replace human annotators , it must be able to automatically extract higher-level constituents rather than base noun phrases .


##W03-0613
Learning <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Word Meanings<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> And <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Descriptive Parameter Spaces<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> From <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Music<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> .
The audio bitstream in music encodes a high amount of statistical , acoustic , emotional and cultural information .
But music also has an important linguistic accessory ; most musical artists are described in great detail in record reviews , fan sites and news items .
We highlight current and ongoing research into extracting relevant <tag name="TECHNIQUE" value="start"/>features from audio<tag name="TECHNIQUE" value="end"/> and simultaneously learning <tag name="TECHNIQUE" value="start"/>language features linked to the music<tag name="TECHNIQUE" value="end"/> .
We show results in a <tag name="DOMAIN" value="start"/>`` query-bydescription '' task<tag name="DOMAIN" value="end"/> in which we learn the perceptual meaning of automatically-discovered single-term descriptive components , as well as a method of automatically uncovering ` <tag name="TECHNIQUE" value="start"/>semantically attached<tag name="TECHNIQUE" value="end"/> ' terms -LRB- terms that have <tag name="TECHNIQUE" value="start"/>perceptual grounding<tag name="TECHNIQUE" value="end"/> . -RRB-
We then show recent work in ` <tag name="DOMAIN" value="start"/>semantic basis functions<tag name="DOMAIN" value="end"/> ' -- parameter spaces of description -LRB- such as fast ... slow or male ... female -RRB- that encode the highest descriptive variance in a semantic space .


##P06-2029
The Benefit Of <tag name="TECHNIQUE" value="start"/>Stochastic<tag name="TECHNIQUE" value="end"/> <tag name="FOCUS" value="start"/>PP Attachment<tag name="FOCUS" value="end"/> To A <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Rule-Based<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>Parser<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
To study <tag name="FOCUS" value="start"/>PP attachment disambiguation<tag name="FOCUS" value="end"/> as a benchmark for empirical methods in natural language processing it has often been reduced to a binary decision problem -LRB- between verb or noun attachment -RRB- in a particular syntactic configuration .
A parser , however , must solve the more general task of deciding between more than two alternatives in many different contexts .
We combine the attachment predictions made by a simple model of <tag name="TECHNIQUE" value="start"/>lexical attraction<tag name="TECHNIQUE" value="end"/> with a full-fledged <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>parser<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> of German to determine the actual benefit of the subtask to <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We show that the combination of <tag name="TECHNIQUE" value="start"/>data-driven<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>rule-based<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> components can reduce the number of all <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> errors by 14 % and raise the attachment accuracy for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>dependency parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> of German to an unprecedented 92 % .


##P87-1027
The Derivation Of A <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Grammatically Indexed Lexicon<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> From The <tag name="TECHNIQUE" value="start"/>Longman Dictionary Of Contemporary English<tag name="TECHNIQUE" value="end"/> .
We describe a methodology and associated software system for the construction of a large <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>lexicon<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> from an existing machine-readable -LRB- published -RRB- <tag name="TECHNIQUE" value="start"/>dictionary<tag name="TECHNIQUE" value="end"/> .
The <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>lexicon<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> serves as a component of an English morphological and syntactic analyesr and contains entries with grammatical definitions compatible with the word and sentence grammar employed by the analyser .
We describe a software system with two integrated components .
One of these is capable of extracting syntactically rich , theory-neutral lexical templates from a suitable machine-readabh source .
The second supports interactive and semi-automatic generation and testing of target lexical entries in order to derive a sizeable , accurate and consistent <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>lexicon<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> from the source <tag name="TECHNIQUE" value="start"/>dictionary<tag name="TECHNIQUE" value="end"/> which contains partial -LRB- and occasionally inaccurate -RRB- information .
Finally , we evaluate the utility of the <tag name="TECHNIQUE" value="start"/>Longman Dictionary of Contemporary EnglgsA<tag name="TECHNIQUE" value="end"/> as a suitable source dictionary for the target lexicon .


##C96-2182
Formal Description Of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Multi-Word Lexemes<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> With The <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Finite-State Formalism IDAREX<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> .
Most <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>multi-word lexemes<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> -LRB- <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>MWLs<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> -RRB- allow certain types of variation .
This has to be taken into account for their description and their recognition in texts .
We suggest to describe their syntactic restrictions and their idiosyncratic peculiarities with local grammar rules , which at the same time allow to express in a general way regularities valid for a whole class of <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>MWLs<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
The local grammars can be written in a very convenient and compact way as regular expressions in the formalism <tag name="TECHNIQUE" value="start"/>IDAREX<tag name="TECHNIQUE" value="end"/> which uses a two-level <tag name="TECHNIQUE" value="start"/>morphology<tag name="TECHNIQUE" value="end"/> .
<tag name="TECHNIQUE" value="start"/>IDAREX<tag name="TECHNIQUE" value="end"/> allows to define various types of variables , and to mix canonical and inflected word forms in the regular expressions .


##C08-1044
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Modeling Chinese Documents<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> with <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Topical Word-Character Models<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> .
As Chinese text is written without word boundaries , effectively recognizing Chinese words is like recognizing collocations in English , substituting characters for words and words for collocations .
However , existing topical models that involve collocations have a common limitation .
Instead of directly assigning a topic to a collocation , they take the topic of a word within the collocation as the topic of the whole collocation .
This is unsatisfactory for topical <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>modeling of Chinese documents<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
Thus , we propose a <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>topical word-character <tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> model -LRB- <tag name="TECHNIQUE" value="start"/>TWC<tag name="TECHNIQUE" value="end"/> -RRB- , which allows two distinct types of topics : <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>word topic<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> and <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>character topic<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> .
We evaluated <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>TWC<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> both qualitatively and quantitatively to show that it is a powerful and a promising <tag name="TECHNIQUE" value="start"/>topic model<tag name="TECHNIQUE" value="end"/> .


##W05-0803
Parsing Word-Aligned Parallel Corpora In A <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Grammar Induction<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Context .
We present an <tag name="TECHNIQUE" value="start"/>Earley-style dynamic programming <tag name="TECHNIQUE" value="end"/> algorithm for <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>parsing sentence pairs <tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> from a parallel corpus simultaneously , building up two phrase structure trees and a correspondence mapping between the nodes .
The intended use of the algorithm is in <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>bootstrapping grammars<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> for less studied languages by using implicit <tag name="TECHNIQUE" value="start"/>grammatical information <tag name="TECHNIQUE" value="end"/> in parallel corpora .
Therefore , we presuppose a given <tag name="TECHNIQUE" value="start"/>-LRB- statistical -RRB- word alignment<tag name="TECHNIQUE" value="end"/> underlying in the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>synchronous parsing<tag name="FOCUS" value="end"/> <tag name="DOMAIN" value="end"/> task ; this leads to a significant reduction of the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> complexity .
The theoretical complexity results are corroborated by a quantitative evaluation in which we ran an implementation of the algorithm on a suite of test sentences from the Europarl parallel corpus .


##W00-1416
On Identifying  <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Sets<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
A range of research has explored the problem of generating referring expressions that uniquely identify a single entity from the shared context .
But what about expressions that identify sets of entities ?
In this paper , I adapt recent semantic research on plural descriptions -- using <tag name="TECHNIQUE" value="start"/>covers<tag name="TECHNIQUE" value="end"/> to abstract collective and distributive readings and using sets of assignments to represent <tag name="TECHNIQUE" value="start"/>dependencies<tag name="TECHNIQUE" value="end"/> among references -- to describe a search problem for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>set-identifying expressions<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> that largely mirrors the search problem for singular referring expressions .
By structuring the search space only in terms of the words that can be added to the description , the proposal defuses potential combinatorial explosions that might otherwise arise with reference to sets .


##W07-2053
<tag name="FOCUS" value="start"/>NUS-ML<tag name="FOCUS" value="end"/> : Improving <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Word Sense Disambiguation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Using <tag name="TECHNIQUE" value="start"/>Topic Features<tag name="TECHNIQUE" value="end"/> .
We participated in SemEval-1 English <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>coarse-grained all-words<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> task -LRB- task 7 -RRB- , English <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>fine-grained all-words<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> task -LRB- task 17 , subtask 3 -RRB- and English <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>coarse-grained lexical sample<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> task -LRB- task 17 , subtask 1 -RRB- .
The same method with different labeled data is used for the tasks ; SemCor is the labeled corpus used to train our system for the allwords tasks while the labeled corpus that is provided is used for the <tag name="DOMAIN" value="start"/>lexical sample<tag name="DOMAIN" value="end"/> task .
The knowledge sources include <tag name="TECHNIQUE" value="start"/>part-of-speech<tag name="TECHNIQUE" value="end"/> of neighboring words , single words in the surrounding <tag name="TECHNIQUE" value="start"/>context , local collocations , and syntactic patterns<tag name="TECHNIQUE" value="end"/> .
In addition , we constructed a <tag name="TECHNIQUE" value="start"/>topic feature<tag name="TECHNIQUE" value="end"/> , targeted to capture the global <tag name="TECHNIQUE" value="start"/>context<tag name="TECHNIQUE" value="end"/> information , using the <tag name="TECHNIQUE" value="start"/>latent dirichlet allocation -LRB- LDA -RRB- algorithm<tag name="TECHNIQUE" value="end"/> with unlabeled corpus .
A modified <tag name="TECHNIQUE" value="start"/>na Â¨ Ä±ve Bayes classifier<tag name="TECHNIQUE" value="end"/> is constructed to incorporate all the features .
We achieved 81.6 % , 57.6 % , 88.7 % for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>coarse-grained allwords<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> task , <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>fine-grained all-words<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> task and <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>coarse-grained lexical sample<tag name="FOCUS" value="end"/> <tag name="DOMAIN" value="end"/> task respectively .


##W04-0202
<tag name="FOCUS" value="start"/>COOPML<tag name="FOCUS" value="end"/> : Towards Annotating <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Cooperative Discourse<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
In this paper , we present a preliminary version of <tag name="FOCUS" value="start"/>COOPML<tag name="FOCUS" value="end"/> , a language designed for annotating <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>cooperative discourse<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We investigate the different <tag name="TECHNIQUE" value="start"/>linguistic marks<tag name="TECHNIQUE" value="end"/> that identify and characterize the different forms of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>cooperativity<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> found in written texts from FAQs , Forums and emails  .


##C96-2129
Automatic Detection Of Omissions In <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Translations<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
<tag name="FOCUS" value="start"/>ADOMIT<tag name="FOCUS" value="end"/> is an algorithln for Automatic Detection of OMissions in <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Translations<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
The algorithm relies solely on <tag name="TECHNIQUE" value="start"/>geometric analysis of bitext maps<tag name="TECHNIQUE" value="end"/> and uses no linguistic information .
This property allows it to deal equally well with omissions that do not correspond to linguistic units , such as might result ti'om word-processing mishaps .
<tag name="FOCUS" value="start"/>ADOMIT<tag name="FOCUS" value="end"/> has proven itself by discovering many errors in a handconstructed gold standard for evaluating bitext mapping algorithms .
Quantitative evaluation on simulated omissions showed that , even with today 's poor bitext mapping technology , <tag name="FOCUS" value="start"/>ADOMIT<tag name="FOCUS" value="end"/> is a valuable quality control tool for translators and translation bureaus .


##N07-2030
On using <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Articulatory Features<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> for <tag name="TECHNIQUE" value="start"/>Discriminative<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Speaker Adaptation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
This paper presents a way to perform <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>speaker adaptation for automatic speech recognition<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> using the <tag name="TECHNIQUE" value="start"/>stream weights in a multi-streamsetup<tag name="TECHNIQUE" value="end"/> , whichincludedacoustic models for <tag name="TECHNIQUE" value="start"/>`` Articulatory Features ''<tag name="TECHNIQUE" value="end"/> such as ROUNDED or VOICED .
We present <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>supervised<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>speaker adaptation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> experiments on a spontaneous <tag name="DOMAIN" value="start"/>speech<tag name="DOMAIN" value="end"/> task and compare the above <tag name="TECHNIQUE" value="start"/>stream-based<tag name="TECHNIQUE" value="end"/> approach to conventional approaches , in which the models , and not stream combination weights , are being adapted .
In the approach we present , <tag name="TECHNIQUE" value="start"/>stream weights<tag name="TECHNIQUE" value="end"/> model the importance of features such as <tag name="TECHNIQUE" value="start"/>VOICED<tag name="TECHNIQUE" value="end"/> for word discrimination , which offers a descriptive interpretation of the adaptation parameters .


##P06-2022
Automatically Extracting <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Nominal Mentions Of Events<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> With A <tag name="TECHNIQUE" value="start"/>Bootstrapped Probabilistic Classifier<tag name="TECHNIQUE" value="end"/> .
Most approaches to <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>event extraction<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> focus on mentions anchored in verbs .
However , many mentions of events surface as noun phrases .
Detecting them can increase the recall of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>event extraction<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> and provide the foundation for detecting relations between events .
This paper describes a <tag name="TECHNIQUE" value="start"/>weaklysupervised <tag name="TECHNIQUE" value="end"/> method for detecting <tag name="DOMAIN" value="start"/> <tag name="FOCUS" value="start"/>nominal event mentions<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> that combines techniques from <tag name="TECHNIQUE" value="start"/>word sense disambiguation -LRB- WSD -RRB- andlexicalacquisitiontocreateaclassifier<tag name="TECHNIQUE" value="end"/> thatlabelsnounphrasesasdenotingevents or non-events .
The classifier uses <tag name="TECHNIQUE" value="start"/>bootstrapped probabilistic generative<tag name="TECHNIQUE" value="end"/> models of the contexts of events and non-events .
Thecontextsarethelexically-anchoredsemantic dependency relations that the NPs appear in .
Our method dramatically improves with <tag name="TECHNIQUE" value="start"/>bootstrapping<tag name="TECHNIQUE" value="end"/> , and comfortably outperforms lexical lookup methods whicharebasedonverymuchlargerhandcrafted resources .


##P84-1093
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Machine-Readable Dictionaries<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
The papers in this panel consider <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>machine-readable dictionaries<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> from several perspectives : research in computational linguistics and computational lexicology , the development of tools for improving accessibility , the design of lexical reference systems for educational purposes , and applications of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>machine-readable dictionaries<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> in information science contexts .
As background and by way of introduction , a description is provided of a workshop on <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>machine-readable dictionaries<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> that was held at SRI International in April 1983 .


##W98-1111
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Language Identification<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> With <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Confidence Limits<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> .
A <tag name="TECHNIQUE" value="start"/>statistical classification <tag name="TECHNIQUE" value="end"/> algorithm and its application to <tag name="DOMAIN" value="start"/>language identification <tag name="DOMAIN" value="end"/> from noisy input are described .
The main innovation is to compute <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>confidence limits on the classification<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> , so that the algorithm terminates when enough evidence to make a clear decision has been made , and so avoiding problems with categories that have similar characteristics .
A second application , to <tag name="DOMAIN" value="start"/>genre identification<tag name="DOMAIN" value="end"/> , is briefly examined .
The results show that some of the problems of other <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>language identification<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> techniques can be avoided , and illustrate a more important point : that a <tag name="TECHNIQUE" value="start"/>statistical <tag name="TECHNIQUE" value="end"/> language process can be used to provide feedback about its own success rate .


##W03-0404
Learning <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/> Subjective Nouns<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Using <tag name="TECHNIQUE" value="start"/>Extraction Pattern Bootstrapping<tag name="TECHNIQUE" value="end"/> .
We explore the idea of creating a <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>subjectivity<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> classifier that uses lists of subjective nouns learned by <tag name="TECHNIQUE" value="start"/>bootstrapping<tag name="TECHNIQUE" value="end"/> algorithms .
The goal of our research is to develop a system that can distinguish <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>subjective<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> sentences from objective sentences .
First , we use two <tag name="TECHNIQUE" value="start"/>bootstrapping <tag name="TECHNIQUE" value="end"/> algorithms that exploit <tag name="TECHNIQUE" value="start"/>extraction patterns<tag name="TECHNIQUE" value="end"/> to learn sets of <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>subjective nouns<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
Then we train a <tag name="TECHNIQUE" value="start"/>Naive Bayes <tag name="TECHNIQUE" value="end"/> classifier using the subjective nouns , <tag name="TECHNIQUE" value="start"/>discourse features , and subjectivity clues<tag name="TECHNIQUE" value="end"/> identified in prior research .
The <tag name="TECHNIQUE" value="start"/>bootstrapping <tag name="TECHNIQUE" value="end"/> algorithms learned over 1000 <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>subjective nouns<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , and the subjectivity classifier performed well , achieving 77 % recall with 81 % precision .


##P08-2063
Choosing Sense Distinctions for <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>WSD<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> : <tag name="FOCUS" value="start"/>Psycholinguistic<tag name="FOCUS" value="end"/> Evidence .
Supervised <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>word sense disambiguation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> requires training corpora that have been tagged with word senses , which begs the question of which word senses to tag with .
The default choice has been WordNet , with its broad coverage and easy accessibility .
However , concerns have been raised about the appropriateness of its fine-grained word senses for WSD .
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>WSD<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> systems have been far more successful in distinguishing coarsegrained senses than fine-grained ones -LRB- Navigli , 2006 -RRB- , but does that approach neglect necessary meaning differences ?
Recent <tag name="FOCUS" value="start"/>psycholinguistic<tag name="FOCUS" value="end"/> evidence seems to indicate that closely related word senses may be represented in the mental lexicon much like a single sense , whereas distantly related senses may be represented more like discrete entities .
These results suggest that , for the purposes of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>WSD<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , closely related word senses can be <tag name="TECHNIQUE" value="start"/>clustered<tag name="TECHNIQUE" value="end"/> together into a more general sense with little meaning loss .
The current paper will describe this <tag name="FOCUS" value="start"/>psycholinguistic<tag name="FOCUS" value="end"/> research and its implications for automatic <tag name="DOMAIN" value="start"/> <tag name="FOCUS" value="start"/>word sense disambiguation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .


##W00-1212
A <tag name="TECHNIQUE" value="start"/>Block-Based<tag name="TECHNIQUE" value="end"/> Robust <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Dependency Parser<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> For Unrestricted Chinese Text .
Although substantial efforts have been made to <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>parse<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Chinese , very few have been practically used due to incapability of handling unrestricted texts .
This paper realizes a practical system for Chinese <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> by using a <tag name="TECHNIQUE" value="start"/>hybrid<tag name="TECHNIQUE" value="end"/> model of <tag name="DOMAIN" value="start"/>phrase structure partial parsing<tag name="DOMAIN" value="end"/> and <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>dependency parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
This system showed good performance and high robustness in <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> unrestricted texts and has been applied in a successful <tag name="DOMAIN" value="start"/>machine translation<tag name="DOMAIN" value="end"/> product .


##W04-2804
<tag name="TECHNIQUE" value="start"/>Ends-Based<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Dialogue Processing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We describe a reusable and scalable <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>dialogue<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> toolbox and its application in multiple systems .
Our main claim is that <tag name="TECHNIQUE" value="start"/>ends-based<tag name="TECHNIQUE" value="end"/> representation and processing throughout the complete dialogue backbone it essential to our approach .


##I05-1049
Relative Compositionality of <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Multi-word Expressions<tag name="DOMAIN" value="end"/> : A Study of <tag name="DOMAIN" value="start"/>Verb-Noun -LRB- V-N -RRB- Collocations<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
Recognition of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Multi-word Expressions -LRB- MWEs -RRB-<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> and their relative compositionality are crucial to Natural Language Processing .
Various statistical techniques have been proposed to recognize <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>MWEs<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
In this paper , we integrate all the existing <tag name="TECHNIQUE" value="start"/> statistical features<tag name="TECHNIQUE" value="end"/> and investigate a range of <tag name="TECHNIQUE" value="start"/>classifiers<tag name="TECHNIQUE" value="end"/> for their suitability for recognizing the <tag name="DOMAIN" value="start"/>non-compositional Verb-Noun -LRB- V-N -RRB- collocations<tag name="DOMAIN" value="end"/> .
In the task of <tag name="DOMAIN" value="start"/>ranking the V-N collocations<tag name="DOMAIN" value="end"/> based on their relative compositionality , we show that the correlation between the ranks computed by the classifier and human ranking is significantly better than the correlation between ranking of individual features and human ranking .
We also show that the properties <tag name="TECHNIQUE" value="start"/>` Distributed frequency of object '<tag name="TECHNIQUE" value="end"/> -LRB- as defined in -LRB- 27 -RRB- -RRB- and ` Nearest <tag name="TECHNIQUE" value="start"/>Mutual Information<tag name="TECHNIQUE" value="end"/> ' -LRB- as adapted from -LRB- 18 -RRB- -RRB- contribute greatly to the recognition of the <tag name="DOMAIN" value="start"/>non-compositional MWEs of the V-N type<tag name="DOMAIN" value="end"/> and to the <tag name="DOMAIN" value="start"/>ranking of the V-N collocations<tag name="DOMAIN" value="end"/> based on their relative compositionality .


##W05-1507
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Machine Translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> As <tag name="TECHNIQUE" value="start"/>Lexicalized Parsing With Hooks<tag name="TECHNIQUE" value="end"/> .
We adapt the <tag name="TECHNIQUE" value="start"/>`` hook '' trick<tag name="TECHNIQUE" value="end"/> for speeding up bilexical <tag name="TECHNIQUE" value="start"/>parsing<tag name="TECHNIQUE" value="end"/> to the decoding problem for  <tag name="DOMAIN" value="start"/>machine translation<tag name="DOMAIN" value="end"/> models that are based on combining a <tag name="TECHNIQUE" value="start"/>synchronous context free grammar<tag name="TECHNIQUE" value="end"/> as the translation model with an <tag name="TECHNIQUE" value="start"/>n-gram <tag name="TECHNIQUE" value="end"/> language model .
This<tag name="TECHNIQUE" value="start"/> dynamic programming<tag name="TECHNIQUE" value="end"/> technique yields lower complexity algorithms than have previously been described for an important class of <tag name="DOMAIN" value="start"/>translation<tag name="DOMAIN" value="end"/> models .


##T78-1031
<tag name="TECHNIQUE" value="start"/>Path-Based<tag name="TECHNIQUE" value="end"/> And <tag name="TECHNIQUE" value="start"/>Node-Based<tag name="TECHNIQUE" value="end"/> Inference In <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Semantic Networks<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
Two styles of performing inference in <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>semantic networks<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> are presented and compared .
<tag name="TECHNIQUE" value="start"/>Path-based<tag name="TECHNIQUE" value="end"/> inference allows an arc or a path of arcs between two given nodes to be inferred from the existence of another specified path between the same two nodes .
<tag name="TECHNIQUE" value="start"/>Path-based<tag name="TECHNIQUE" value="end"/> inference rules may be written using a <tag name="TECHNIQUE" value="start"/>binary relational calculus notation<tag name="TECHNIQUE" value="end"/> .
<tag name="TECHNIQUE" value="start"/>Node-based<tag name="TECHNIQUE" value="end"/> inference allows a structure of nodes to be inferred from the existence of an instance of a pattern of node structures .
<tag name="TECHNIQUE" value="start"/>Node-based<tag name="TECHNIQUE" value="end"/> inference rules can be constructed in a <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>semantic network<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> using a variant of a <tag name="TECHNIQUE" value="start"/>predicate calculus notation<tag name="TECHNIQUE" value="end"/> .
<tag name="TECHNIQUE" value="start"/>Path-based<tag name="TECHNIQUE" value="end"/> inference is more efficient , while <tag name="TECHNIQUE" value="start"/>node-based<tag name="TECHNIQUE" value="end"/> inference is more general .
A method is described of combining the two styles in a single system in order to take advantage of the strengths of each .
Applications of <tag name="TECHNIQUE" value="start"/>path-based<tag name="TECHNIQUE" value="end"/> inference rules to the representation of the extensional equivalence of intensional concepts , and to the explication of inheritance in hierarchies are sketched .


##D08-1099
Automatic <tag name="TECHNIQUE" value="start"/> Set Expansion<tag name="TECHNIQUE" value="end"/> for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>List Question Answering<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
This paper explores the use of <tag name="TECHNIQUE" value="start"/>set expansion -LRB- SE -RRB-<tag name="TECHNIQUE" value="end"/> to improve <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>question answering<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> -LRB- <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>QA<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> -RRB- when the expected answer is a list of entities belonging to a certain class .
Given a small set of seeds , <tag name="TECHNIQUE" value="start"/>SE<tag name="TECHNIQUE" value="end"/> algorithms mine textual resources to produce an extended list including additional members of the class represented by the seeds .
We explore the hypothesis that a <tag name="TECHNIQUE" value="start"/>noise-resistant SE <tag name="TECHNIQUE" value="end"/> algorithm can be used to extend candidate answers produced by a <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>QA<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/>  system and generate a new list of answers that is better than the original list produced by the <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>QA<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> system .
We further introduce a <tag name="TECHNIQUE" value="start"/>hybrid<tag name="TECHNIQUE" value="end"/> approach which combines the original answers from the <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>QA<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> system with the output from the <tag name="TECHNIQUE" value="start"/>SE <tag name="TECHNIQUE" value="end"/> algorithm .
Experimental results for several state-of-the-art <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>QA<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> systems show that the <tag name="TECHNIQUE" value="start"/>hybrid<tag name="TECHNIQUE" value="end"/> system performs better than the <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>QA<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> systems alone when tested on list question data from past TREC evaluations .


##W97-0901
Reuse Of A <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Proper Noun Recognition <tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> System In Commercial And Operational NLP Applications .
SRA 's proprietary product , <tag name="FOCUS" value="start"/>NameTag TM<tag name="FOCUS" value="end"/> , which provides fast and accurate <tag name="TECHNIQUE" value="start"/>name recognition<tag name="TECHNIQUE" value="end"/> , has been reused in many applications in recent and ongoing efforts , including multilingual information retrieval and browsing , text clustering , and assistance to manual text indexing .
This paper reports on SRA 's experience in embedding <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>name recognition<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> in these three specific applications , and the mutual impacts that occur , both on the algorithmic level and in the role that <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>name recognition<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> plays in user interaction with a system .
In the course of this , we touch upon various interactions between <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>proper name recognition<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> and <tag name="DOMAIN" value="start"/>machine translation -LRB- MT -RRB- <tag name="DOMAIN" value="end"/>, as well as the role of accurate name recognition in improving the performance of <tag name="DOMAIN" value="start"/>word segmentation algorithms<tag name="DOMAIN" value="end"/> needed for languages whose writing systems do not segment words .


##W08-1133
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Referring Expression Generation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Using <tag name="TECHNIQUE" value="start"/>Speaker-based Attribute Selection<tag name="TECHNIQUE" value="end"/> and Trainable <tag name="TECHNIQUE" value="start"/>Realization -LRB- ATTR -RRB-<tag name="TECHNIQUE" value="end"/> .
In the first REG competition , researchers proposed several general-purpose algorithms for attribute selection for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>referring expression generation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
However , most of this work did not take into account : a -RRB- stylistic differences between speakers ; or b -RRB- trainable <tag name="TECHNIQUE" value="start"/>surface realization<tag name="TECHNIQUE" value="end"/> approaches that combine <tag name="TECHNIQUE" value="start"/>semantic<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>word order<tag name="TECHNIQUE" value="end"/> information .
In this paper we describe and evaluate several end-to-end <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>referring expression generation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/>  algorithms that take into consideration <tag name="TECHNIQUE" value="start"/>speaker style<tag name="TECHNIQUE" value="end"/> and use <tag name="TECHNIQUE" value="start"/>data-driven surface realization<tag name="TECHNIQUE" value="end"/> techniques .


##P06-1053
Integrating <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/> Syntactic Priming <tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/>  Into An Incremental <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Probabilistic Parser<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/>, With An Application To <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Psycholinguistic Modeling<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
The psycholinguistic literature provides evidence for syntactic priming , i.e. , the tendency to repeat structures .
This paper describes a method for <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>incorporating priming into an incremental probabilistic parser<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> .
Three models are compared , which involve <tag name="TECHNIQUE" value="start"/>priming of rules between sentences , within sentences , and within coordinate structures<tag name="TECHNIQUE" value="end"/> .
These models simulate the reading time advantage for parallel structures found in human data , and also yield a small increase in overall <tag name="DOMAIN" value="start"/>parsing<tag name="DOMAIN" value="end"/> accuracy .


##W07-1429
<tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Biology Based<tag name="TECHNIQUE" value="end"/> Alignments of <tag name="DOMAIN" value="start"/>Paraphrases for Sentence Compression<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
1 In this paper , we present a study for extracting and aligning <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>paraphrases<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> in the context of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Sentence Compression<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
First , we justify the application of a new measure for the automatic extraction of <tag name="DOMAIN" value="start"/> <tag name="FOCUS" value="start"/>paraphrase <tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> corpora .
Second , we discuss the work done by -LRB- Barzilay & Lee , 2003 -RRB- who use clustering of paraphrases to induce rewriting rules .
We will see , through classical visualization methodologies -LRB- Kruskal & Wish , 1977 -RRB- and exhaustive experiments , that clustering may not be the best approach for automatic pattern identification .
Finally , we will provide some results of different <tag name="TECHNIQUE" value="start"/>biology based <tag name="TECHNIQUE" value="end"/> methodologies for pairwise  <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>paraphrase<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> alignment .
1 Introduction Sentence Compression can be seen as the removal of redundant words or phrases from an input sentence by creating a new sentence in which the gist of the original meaning of the sentence remains unchanged .
Sentence Compression takes an important place for Natural Language Processing -LRB- NLP -RRB- tasks where specific constraints must be satisfied , such as length in summarization -LRB- Barzilay & Lee , 2002 ; Knight & Marcu , 2002 ; Shinyama et al. , 2002 ; Barzilay & Lee , 2003 ; Le Nguyen & Ho , 2004 ; Unno et al. , 2006 -RRB- , style in text simplification -LRB- Marsi & Krahmer , 2005 -RRB- or sentence simplification for subtitling -LRB- Daelemans et al. , 2004 -RRB- .


##W01-1006
Semi-Automatic Practical <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/> Ontology Construction<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> By Using A <tag name="TECHNIQUE" value="start"/>Thesaurus , Computational Dictionaries , And Large Corpora<tag name="TECHNIQUE" value="end"/> .
This paper presents the <tag name="TECHNIQUE" value="start"/>semi-automatic<tag name="TECHNIQUE" value="end"/> construction  method of a practical <tag name="DOMAIN" value="start"/> <tag name="FOCUS" value="start"/>ontology<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> by using various resources .
In order to acquire a reasonably practical ontology in a limited time and with less manpower , we extend the <tag name="TECHNIQUE" value="start"/> Kadokawa thesaurus<tag name="TECHNIQUE" value="end"/> by inserting additional <tag name="TECHNIQUE" value="start"/>semantic relations<tag name="TECHNIQUE" value="end"/> into its hierarchy , which are classified as case relations and other semantic relations .
The former can be obtained by converting valency information and case frames from previously-built computational dictionaries used in machine translation .
The latter can be acquired from concept co-occurrence information , which is extracted automatically from large corpora .
The ontology stores rich semantic constraints among 1,110 concepts , and enables a natural language processing system to resolve semantic ambiguities by making inferences with the concept network of the ontology .
In our practical <tag name="DOMAIN" value="start"/>machine translation <tag name="DOMAIN" value="end"/> system , our <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>ontology-based<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> <tag name="DOMAIN" value="start"/>word sense disambiguation<tag name="DOMAIN" value="end"/> method achieved an 8.7 % improvement over methods which do not use an ontology for Korean translation .


##W09-0407
The RWTH <tag name="TECHNIQUE" value="start"/>System Combination<tag name="TECHNIQUE" value="end"/> System for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>WMT<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> 2009 .
RWTH participated in the <tag name="TECHNIQUE" value="start"/>System Combination<tag name="TECHNIQUE" value="end"/> task of the Fourth Workshop on <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Statistical<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>Machine Translation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> -LRB- WMT 2009 -RRB- .
Hypotheses from 9 German â English MT systems were combined into a consensus <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
This consensus <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> scored 2.1 % better in BLEU and 2.3 % better in TER -LRB- abs . -RRB-
than the best single system .
In addition , cross-lingual output from 10 French , German , and Spanish â English systems was combined into a consensus <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , which gave an improvement of 2.0 % in BLEU\/3 .5 % in TER -LRB- abs . -RRB-
over the best single system .


##P08-2037
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Event Matching<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> Using the <tag name="TECHNIQUE" value="start"/>Transitive Closure of Dependency Relations<tag name="TECHNIQUE" value="end"/> .
This paper describes a novel <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>event-matching<tag name="FOCUS" value="end"/> <tag name="DOMAIN" value="end"/> strategy using features obtained from the <tag name="TECHNIQUE" value="start"/>transitive closure of dependency relations<tag name="TECHNIQUE" value="end"/> .
The method yields a model capable of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>matching events<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> with an F-measure of 66.5 % .


##W99-0606
<tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Boosting<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> Applied To <tag name="DOMAIN" value="start"/>Tagging<tag name="DOMAIN" value="end"/> And <tag name="DOMAIN" value="start"/>PP Attachment<tag name="DOMAIN" value="end"/> .
<tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Boosting<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> is a machine learning algorithm that is not well known in computational linguistics .
We apply it to <tag name="DOMAIN" value="start"/>part-of-speech tagging<tag name="DOMAIN" value="end"/> and <tag name="DOMAIN" value="start"/>prepositional phrase attachment<tag name="DOMAIN" value="end"/> .
Performance is very encouraging .
We also show how to improve data quality by using <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>boosting<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> to identify <tag name="DOMAIN" value="start"/>annotation errors<tag name="DOMAIN" value="end"/> .


##I08-5013
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Named Entity Recognition<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> for South Asian Languages .
Much work has already been done on building <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>named entity recognition<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> systems .
However most of this work has been concentrated on English and other European languages .
Hence , building a <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>named entity recognition<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> -LRB- <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>NER<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> -RRB- system for South Asian Languages -LRB- SAL -RRB- is still an open problem because they exhibit characteristics different from English .
This paper builds a <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>named entity recognizer<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> which also identifies <tag name="DOMAIN" value="start"/>nested name entities<tag name="DOMAIN" value="end"/> for the Hindi language using <tag name="TECHNIQUE" value="start"/>machine learning <tag name="TECHNIQUE" value="end"/> algorithm , trained on an annotated corpus .
However , the algorithm is designed in such a manner that it can easily be ported to other South Asian Languages provided the necessary NLP tools like <tag name="TECHNIQUE" value="start"/>POS tagger<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>chunker<tag name="TECHNIQUE" value="end"/> are available for that language .
I compare results of Hindi data with English data of CONLL shared task of 2003 .


##J87-3002
Large <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Lexicons<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> For Natural Language Processing : Utilising The <tag name="TECHNIQUE" value="start"/>Grammar Coding<tag name="TECHNIQUE" value="end"/> System Of LDOCE .
This article focusses on the derivation of large <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>lexicons<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> for natural language processing .
We describe thedevelopment of a  <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>dictionary<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> support environment  linking a restructured version of the Longman Dictionary of Contemporary English to natural language processing systems .
The process of <tag name="TECHNIQUE" value="start"/>restructuring<tag name="TECHNIQUE" value="end"/> the information in the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>machine readable version of the dictionary<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> is discussed .
The <tag name="TECHNIQUE" value="start"/>Longman grammar code<tag name="TECHNIQUE" value="end"/> system is used to construct <tag name="DOMAIN" value="start"/>` theory neutral ' lexical entries<tag name="DOMAIN" value="end"/> .
We demonstrate how such lexical entries can be put to practical use by linking up the system described here with the experimental PATR-II grammar development environment .
Finally , we offer an evaluation of the utility of the <tag name="TECHNIQUE" value="start"/>grammar coding<tag name="TECHNIQUE" value="end"/> system for use by automatic natural language  <tag name="DOMAIN" value="start"/>parsing<tag name="DOMAIN" value="end"/> systems .


##H92-1088
Towards Using <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Prosody<tag name="TECHNIQUE" value="end"/> In <tag name="DOMAIN" value="start"/>Speech Recognition\/Understanding <tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> Systems : Differences Between Read And Spontaneous Speech .
A persistent problem for keyword-driven <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>speech recognition<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> systems is that users often embed the to-be-recognized words or phrases in longer utterances .
The recognizer needs to locate the relevant sections of the speech signal and ignore extraneous words .
Prosody might provide an extra source of information to help locate target words embedded in other speech .
In this paper we examine some <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>prosodic <tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> characteristics of 160 such utterances and compare matched read and spontaneous versions .
Half of the utterances are from a corpus of spontaneous answers to requests for the name of a city , recorded from calls to Directory Assistance Operators .
The other half are the same word strings read by volunteers attempting to model the real dialogue .
Results show a consistent pattern across both sets of data : embedded city names almost always bear nuclear pitch accents and are in their own intonational phrases .
However the distributions of tonal make-up of these <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>prosodic features<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> differ markedly in read versus spontaneous speech , implying that if algorithms that exploit these prosodic regularities are trained on read speech , then the probabilities are likely to be incorrect models of real user speech .


##W04-3213
<tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Unsupervised<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>Semantic Role Labeling<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
We present an <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>unsupervised <tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> method for  labeling the arguments of verbs with their <tag name="DOMAIN" value="start"/> <tag name="FOCUS" value="start"/>semantic roles<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
Our <tag name="TECHNIQUE" value="start"/>bootstrapping<tag name="TECHNIQUE" value="end"/> algorithm makes initial unambiguous role assignments , and then iteratively updates the <tag name="TECHNIQUE" value="start"/>probability<tag name="TECHNIQUE" value="end"/> model on which future assignments are based .
A novel aspect of our approach is the use of <tag name="TECHNIQUE" value="start"/>verb , slot , and noun class <tag name="TECHNIQUE" value="end"/> information as the basis for <tag name="TECHNIQUE" value="start"/>backing off<tag name="TECHNIQUE" value="end"/> in our <tag name="TECHNIQUE" value="start"/>probability<tag name="TECHNIQUE" value="end"/> model .
We achieve 50 -- 65 % reduction in the error rate over an informed baseline , indicating the potential of our approach for a task that has heretofore relied on large amounts of manually generated training data .


##W03-1603
Preferential Presentation Of Japanese<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/> Near-Synonyms<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> Using <tag name="TECHNIQUE" value="start"/>Definition<tag name="TECHNIQUE" value="end"/> Statements .
This paper proposes a new method of <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>ranking near-synonyms<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> ordered by their suitability of nuances in a particular context .
Our method distincts <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>near-synonyms<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> by <tag name="TECHNIQUE" value="start"/>semantic features<tag name="TECHNIQUE" value="end"/> extracted from their <tag name="TECHNIQUE" value="start"/>definition<tag name="TECHNIQUE" value="end"/> statements in an ordinary <tag name="TECHNIQUE" value="start"/>dictionary<tag name="TECHNIQUE" value="end"/> , and ranks them by the types of features and a particular context .
Our method is an initial step to achieve a <tag name="DOMAIN" value="start"/>semantic paraphrase <tag name="DOMAIN" value="end"/> system for authoring support .


##W06-1007
<tag name="FOCUS" value="start"/>Structural Properties<tag name="FOCUS" value="end"/> Of <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/> Lexical Systems<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> : Monolingual And Multilingual Perspectives .
We introduce a new type of <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>lexical structure<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> called <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>lexical system<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> , an interoperable model that can feed both monolingual and multilingual language resources .
We begin with a formal characterization of <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>lexical systems<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> as `` pure '' <tag name="TECHNIQUE" value="start"/>directed graphs<tag name="TECHNIQUE" value="end"/> , solely made up of nodes corresponding to lexical entities and links .
To illustrate our approach , we present data borrowed from a <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>lexical system<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> that has been generated from the French DiCo database .
We later explain how the compilation of the original dictionary-like database into a net-like one has been made possible .
Finally , we discuss the potential of the proposed <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>lexical structure<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> for designing <tag name="DOMAIN" value="start"/>multilingual lexical resources<tag name="DOMAIN" value="end"/> .


##P07-2055
A <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Hybrid<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> Approach to <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Word Segmentation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> and <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>POS Tagging<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
In this paper , we present a <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>hybrid <tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> method for <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>word segmentation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> and <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>POS tagging<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
The target languages are those in which word boundaries are ambiguous , such as Chinese and Japanese .
In the method , <tag name="TECHNIQUE" value="start"/>word-based<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>character-based processing<tag name="TECHNIQUE" value="end"/> is combined , and <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>word segmentation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> and <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>POS tagging<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> are conducted simultaneously .
Experimental results on multiple corpora show that the integrated method has high accuracy .


##H94-1120
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Natural Language Planning Dialogue<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> For Interactive .


##W06-3325
The Difficulties Of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Taxonomic Name<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Extraction And A Solution .
In modern biology , digitization of biosystematics publications is an important task .
Extraction of <tag name="DOMAIN" value="start"/>taxonomic names<tag name="DOMAIN" value="end"/> from such documents is one of its major issues .
This is because these names identify the various genera and species .
This article reports on our experiences with <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>learning<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> techniques for this particular task .
We say why established <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Named-Entity Recognition<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> techniques are somewhat difficult to use in our context .
One reason is that we have only very little training data available .
Our experiments show that a combining approach that relies on <tag name="TECHNIQUE" value="start"/>regular expressions<tag name="TECHNIQUE" value="end"/> , <tag name="TECHNIQUE" value="start"/>heuristics<tag name="TECHNIQUE" value="end"/> , and <tag name="TECHNIQUE" value="start"/>word-level language recognition<tag name="TECHNIQUE" value="end"/> achieves very high precision and recall and allows to cope with those difficulties .


##N09-2065
Recognising the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Predicate-argument Structure<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> of Tagalog .
This paper describes research on <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Tagalog text for <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>predicate -- argument structure -LRB- PAS -RRB-<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
We first outline the linguistic phenomenon and corpus annotation process , then detail a series of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>PAS parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> experiments .


##W09-0435
A <tag name="TECHNIQUE" value="start"/>POS-Based <tag name="TECHNIQUE" value="end"/> Model for <tag name="FOCUS" value="start"/>Long-Range Reorderings in <tag name="DOMAIN" value="start"/>SMT<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
In this paper we describe a new approach to model <tag name="FOCUS" value="start"/>long-range word reorderings<tag name="FOCUS" value="end"/> in   <tag name="FOCUS" value="start"/> <tag name="TECHNIQUE" value="start"/>statistical<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>machine translation -LRB- SMT -RRB-<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
Until now , most SMT approaches are only able to model local reorderings .
But even the word order of related languages like German and English can be very different .
In recent years approaches that reorder the source sentence in a preprocessing step to better match target sentences according to <tag name="TECHNIQUE" value="start"/>POS -LRB- Part-of-Speech -RRB- - based rules<tag name="TECHNIQUE" value="end"/> have been applied successfully .
We enhance this approach to model <tag name="FOCUS" value="start"/> long-range reorderings<tag name="FOCUS" value="end"/> by introducing <tag name="TECHNIQUE" value="start"/> discontinuous rules<tag name="TECHNIQUE" value="end"/> .
We tested this new approach on a GermanEnglish <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> task and could significantly improve the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> quality , by up to 0.8 BLEU points , compared to a system which already uses continuous POSbased rules to model short-range reorderings .


##N09-1066
Using <tag name="TECHNIQUE" value="start"/>Citations<tag name="TECHNIQUE" value="end"/> to <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Generate surveys of Scientific Paradigms<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
The number of research publications in various disciplines is growing exponentially .
Researchers and scientists are increasingly finding themselves in the position of having to quickly understand large amounts of technical material .
In this paper we present the first steps in producing an automatically generated , readily consumable , <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>technical survey<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
Specifically we explore the combination of <tag name="TECHNIQUE" value="start"/>citation information<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>summarization <tag name="TECHNIQUE" value="end"/> techniques .
Even though prior work -LRB- Teufel et al. , 2006 -RRB- argues that citation text is unsuitable for summarization , we show that in the framework of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>multi-document survey creation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , citation texts can play a crucial role .


##H05-1069
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Word Sense Disambiguation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> Using <tag name="TECHNIQUE" value="start"/>Sense Examples<tag name="TECHNIQUE" value="end"/> Automatically Acquired From A Second Language .
We present a novel <tag name="TECHNIQUE" value="start"/>almost-unsupervised<tag name="TECHNIQUE" value="end"/> approach to the task of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Word Sense Disambiguation<tag name="FOCUS" value="end"/> -LRB- WSD -RRB-<tag name="DOMAIN" value="end"/> .
We build <tag name="TECHNIQUE" value="start"/>sense examples<tag name="TECHNIQUE" value="end"/> automatically , using large quantities of Chinese text , and English-Chinese and Chinese-English bilingual dictionaries , taking advantage of the observation that mappings between words and meanings are often different in typologically distant languages .
We train a <tag name="TECHNIQUE" value="start"/>classifier<tag name="TECHNIQUE" value="end"/> on the <tag name="TECHNIQUE" value="start"/>sense examples<tag name="TECHNIQUE" value="end"/> and test it on a gold standard English WSD dataset .
The evaluation gives results that exceed previous state-of-the-art results for comparable systems .
We also demonstrate that a little manual effort can improve the quality of sense examples , as measured by <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>WSD<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> accuracy .
The performance of the <tag name="TECHNIQUE" value="start"/>classifier<tag name="TECHNIQUE" value="end"/> on <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>WSD<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> also improves as the number of training sense examples increases .


##C00-1029
A <tag name="FOCUS" value="start"/>Class-Based Probabilistic<tag name="FOCUS" value="end"/> Approach To <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Structural Disambiguation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
Knowledge of which words are able to fill p ~ rticular argum .
ent slots of a predicate can be used tbr <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>structural disambiguation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
This paper describes a proposal : for acquiring such knowledge , and in line with much of the recent work in this area , a <tag name="TECHNIQUE" value="start"/>probabilistic<tag name="TECHNIQUE" value="end"/> approach is taken .
We develop a novel way of using a <tag name="TECHNIQUE" value="start"/>semantic hierarchy<tag name="TECHNIQUE" value="end"/> to estimate the <tag name="TECHNIQUE" value="start"/>probabilities<tag name="TECHNIQUE" value="end"/> , and demonstrate the general approach using a <tag name="TECHNIQUE" value="start"/>prepositional phrase atta<tag name="TECHNIQUE" value="end"/> .
chment experiment .


##C08-1054
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Coordination Disambiguation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> without Any Similarities .
The use of similarities has been one of the main approaches to resolve the ambiguities of coordinate structures .
In this paper , we present an alternative method for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>coordination disambiguation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , which does not use similarities .
Our hypothesis is that coordinate structures are supported by surrounding <tag name="TECHNIQUE" value="start"/>dependency relations<tag name="TECHNIQUE" value="end"/> , and that such <tag name="TECHNIQUE" value="start"/>dependency relations<tag name="TECHNIQUE" value="end"/> rather yield similarity between conjuncts , which humans feel .
Based on this hypothesis , we built a Japanese <tag name="TECHNIQUE" value="start"/>fully-lexicalized generative<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>parser<tag name="DOMAIN" value="end"/> that includes <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>coordination disambiguation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
Experimental results on web sentencesindicatedtheeffectivenessofor approach , and endorsed our hypothesis .


##N09-2037
Evaluating the <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Syntactic Transformations<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> in Gold Standard Corpora for <tag name="FOCUS" value="start"/>Statistical <tag name="DOMAIN" value="start"/>Sentence Compression<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
We present a <tag name="TECHNIQUE" value="start"/>policy-based eror analysis<tag name="TECHNIQUE" value="end"/> aproach that demonstrates a limitation to the curent comonly adopted paradigm for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>sentence compresion<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We demonstrate that these limitations arise from the strong asumption of locality of the decision making proces in the search for an aceptable derivation in this paradigm .


##W08-1109
The Use of <tag name="TECHNIQUE" value="start"/>Spatial Relations<tag name="TECHNIQUE" value="end"/> in <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Referring Expression Generation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
There is a prevailing assumption in the literature on <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>referring expression generation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> that relations are used in descriptions only ` as a last resort ' , typically on the basis that including the second entity in the relation introduces an additional cognitive load for either speaker or hearer .
In this paper , we describe an experiemt that attempts to test this assumption ; we determine that , even in simple scenes where the use of relations is not strictly required in order to identify an entity , relations are in fact often used .
We draw some conclusions as to what this means for the development of algorithms for the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>generation of referring expressions<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .


##P08-1072
Robust <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Dialog Management<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> with <tag name="TECHNIQUE" value="start"/>N-Best Hypotheses<tag name="TECHNIQUE" value="end"/> Using <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Dialog<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> <tag name="TECHNIQUE" value="start"/>Examples and Agenda<tag name="TECHNIQUE" value="end"/> .
This work presents an <tag name="TECHNIQUE" value="start"/>agenda-based<tag name="TECHNIQUE" value="end"/> approach to improve the robustness of the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>dialog manager<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> by using <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>dialog<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> <tag name="TECHNIQUE" value="start"/>examples and n-best recognition hypotheses<tag name="TECHNIQUE" value="end"/> .
This approach supports <tag name="TECHNIQUE" value="start"/>n-best hypotheses<tag name="TECHNIQUE" value="end"/> in the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>dialog manager<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> and keeps track of the dialog state using a discourse interpretation algorithm with the <tag name="TECHNIQUE" value="start"/>agenda graph and focus stack<tag name="TECHNIQUE" value="end"/> .
Given the <tag name="TECHNIQUE" value="start"/>agenda graph<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>n-best hypotheses<tag name="TECHNIQUE" value="end"/> , the system can predict the next system actions to maximize <tag name="TECHNIQUE" value="start"/>multi-level score functions<tag name="TECHNIQUE" value="end"/> .
To evaluate the proposed method , a <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>spoken dialog<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> system for a building guidance robot was developed .
Preliminary evaluation shows this approach would be effective to improve the robustness of <tag name="TECHNIQUE" value="start"/>example-based<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>dialog<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> modeling .


##P91-1025
Resolving <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Mismatches With <tag name="TECHNIQUE" value="start"/>Information Flow<tag name="TECHNIQUE" value="end"/> .
Languages differ in the concepts and real-world entities for which they have words and grammatical constructs .
Therefore <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>translation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> must sometimes be a matter of approximating the meaning of a source language text rather than finding an exact counterpart in the target language .
We propose a <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> framework based on <tag name="TECHNIQUE" value="start"/>Situation Theory<tag name="TECHNIQUE" value="end"/> .
The basic ingredients are an <tag name="TECHNIQUE" value="start"/>information lattice<tag name="TECHNIQUE" value="end"/> , a representation scheme for  <tag name="TECHNIQUE" value="start"/>utterances embedded in contexts<tag name="TECHNIQUE" value="end"/> , and a mismatch resolution scheme defined in terms of  <tag name="TECHNIQUE" value="start"/>information flow<tag name="TECHNIQUE" value="end"/> .
We motivate our approach with examples of <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>translation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> between English and Japanese .


##C04-1093
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Summarizing Encyclopedic Term Descriptions<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> On The Web .
We are developing an automatic method to compile an <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>encyclopedic corpus<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> from the Web .
In our previous work , paragraph-style descriptions for a term are extracted from Web pages and organized based on domains .
However , these descriptions are independent and do not comprise a condensed text as in hand-crafted encyclopedias .
To resolve this problem , we propose a <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>summarization<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> method , which produces a single text from multiple descriptions .
The resultant summary concisely describes a term from different viewpoints .
We also show the eï¬ectiveness of our method by means of experiments .


##P96-1021
A Polynomial-Time Algorithm For <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Statistical <tag name="TECHNIQUE" value="end"/><tag name="DOMAIN" value="start"/>Machine Translation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
We introduce a polynomial-time algorithm for <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>statistical <tag name="TECHNIQUE" value="end"/><tag name="DOMAIN" value="start"/>machine translation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
This algorithm can be used in place of the expensive , slow best-first search strategies in current <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>statistical<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>translation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> architectures .
The approach employs the <tag name="TECHNIQUE" value="start"/>stochastic bracketing transduction grammar -LRB- SBTG -RRB- <tag name="TECHNIQUE" value="end"/> model we recently introduced to replace earlier word alignment channel models , while retaining a <tag name="TECHNIQUE" value="start"/>bigram language model<tag name="TECHNIQUE" value="end"/> .
The new algorithm in our experience yields major speed improvement with no significant loss of accuracy .


##W01-0704
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Semantic Pattern Learning<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> Through <tag name="TECHNIQUE" value="start"/>Maximum Entropy-Based WSD<tag name="TECHNIQUE" value="end"/> Technique .
This paper describes a Natural Language Learning method that extracts knowledge in the form of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>semantic patterns<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> with ontology elements associated to syntactic components in the text .
The method combines the use of <tag name="TECHNIQUE" value="start"/>EuroWordNet 's ontological concepts<tag name="TECHNIQUE" value="end"/> and the correct sense of each word assigned by a <tag name="TECHNIQUE" value="start"/>Word Sense Disambiguation -LRB- WSD -RRB-<tag name="TECHNIQUE" value="end"/> module to extract three sets of patterns : subject-verb , verb-direct object and verb-indirect object .
These sets define the semantic behavior of the main textual elements based on their syntactic role .
On the one hand , it is shown that <tag name="TECHNIQUE" value="start"/>Maximum Entropy<tag name="TECHNIQUE" value="end"/> models applied to <tag name="TECHNIQUE" value="start"/>WSD<tag name="TECHNIQUE" value="end"/> tasks provide good results .
The evaluation of the <tag name="TECHNIQUE" value="start"/>WSD <tag name="TECHNIQUE" value="end"/> module has revealed a accuracy rate of 64 % in a preliminary test .
On the other hand , we explain how an adequate set of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>semantic or ontological patterns<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> can improve the success rate of NLP tasks such us pronoun resolution .
We have implemented both modules in C + + and although the evaluation has been performed for English , their general features allow the treatment of other languages like Spanish .
a1This paper has been partially supported by the Spanish Government -LRB- CICYT -RRB- project number TIC2000-0664-C0202 .


##C92-2089
A <tag name="TECHNIQUE" value="start"/>Feature-Based<tag name="TECHNIQUE" value="end"/> Model For <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Lexical Databases<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
To date , no fully suitable data model for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>lexical databases<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> has been proposed .
As <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>lexical databases<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> have prolifcrated in multiple formats , there has been growing concern over the reusability of lexical resources .
In this paper , we propose a model based on <tag name="TECHNIQUE" value="start"/>feature structures<tag name="TECHNIQUE" value="end"/> which overcomes most of the problems inherent in classical database models , anti in particular enables accessing , manipulating or merging information structured in multiple ways .
Because of their widespread use in file representation of linguistic information , the applicability of <tag name="TECHNIQUE" value="start"/>feature structures<tag name="TECHNIQUE" value="end"/> to <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>lexical databases<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> seems natural , although to our knowledge this has not yet been implemented .
The nse of <tag name="TECHNIQUE" value="start"/>feature structures<tag name="TECHNIQUE" value="end"/> in <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>lexical databases<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> also opens up the possibility of compatibility with computational lexicons .


##W06-1640
<tag name="TECHNIQUE" value="start"/>Partially Supervised Coreference Resolution<tag name="TECHNIQUE" value="end"/> For <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Opinion Summarization<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Through <tag name="TECHNIQUE" value="start"/>Structured Rule Learning<tag name="TECHNIQUE" value="end"/> .
Combining fine-grained opinion information to produce <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>opinion summaries<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> is important for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>sentiment analysis<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> applications .
Toward that end , we tackle the problem of source <tag name="DOMAIN" value="start"/>coreference resolution<tag name="DOMAIN" value="end"/> -- linking together source mentions that refer to the same entity .
The <tag name="TECHNIQUE" value="start"/>partially supervised<tag name="TECHNIQUE" value="end"/> nature of the problem leads us to define and approach it as the novel problem of <tag name="TECHNIQUE" value="start"/>partially supervised clustering<tag name="TECHNIQUE" value="end"/> .
We propose and evaluate a new algorithm for the task of <tag name="TECHNIQUE" value="start"/>source coreference resolution<tag name="TECHNIQUE" value="end"/> that outperforms competitive baselines .


##D07-1118
Building <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Domain-Specific Taggers<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> without Annotated -LRB- Domain -RRB- Data .
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Part of spech taging<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> is a fundamental component in many NLP systems .
When tagers developed in one domain are used in another domain , the perforance can degrade considerably .
We present a method for developing tagers for new doains without requiring POS anotated text in the ne domain .
Our method involves using <tag name="TECHNIQUE" value="start"/>raw doain text<tag name="TECHNIQUE" value="end"/> and identifying <tag name="TECHNIQUE" value="start"/>related ords<tag name="TECHNIQUE" value="end"/> to form a domain specific <tag name="TECHNIQUE" value="start"/>lexicon<tag name="TECHNIQUE" value="end"/> .
This <tag name="TECHNIQUE" value="start"/>lexicon<tag name="TECHNIQUE" value="end"/> provides the initial lexical probabilities for <tag name="TECHNIQUE" value="start"/>EM trainig<tag name="TECHNIQUE" value="end"/> of an <tag name="TECHNIQUE" value="start"/>HM model<tag name="TECHNIQUE" value="end"/> .
We evaluate the method by aplying it in the <tag name="DOMAIN" value="start"/>Biolgy doain<tag name="DOMAIN" value="end"/> and show that we achieve results that are comparable ith some tagers developed for this domain .


##E09-2003
<tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Grammatical<tag name="TECHNIQUE" value="end"/> Framework <tag name="DOMAIN" value="start"/>Web Service<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
We present a <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>web service for natural language parsing , prediction , generation , and translation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> using <tag name="TECHNIQUE" value="start"/>grammars in Portable Grammar Format<tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/>PGF<tag name="TECHNIQUE" value="end"/> -RRB- , the target format of the <tag name="TECHNIQUE" value="start"/>Grammatical Framework -LRB- GF -RRB- grammar compiler<tag name="TECHNIQUE" value="end"/> .
The <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>web service<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> implementation is open source , works with any <tag name="TECHNIQUE" value="start"/>PGF grammar<tag name="TECHNIQUE" value="end"/> , and with any web server that supports FastCGI .
The service exposes a simple interface which makes it possible to use it for interactive natural language <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>web applications<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We describe the functionality and interface of the<tag name="FOCUS" value="start"/> <tag name="DOMAIN" value="start"/>web service<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> , and demonstrate several applications built on top of it .


##W91-0114
<tag name="FOCUS" value="start"/>Shared Preferences<tag name="FOCUS" value="end"/> .
This paper attempts to develop a <tag name="FOCUS" value="start"/>theory of heuristics or preferences<tag name="FOCUS" value="end"/> that can be shared between <tag name="DOMAIN" value="start"/>understanding and generation systems<tag name="DOMAIN" value="end"/> .
We first develop a <tag name="FOCUS" value="start"/>formal analysis of preferences<tag name="FOCUS" value="end"/> and consider the relation between their uses in <tag name="DOMAIN" value="start"/>generation and understanding<tag name="DOMAIN" value="end"/> .
We then present a <tag name="TECHNIQUE" value="start"/>bidirectional<tag name="TECHNIQUE" value="end"/> algorithm for applying them and examine typical heuristics for lexical choice , scope and anaphora in : , more detail .


##C88-1003
<tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Functional Constraints<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> In <tag name="DOMAIN" value="start"/>Knowledge-Based Natural Language Understanding<tag name="DOMAIN" value="end"/> .
Many knowledge-based systems of semantic interpretation rely explicitly or implicitly on an assumption of structural isomorphy between syntaotic and semantic objects , handling exceptions by ad hoc measures .
In this paper I argue that <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>constraint equations<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> of the kind used in the LFG -LRB- or PATR - -RRB- formalisms provide a more general , and yet restricted formalism : in which not only isomorphic correspondences are expressible , but also many cases of non-isomorphic correspondences .
I illustrate with treatments of <tag name="DOMAIN" value="start"/>idioms<tag name="DOMAIN" value="end"/> , <tag name="DOMAIN" value="start"/>speech act interpretation<tag name="DOMAIN" value="end"/> and <tag name="DOMAIN" value="start"/>discourse pragmatics<tag name="DOMAIN" value="end"/> .


##W04-1312
Modelling Atypical <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/> Syntax Processing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We evaluate the inferences that can be drawn from <tag name="TECHNIQUE" value="start"/>dissociations<tag name="TECHNIQUE" value="end"/> in <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>syntax processing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> identified in developmental disorders and acquired language deficits .
We use an <tag name="TECHNIQUE" value="start"/>SRN<tag name="TECHNIQUE" value="end"/> to simulate empirical data from Dick et al. -LRB- 2001 -RRB- on the relative difficulty of comprehending different syntactic constructions under normal conditions and conditions of damage .
We conclude that <tag name="TECHNIQUE" value="start"/>task constraints<tag name="TECHNIQUE" value="end"/> and internal <tag name="TECHNIQUE" value="start"/>computational constraints<tag name="TECHNIQUE" value="end"/> interact to predict patterns of difficulty .
Difficulty is predicted by frequency of constructions , by the requirement of the task to focus on local vs. global sequence information , and by the ability of the system to maintain sequence information .
We generate a testable prediction on the empirical pattern that should be observed under conditions of developmental damage .


##A88-1008
Handling <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Scope Ambiguities<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> In English .
This paper describes a program for handling `` <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>scope ambiguities<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> '' in individual English sentences .
The program operates on initial <tag name="TECHNIQUE" value="start"/>logical translations<tag name="TECHNIQUE" value="end"/> , generated by a <tag name="TECHNIQUE" value="start"/>parser\/translator<tag name="TECHNIQUE" value="end"/> , in which `` unscoped elements '' such as quantifiers , coordinators and negation are left in place to be extracted and positioned by the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>scoping<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> program .
The program produces the set of valid scoped readings , omitting logically redundant readings , and places the readings in an approximate order of preference using a set of <tag name="TECHNIQUE" value="start"/>domain-independent heuristics<tag name="TECHNIQUE" value="end"/> .
The heuristics are based on information about the <tag name="TECHNIQUE" value="start"/>lexical type<tag name="TECHNIQUE" value="end"/> of each operator and on `` <tag name="TECHNIQUE" value="start"/>structural relations<tag name="TECHNIQUE" value="end"/> '' between pairs of operators .
The need for such <tag name="TECHNIQUE" value="start"/>domain-independent heuristics<tag name="TECHNIQUE" value="end"/> is emphasized ; in some cases they can be decisive and in general they will serve as a guide to the use of further heuristics based on domain-specific knowledge and on the context of discourse .
The emphasis of this paper is on discussing several of the more problematic aspects of the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>scoping<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> protocol which wcre encountered during the design of the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>scoping<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> program .


##W00-1402
A <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Task-Based<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> Framework To Evaluate <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Evaluative Arguments<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We present an evaluation framework in which the effectiveness of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>evaluative arguments<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> can be measured with real users .
The framework is based on the <tag name="TECHNIQUE" value="start"/>task-efficacy<tag name="TECHNIQUE" value="end"/> evaluation method .
An <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>evaluative argument<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> is presented in the context of a <tag name="DOMAIN" value="start"/>decision task<tag name="DOMAIN" value="end"/> and measures related to its effectiveness are assessed .
Within this framework , we are currently running a formal experiment to verify whether argument effectiveness can be increased by tailoring the argument to the user and by varying the degree of argument conciseness .


##W09-1408
A <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>memory-based learning<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> approach to <tag name="DOMAIN" value="start"/>event extraction in biomedical texts<tag name="DOMAIN" value="end"/> .
In this paper we describe the <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>memory-based machine learning<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> system that we submitted to the BioNLP Shared Task on <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Event Extraction<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We modeled the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>event extraction<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> task using an approach that has been previously applied to other natural language processing tasks like semantic role labeling or negation scope finding .
The results obtained by our system -LRB- 30.58 F-score in Task 1 and 29.27 in Task 2 -RRB- suggest that the approach and the system need further adaptation to the complexity involved in extracting <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>biomedical events<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .


##C04-1118
Controlling <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/> Gender Equality<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> With <tag name="TECHNIQUE" value="start"/>Shallow<tag name="TECHNIQUE" value="end"/> NLP Techniques .
This paper introduces the Gendercheck Editor '' , a tool to check German texts for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>gender discriminatory formulations<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
It relays on <tag name="TECHNIQUE" value="start"/>shallow rule-based<tag name="TECHNIQUE" value="end"/> techniques as used in the Controlled Language Authoring Technology -LRB- CLAT -RRB- .
The paper outlines major sources of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>gender imbalances<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> in German texts .
It gives a background on the underlying CLAT technology and describes the marking and annotation strategy to automatically detect and visualize the questionable pieces of text .
The paper provides a detailed evaluation of the editor .


##P91-1039
<tag name="TECHNIQUE" value="start"/>Factorization Of Language Constraints<tag name="TECHNIQUE" value="end"/> In <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Speech Recognition<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
Integration of language constraints into a large vocabulary speech recognition system often leads to prohibitive complexity .
We propose to <tag name="TECHNIQUE" value="start"/>factor the constraints<tag name="TECHNIQUE" value="end"/> into two components .
The first is characterized by a <tag name="TECHNIQUE" value="start"/>covering grammar<tag name="TECHNIQUE" value="end"/> which is small and easily integrated into existing <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>speech recognizers<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
The recognized string is then decoded by means of an efficient language post-processor in which the full set of <tag name="TECHNIQUE" value="start"/>constraints<tag name="TECHNIQUE" value="end"/> is imposed to correct possible errors introduced by the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>speech recognizer<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .


##I05-2011
Automatic Detection of <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Opinion Bearing Words and Sentences<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
We describe a sentence-level <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>opinion detection<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> system .
We first define what an opinion means in our research and introduce an effective method for obtaining <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>opinion-bearing and nonopinion-bearing words<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
Then we describe recognizing opinion-bearing sentences using these words We test the system on 3 different test sets : MPQA data , an internal corpus , and the TREC2003 Novelty track data .
We show that our automatic method for obtaining opinion-bearing words can be used effectively to identify <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>opinion-bearing sentences<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .


##H92-1085
Automatic Detection And Correction Of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Repairs In Human-Computer Dialog<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We have analyzed 607 sentences of spontaneous humancomputer <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>speech data containing repairs<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> -LRB- drawn from a corpus of 10,718 -RRB- .
We present here criteria and techniques for automatically detecting the presence of a  <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>repair<tag name="FOCUS" value="end"/> <tag name="DOMAIN" value="end"/> , its location , and making the appropriate correction.
The criteria involve integration of knowledge from several sources : <tag name="TECHNIQUE" value="start"/>pattern matching<tag name="TECHNIQUE" value="end"/> , <tag name="TECHNIQUE" value="start"/>syntactic and semantic<tag name="TECHNIQUE" value="end"/> analysis , and <tag name="TECHNIQUE" value="start"/>acoustics<tag name="TECHNIQUE" value="end"/> .


##J95-4003
<tag name="TECHNIQUE" value="start"/>Modularity And Information Content Classes<tag name="TECHNIQUE" value="end"/> In <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Principle-Based<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>Parsing<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
In recent years models of parsing that are isomorphic to a <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>principle-based theory of grammar<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> -LRB- most notably Government and Binding -LRB- GB -RRB- Theory -RRB- have been proposed -LRB- Berwick et al. 1991 -RRB- .
These models are natural and direct implementations of the grammar , but they are not efficient , because GB is not a computationally modular theory .
This paper investigates one problem related to the tension between building linguistically based parsers and building efficient ones .
In particular , the issue of what is a linguistically motivated way of deriving a <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>parser<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> from <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>principle-based theories of grammar<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> is explored .
It is argued that an efficient and faithful <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>parser<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> can be built by taking advantage of the way in which principles are stated .
To support this claim , two features of an implemented <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>parser<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> are discussed .
First , <tag name="TECHNIQUE" value="start"/>configurations and lexical information<tag name="TECHNIQUE" value="end"/> are precompiled separately into two tables -LRB- an X table and a table of <tag name="TECHNIQUE" value="start"/>lexical co-occurrence<tag name="TECHNIQUE" value="end"/> -RRB- which gives rise to more compact data structures .
Secondly , precomputation of <tag name="TECHNIQUE" value="start"/>syntactic features<tag name="TECHNIQUE" value="end"/> -LRB- O-roles , case , etc. -RRB- results in efficient computation of chains , because it reduces several problems of chain formation to a local computation , thus avoiding extensive search of the tree for an antecedent or extensive backtracking .
It is also shown that this method of building long-distance dependencies can be computed incrementally .


##N09-2017
Evaluation of a System for <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Noun Concepts Acquisition<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> from <tag name="TECHNIQUE" value="start"/>Utterances about Images<tag name="TECHNIQUE" value="end"/> -LRB- <tag name="FOCUS" value="start"/>SINCA<tag name="FOCUS" value="end"/> -RRB- Using Daily <tag name="TECHNIQUE" value="start"/>Conversation<tag name="TECHNIQUE" value="end"/> Data .
For a robot working in an open environment , a task-oriented language capability will not be sufficient .
In order to adapt to the environment , such a robot will have to learn language dynamically .
We developed a System for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Noun Concepts Acquisition<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> from utterances about Images , <tag name="FOCUS" value="start"/>SINCA<tag name="FOCUS" value="end"/> in short .
It is a <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>language acquisition<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> system without knowledge of grammar and vocabulary , which learns noun concepts from <tag name="TECHNIQUE" value="start"/>user utterances<tag name="TECHNIQUE" value="end"/> .
We recorded a video of a child 's daily life to collect <tag name="TECHNIQUE" value="start"/>dialogue<tag name="TECHNIQUE" value="end"/> data that was spoken to and around him .
The child is a member of a family consisting of the parents and his sister .
We evaluated the performance of <tag name="FOCUS" value="start"/>SINCA<tag name="FOCUS" value="end"/> using the collected data .
In this paper , we describe the algorithms of <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>SINCA<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> and an evaluation experiment .
We work on Japanese <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>language acquisition<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , however our method can easily be adapted to other languages .


##H01-1011
Automatic <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Title Generation For Spoken Broadcast News<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
In this paper , we implemented a set of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>title generation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> methods using <tag name="TECHNIQUE" value="start"/>training<tag name="TECHNIQUE" value="end"/> set of 21190 news stories and evaluated them on an independent test corpus of 1006 broadcast news documents , comparing the results over manual transcription to the results over automatically recognized speech .
We use both F1 and the average number of correct title words in the correct order as metric .
Overall , the results show that <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>title generation for speech recognized news documents<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/>  is possible at a level approaching the accuracy of titles generated for perfect text transcriptions .
Keywords Machine learning , title generation


##W97-0212
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Sense Tagging<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> In Action Combining Different Tests With <tag name="TECHNIQUE" value="start"/>Additive Weighangs<tag name="TECHNIQUE" value="end"/> .
This paper describes a working <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>sense tagger<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , which attempts to automatically link each word in a text corpus to its corresponding sense in a machinereadable <tag name="TECHNIQUE" value="start"/>dictionary<tag name="TECHNIQUE" value="end"/> .
It uses information automatically extracted from the <tag name="TECHNIQUE" value="start"/>MRD<tag name="TECHNIQUE" value="end"/> to find matches between the <tag name="TECHNIQUE" value="start"/>dictionary<tag name="TECHNIQUE" value="end"/> and the Corpus sentences , and combines different types of information by simple <tag name="TECHNIQUE" value="start"/>additive scores<tag name="TECHNIQUE" value="end"/> with manually <tag name="TECHNIQUE" value="start"/>set weightings<tag name="TECHNIQUE" value="end"/> .


##J88-3003
Modeling The User 's <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Plans And Goals<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> .
This work is an ongoing research effort aimed both at developing techniques for inferring and constructing a user model from an  <tag name="DOMAIN" value="start"/>information-seeking dialog<tag name="DOMAIN" value="end"/> and at identifying strategies for applying this model to enhance robust <tag name="DOMAIN" value="start"/>communication<tag name="DOMAIN" value="end"/> .
One of the most important components of a user model is a representation of the system 's beliefs about the underlying <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>task-related plan<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> motivating an information-seeker 's queries .
These beliefs can be used to interpret subsequent utterances and produce useful responses .
This paper describes the <tag name="FOCUS" value="start"/>IREPS<tag name="FOCUS" value="end"/> system , emphasizing its dynamic construction of the <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>task-related plan<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> motivating the information-seeker 's queries and the application of this component of a user model to handling utterances that violate the <tag name="TECHNIQUE" value="start"/>pragmatic rules<tag name="TECHNIQUE" value="end"/> of the system 's world model .
By reasoning on a model of the user 's <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>plans and goals<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> , the system often can deduce the intended meaning of faulty utterances and allow the <tag name="DOMAIN" value="start"/>dialogue<tag name="DOMAIN" value="end"/> to continue without interruption .
Some limitations of current plan inference systems are discussed .
It is suggested that the problem of detecting and recovering from discrepancies between the system 's model of the user 's <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>plan<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> and the actual plan under construction by the user requires an enriched model that differentiates among its components on the basis of the support the system accords each component as a correct and intended part of the user 's <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>plan<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> .


##E87-1012
A Tool For The Automatic Creation , Extension And Updating Of  <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Lexical Knowledge Bases<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
A tool is described which helps in the creation , extension and updating of <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>lexical knowledge bases<tag name="DOMAIN" value="end"/> -LRB- <tag name="DOMAIN" value="start"/>LKBs<tag name="DOMAIN" value="end"/> -RRB-<tag name="FOCUS" value="end"/> .
Two levels of representation are distinguished : a <tag name="TECHNIQUE" value="start"/>static storage level<tag name="TECHNIQUE" value="end"/> and a <tag name="TECHNIQUE" value="start"/>dynamic knowledge level<tag name="TECHNIQUE" value="end"/> .
The latter is an object-oriented environment containing <tag name="TECHNIQUE" value="start"/>linguistic and lexicographic knowledge<tag name="TECHNIQUE" value="end"/> .
At the knowledge level , constructors and filters can be defined .
Constructors are objects which extend the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>LKB<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> both horizontally -LRB- new information -RRB- and vertically -LRB- new entries -RRB- using the <tag name="TECHNIQUE" value="start"/>linguistic knowledge<tag name="TECHNIQUE" value="end"/> .
Filters are objects which derive new <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>LKBs<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> from existing ones thereby optionally changing the storage structure .
The latter use <tag name="TECHNIQUE" value="start"/>lexicographic knowledge<tag name="TECHNIQUE" value="end"/> .


##W98-1123
<tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Linear<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>Segmentation<tag name="DOMAIN" value="end"/> And <tag name="DOMAIN" value="start"/>Segment Significance<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
We present a new method for discovering a <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>segmental discourse structure<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> of a document while categorizing each segment 's function and importance .
Segments are determined by a <tag name="TECHNIQUE" value="start"/>zero-sum weighting<tag name="TECHNIQUE" value="end"/> scheme , used on occurrences of <tag name="TECHNIQUE" value="start"/>noun phrases and pronominal forms<tag name="TECHNIQUE" value="end"/> retrieved from the document .
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Segment roles<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> are then calculated from the distribution of the terms in the segment .
Finally , we present results of evaluation in terms of precision and recall which surpass earlier approaches ' .


##P97-1067
Choosing The Word Most Typical In Context Using A <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Lexical Co-Occurrence Network<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> .
This paper presents a partial solution to a component of the problem of <tag name="DOMAIN" value="start"/>lexical choice<tag name="DOMAIN" value="end"/> : choosing the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>synonym<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> most typical , or expected , in context .
We apply a new <tag name="TECHNIQUE" value="start"/>statistical<tag name="TECHNIQUE" value="end"/> approach to representing the context of a word through <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>lexical co-occurrence networks<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> .
The implementation was trained and evaluated on a large corpus , and results show that the inclusion of second-order <tag name="TECHNIQUE" value="start"/>co-occurrence relations<tag name="TECHNIQUE" value="end"/> improves the performance of our implemented lexical choice program .


##P08-1078
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Contextual Preferences<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
The validity of <tag name="DOMAIN" value="start"/>semantic inferences<tag name="DOMAIN" value="end"/> depends on the contexts in which they are applied .
We propose a generic framework for handling contextual considerations within applied inference , termed <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Contextual Preferences<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
This framework defines the various context-aware components needed for inference and their relationships .
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Contextual preferences<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> extend and generalize previous notions , such as selectional preferences , while experiments show that the extended framework allows improving inference quality on real application data .


##W04-1108
Combining <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Neural Networks<tag name="TECHNIQUE" value="end"/> <tag name="FOCUS" value="end"/> And <tag name="TECHNIQUE" value="start"/>Statistics <tag name="TECHNIQUE" value="end"/>For Chinese <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Word Sense Disambiguation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
The input of network is the key problem for Chinese Word sense disambiguation utilizing the Neural Network .
This paper presents an input model of <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Neural Network<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> that calculates the <tag name="TECHNIQUE" value="start"/>Mutual Information<tag name="TECHNIQUE" value="end"/> between contextual words and ambiguous word by using <tag name="TECHNIQUE" value="start"/>statistical<tag name="TECHNIQUE" value="end"/> method and taking the contextual words to certain number beside the ambiguous word according to -LRB- - M , + N -RRB- .
The experiment adopts triple-layer <tag name="TECHNIQUE" value="start"/>BP Neural Network<tag name="TECHNIQUE" value="end"/> model and proves how the size of training set and the value of M and N affect the performance of <tag name="TECHNIQUE" value="start"/>Neural Network<tag name="TECHNIQUE" value="end"/> model .
The experimental objects are six pseudowords owning three word-senses constructed according to certain principles .
Tested accuracy of our approach on a close-corpus reaches 90.31 % , , and 89.62 % on a open-corpus .
The experiment proves that the <tag name="TECHNIQUE" value="start"/>Neural Network<tag name="TECHNIQUE" value="end"/> model has good performance on <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Word sense disambiguation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .


##D08-1112
An Analysis of <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Active Learning<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> Strategies for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Sequence Labeling<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Tasks .
<tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Active learning<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> is well-suited to many problems in natural language processing , where unlabeled data may be abundant but annotation is slow and expensive .
This paper aims to shed light on the best <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>active learning<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> approaches for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>sequence labeling<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> tasks such as <tag name="DOMAIN" value="start"/>information extraction<tag name="DOMAIN" value="end"/> and <tag name="DOMAIN" value="start"/>document segmentation<tag name="DOMAIN" value="end"/> .
Wesurveypreviouslyusedqueryselection strategies for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>sequence<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> models , and propose several novel algorithms to address their shortcomings .
We also conduct a large-scale empirical comparison using multiple corpora , which demonstrates that our proposed methods advance the state of the art .


##H05-1002
<tag name="TECHNIQUE" value="start"/>Data-Driven<tag name="TECHNIQUE" value="end"/> Approaches For <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Information Structure<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Identification .
This paper investigates automatic identification of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Information Structure -LRB- IS -RRB-<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> in texts .
The experiments use the Prague Dependency Treebank which is annotated with IS following the Praguian approach of Topic Focus Articulation .
We automatically detect <tag name="DOMAIN" value="start"/>t -LRB- opic -RRB- and f -LRB- ocus -RRB-<tag name="DOMAIN" value="end"/> , using <tag name="TECHNIQUE" value="start"/>node attributes<tag name="TECHNIQUE" value="end"/> from the treebank as basic features and derived features inspired by the annotation guidelines .
We present the performance of <tag name="TECHNIQUE" value="start"/>decision trees -LRB- C4 .5 -RRB-<tag name="TECHNIQUE" value="end"/> , <tag name="TECHNIQUE" value="start"/>maximum entropy<tag name="TECHNIQUE" value="end"/> , and <tag name="TECHNIQUE" value="start"/>rule induction -LRB- RIPPER -RRB- classifiers<tag name="TECHNIQUE" value="end"/> on all tectogrammatical nodes .
We compare the results against a baseline system that always assigns f -LRB- ocus -RRB- and against a rule-based system .
The best system achieves an accuracy of 90.69 % , which is a 44.73 % improvement over the baseline -LRB- 62.66 % -RRB- .


##W06-2502
<tag name="TECHNIQUE" value="start"/>Cluster Stopping Rules<tag name="TECHNIQUE" value="end"/> For <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Word Sense Discrimination<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
As text data becomes plentiful , <tag name="TECHNIQUE" value="start"/>unsupervised<tag name="TECHNIQUE" value="end"/> methods for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Word Sense Disambiguation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> -LRB- <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>WSD<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> -RRB- become more viable .
A problem encountered in applying <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>WSD<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> methods is finding the exact number of senses an ambiguity has in a training corpus collected in an automated manner .
That number is not known a priori ; rather it needs to be determined based on the data itself .
We address that problem using <tag name="TECHNIQUE" value="start"/>cluster stopping<tag name="TECHNIQUE" value="end"/> methods .
Such techniques have not previously applied to WSD .
We implement the methods of Calinski and Harabasz -LRB- 1975 -RRB- and Hartigan -LRB- 1975 -RRB- and our adaptation of the <tag name="TECHNIQUE" value="start"/>Gap statistic<tag name="TECHNIQUE" value="end"/> -LRB- Tibshirani , Walter and Hastie , 2001 -RRB- .
For evaluation , we use the <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>WSD<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> Test Set from the National Library of Medicine , whose sense inventory is the Unified Medical Language System .
The best accuracy for selecting the correct number of clusters is 0.60 with the C&H method .
Our error analysis shows that the cluster stopping methods make finergrained sense distinctions by creating additional clusters .
The highest F-scores -LRB- 82.89 -RRB- , indicative of the quality of cluster membership assignment , are comparable to the baseline majority sense -LRB- 82.63 -RRB- and point to a path towards accuracy improvement via additional cluster pruning .
The importance and significance of the current work is in applying <tag name="TECHNIQUE" value="start"/>cluster stopping rules<tag name="TECHNIQUE" value="end"/> to <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>WSD<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .


##W91-0102
Reversibility In A <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Constraint And Type Based Logic Grammar<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> : Application To <tag name="DOMAIN" value="start"/>Secondary Predication<tag name="DOMAIN" value="end"/> .
In this document , we present a <tag name="FOCUS" value="start"/>formalism<tag name="FOCUS" value="end"/> for natural language processing which associates <tag name="TECHNIQUE" value="start"/>type construction principles<tag name="TECHNIQUE" value="end"/> to <tag name="TECHNIQUE" value="start"/>constraint logic programming<tag name="TECHNIQUE" value="end"/> .
We show that it provides more uniform , expressive and efficient tools for <tag name="DOMAIN" value="start"/>parsing<tag name="DOMAIN" value="end"/> and <tag name="DOMAIN" value="start"/>generating language<tag name="DOMAIN" value="end"/> .
Next , we present two abstract machines which enable us to design , in a symmetric way , a <tag name="DOMAIN" value="start"/>parser<tag name="DOMAIN" value="end"/> and a <tag name="DOMAIN" value="start"/>generator<tag name="DOMAIN" value="end"/> from that formalism .
This abstract machinery is then exemplified by a detailed study of <tag name="DOMAIN" value="start"/>secondary predication<tag name="DOMAIN" value="end"/> within the framework of a principledbased description of language : Government and Binding theory .


##C08-1023
Pedagogically Useful <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Extractive Summaries<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Science Education<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
This paper describes the design and evaluation of an <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>extractive summarizer<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>educational science<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> content called <tag name="FOCUS" value="start"/>COGENT<tag name="FOCUS" value="end"/> .
<tag name="FOCUS" value="start"/>COGENT<tag name="FOCUS" value="end"/> extends MEAD based on strategies elicited from an empirical study with science domain and instructional design experts .
<tag name="FOCUS" value="start"/>COGENT<tag name="FOCUS" value="end"/> identifies sentences containing pedagogically relevant concepts for a specific science domain .
The algorithms pursue a <tag name="TECHNIQUE" value="start"/>hybrid<tag name="TECHNIQUE" value="end"/> approach integrating both domain independent <tag name="TECHNIQUE" value="start"/>bottom-up sentence scoring features<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>domain-aware top-down features<tag name="TECHNIQUE" value="end"/> .
Evaluation results indicate that <tag name="FOCUS" value="start"/>COGENT<tag name="FOCUS" value="end"/> outperforms existing <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>summarizers<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> and generates <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>summaries<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> that closely resemble those generated by human experts .
<tag name="FOCUS" value="start"/>COGENT<tag name="FOCUS" value="end"/> concept inventories appear to also support the computational identification of student misconceptions about earthquakes and plate tectonics .


##D08-1090
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Language and Translation<tag name="FOCUS" value="end"/> <tag name="DOMAIN" value="end"/> Model Adaptation using Comparable Corpora .
Traditionally , statistical machine translation systems have relied on parallel bi-lingual data to train a translation model .
While bi-lingual parallel data are expensive to generate , monolingual data are relatively common .
Yet monolingual data have been under-utilized , having been used primarily for training a language model in the target language .
This paper describes a novel method for utilizing monolingual target data to improve the performance of a <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>statistical<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>machine translation system<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> on news stories .
The method exploits the existence of comparable text -- multiple texts in the target language that discuss the same or similar stories as found in the source language document .
For every source document that is to be translated , a large monolingual data set in the target language is searched for documents that might be comparable to the source documents .
These documents are then used to adapt the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>MT<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> system to increase the probability of generating texts that resemble the comparable document .
Experimental results obtained by adapting both the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>language and translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> models show substantial gains over the baseline system .


##N06-2039
<tag name="TECHNIQUE" value="start"/>Unsupervised<tag name="TECHNIQUE" value="end"/> Induction Of Modern Standard Arabic <tag name="DOMAIN" value="start"/>Verb Classes<tag name="DOMAIN" value="end"/> .
We exploit the resources in the Arabic Treebank -LRB- ATB -RRB- for the novel task of automatically creating <tag name="DOMAIN" value="start"/>lexical semantic verb classes<tag name="DOMAIN" value="end"/> for Modern Standard Arabic -LRB- MSA -RRB- .
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Verbs<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> are <tag name="TECHNIQUE" value="start"/>clustered<tag name="TECHNIQUE" value="end"/> into groups that share <tag name="TECHNIQUE" value="start"/>semantic elements of meaning<tag name="TECHNIQUE" value="end"/> as they exhibit similar <tag name="TECHNIQUE" value="start"/>syntactic behavior<tag name="TECHNIQUE" value="end"/> .
The results of the <tag name="TECHNIQUE" value="start"/>clustering<tag name="TECHNIQUE" value="end"/> experiments are compared with a gold standard set of classes , which is approximated by using the noisy English translations provided in the ATB to create Levin-like classes for MSA .
The quality of the clusters is found to be sensitive to the inclusion of information about <tag name="TECHNIQUE" value="start"/>lexical heads of the constituents in the syntactic frames<tag name="TECHNIQUE" value="end"/> , as well as parameters of the <tag name="TECHNIQUE" value="start"/>clustering<tag name="TECHNIQUE" value="end"/> algorithm .
The best set of parameters yields an F Î² = 1 score of 0.501 , compared to a random baseline with an F Î² = 1 score of 0.37 .


##W07-1416
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Textual Entailment<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Using <tag name="TECHNIQUE" value="start"/>Univariate Density<tag name="TECHNIQUE" value="end"/> Model and <tag name="TECHNIQUE" value="start"/>Maximizing Discriminant Function<tag name="TECHNIQUE" value="end"/> .
The primary focuses of this entry this year was firstly , to develop a framework to allow multiple researchers from our group to easily contribute metrics measuring <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>textual entailment<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> , and secondly , to provide a baseline which we could use in our tools to evaluate and compare new metrics .
A development environment tool was created to quickly allow for testing of various metrics and to easily randomize the development and test sets .
For each test , this RTE tool calculated two sets of results by applying the metrics to both a <tag name="TECHNIQUE" value="start"/>univariate Gaussian density<tag name="TECHNIQUE" value="end"/> and by <tag name="TECHNIQUE" value="start"/>maximizing a linear discriminant function<tag name="TECHNIQUE" value="end"/> .
The metrics used for the submission were a <tag name="TECHNIQUE" value="start"/>lexical similarity<tag name="TECHNIQUE" value="end"/> metric and a <tag name="TECHNIQUE" value="start"/>lexical similarity<tag name="TECHNIQUE" value="end"/> metric using <tag name="TECHNIQUE" value="start"/>synonym<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>antonym<tag name="TECHNIQUE" value="end"/> replacement .
The two submissions for RTE 2007 scored an accuracy of 61.00 % and 62.62 % .


##H93-1066
A <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Speech-First<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> Model For <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Repair Detection And Correction<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
Interpreting fttUy natural speech is an important goal for spoken language understanding systems .
However , while corpus studies have shown that about 10 % of spontaneous utterances contain <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>selfcorrections<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , or <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>REPAIRS<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> , little is known about the extent to which cues in the speech signal may facilitate repair processing .
We identify several cues based on <tag name="TECHNIQUE" value="start"/>acoustic<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>prosodic<tag name="TECHNIQUE" value="end"/> analysis of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>repairs<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> in the DARPA Air Travel In .
formation System database , and propose methods for exploiting these cues to <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>detect and correct repairs<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .


##C08-5001
Advanced <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Dynamic Programming<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> in <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Semiring and Hypergraph Frameworks<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
<tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Dynamic Programming<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>DP<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> -RRB- is an important class of algorithms widely used in many areas of speech and language processing .
Recently there have been a series of work trying to formalize many instances of <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>DP<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> algorithms under algebraic and graph-theoretic frameworks .
This tutorial surveys two such frameworks , namely <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>semirings and directed hypergraphs<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , and draws connections between them .
We formalize two particular types of <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>DP<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> algorithms under each of these frameworks : the <tag name="TECHNIQUE" value="start"/>Viterbi-style topological<tag name="TECHNIQUE" value="end"/> algorithms and the <tag name="TECHNIQUE" value="start"/>Dijkstra-style best-first<tag name="TECHNIQUE" value="end"/> algorithms .
Wherever relevant , we also discuss typical applications of these algorithms in Natural Language Processing .


##W05-0836
Training And Evaluating <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Error Minimization Decision Rules<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> For <tag name="TECHNIQUE" value="start"/>Statistical<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>Machine Translation<tag name="DOMAIN" value="end"/> .
<tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Decision rules<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> that explicitly account for non-probabilistic evaluation metrics in machine translation typically require special training , often to estimate parameters in exponential models that govern the search space and the selection of candidate translations .
While the traditional Maximum A Posteriori -LRB- MAP -RRB- decision rule can be optimized as a piecewise linear function in a greedy search of the parameter space , the Minimum Bayes Risk -LRB- MBR -RRB- decision rule is not well suited to this technique , a condition that makes past results difficult to compare .
We present a novel training approach for <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>non-tractable decision rules<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> , allowing us to compare and evaluate these and other decision rules on a large scale <tag name="DOMAIN" value="start"/>translation<tag name="DOMAIN" value="end"/> task , taking advantage of the high dimensional parameter space available to the phrase based Pharaoh decoder .
This comparison is timely , and important , as decoders evolve to represent more complex search space decisions and are evaluated against innovative evaluation metrics of translation quality .


##C94-1084
Towards Automatic Extraction Of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Monolingual And Bilingual Terminology<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
In this paper , we make use of <tag name="TECHNIQUE" value="start"/>linguistic<tag name="TECHNIQUE" value="end"/> knowledge to identify certain <tag name="DOMAIN" value="start"/>noun phrases<tag name="DOMAIN" value="end"/> , both in English and French , which are likely to be <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>terms<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We then test and cmnl -RRB- are -LRB- lifl ` e. rent statistical scores to select the `` good '' ones among tile candidate terms , and finally propose a <tag name="TECHNIQUE" value="start"/>statistical<tag name="TECHNIQUE" value="end"/> method to build correspondences of multi-words units across languages .
Acknowledgement Most of this work was carried out under project EUII .
OTP ~ A ET-10 \/ 63 , co-sponsored by the European Economic Conmmnity .


##N04-1008
Automatic <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Question Answering<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> : Beyond The Factoid .
In this paper we describe and evaluate a <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Question Answering<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> system that goes beyond answering factoid questions .
We focus on <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>FAQlike questions and answers<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> , and build our system around a <tag name="TECHNIQUE" value="start"/>noisy-channel<tag name="TECHNIQUE" value="end"/> architecture which exploits both a <tag name="TECHNIQUE" value="start"/>language model<tag name="TECHNIQUE" value="end"/> for answers and a <tag name="TECHNIQUE" value="start"/>transformation<tag name="TECHNIQUE" value="end"/> model for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>answer\/question<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> terms , trained on a corpus of 1 million question\/answer pairs collected from the Web .


##W97-0613
The `` Casual Cashmere Diaper Bag '' : Constraining <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Speech Recognition<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Using <tag name="TECHNIQUE" value="start"/>Examples<tag name="TECHNIQUE" value="end"/> .
We describe a new technology for using small collections of <tag name="TECHNIQUE" value="start"/>example<tag name="TECHNIQUE" value="end"/> sentences to automatically restrict a <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>speech recognition grammar<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> to allow only the more plausible subset of the sentences it would otherwise admit .
This technology is unusual because it bridges the gap between hand-built grammars -LRB- used with no training data -RRB- and statistical approaches -LRB- which require significant data -RRB- .


##P06-1022
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Dependency Parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Of Japanese <tag name="FOCUS" value="start"/>Spoken Monologue<tag name="FOCUS" value="end"/> Based On <tag name="TECHNIQUE" value="start"/>Clause Boundaries<tag name="TECHNIQUE" value="end"/> .
<tag name="FOCUS" value="start"/>Spoken monologues<tag name="FOCUS" value="end"/> feature greater sentence length and structural complexity than do spoken dialogues .
To achieve high <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> performance for <tag name="FOCUS" value="start"/>spoken monologues<tag name="FOCUS" value="end"/> , it could prove effective to simplify the structure by dividing a sentence into suitable language units .
This paper proposes a method for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>dependency parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> of <tag name="FOCUS" value="start"/>Japanese monologues<tag name="FOCUS" value="end"/> based on <tag name="TECHNIQUE" value="start"/>sentence segmentation<tag name="TECHNIQUE" value="end"/> .
In this method , the <tag name="DOMAIN" value="start"/>dependency parsing<tag name="DOMAIN" value="end"/> is executed in two stages : at the clause level and the sentence level .
First , the dependencies within a clause are identified by dividing a sentence into clauses and executing <tag name="TECHNIQUE" value="start"/>stochastic<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>dependency parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> for each clause .
Next , the dependencies over clause boundaries are identified <tag name="TECHNIQUE" value="start"/>stochastically<tag name="TECHNIQUE" value="end"/> , and the dependency structure of the entire sentence is thus completed .
An experiment using a <tag name="FOCUS" value="start"/>spoken monologue<tag name="FOCUS" value="end"/> corpus shows this method to be effective for efficient <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>dependency parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> of <tag name="FOCUS" value="start"/>Japanese monologue<tag name="FOCUS" value="end"/> sentences .


##P08-3012
A <tag name="TECHNIQUE" value="start"/>Hierarchical<tag name="TECHNIQUE" value="end"/> Approach to Encoding <tag name="DOMAIN" value="start"/> <tag name="FOCUS" value="start"/>Medical Concepts for Clinical Notes<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
This paper proposes a <tag name="TECHNIQUE" value="start"/>hierarchical text categorization<tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/>TC<tag name="TECHNIQUE" value="end"/> -RRB- approach to encoding free-text <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>clinical notes<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> with ICD-9-CM codes .
Preliminary experimental result on the 2007 Computational Medicine Challenge data shows a <tag name="TECHNIQUE" value="start"/>hierarchical TC<tag name="TECHNIQUE" value="end"/> system has achieved a microaveraged F1 value of 86.6 , which is comparable to the performance of state-of-the-art flat classification systems .


##E09-3005
<tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Structural Correspondence Learning<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Parse Disambiguation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
The paper presents an application of <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Structural Correspondence Learning<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/>SCL<tag name="TECHNIQUE" value="end"/> -RRB- -LRB- Blitzer et al. , 2006 -RRB- for <tag name="DOMAIN" value="start"/>domain adaptation of a stochastic attribute-value grammar<tag name="DOMAIN" value="end"/> -LRB- <tag name="DOMAIN" value="start"/>SAVG<tag name="DOMAIN" value="end"/> -RRB- .
So far , <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>SCL<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> has been applied successfully in NLP for Part-of-Speech tagging and Sentiment Analysis -LRB- Blitzer et al. , 2006 ; Blitzer et al. , 2007 -RRB- .
An attempt was made in the CoNLL 2007 shared task to apply <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>SCL<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> to non-projective dependency parsing -LRB- Shimizu and Nakagawa , 2007 -RRB- , however , without any clear conclusions .
We report on our exploration of applying <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>SCL<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> to adapt a syntactic disambiguation model and show promising initial results on <tag name="DOMAIN" value="start"/>Wikipedia<tag name="DOMAIN" value="end"/> domains .


##C08-1056
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Normalizing SMS<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> : are Two Metaphors Better than One ?
Electronic written texts used in computermediated interactions -LRB- e-mails , blogs , chats , etc -RRB- present major deviations from the norm of the language .
This paper presents an comparative study of systems aiming at <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>normalizing the orthography of French SMS messages<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> : after discussing the linguistic peculiarities of these messages , and possible approaches to their automatic <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>normalization<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , we present , evaluate and contrast two systems , one drawing inspiration from the <tag name="TECHNIQUE" value="start"/>Machine Translation<tag name="TECHNIQUE" value="end"/> task ; the other using techniques that are commonly used in automatic <tag name="TECHNIQUE" value="start"/>speech recognition<tag name="TECHNIQUE" value="end"/> devices .
Combining both approaches , our best <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>normalization<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> system achieves about 11 % Word Error Rate on a test set of about 3000 unseen messages .


##C04-1131
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Word Sense Disambiguation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Criteria : A Systematic Study .
This article describes the results of a systematic indepth study of the criteria used for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>word sense disambiguation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
Our study is based on 60 target words : 20 nouns , 20 adjectives and 20 verbs .
Our results are not always in line with some practices in the field .
For example , we show that omitting noncontent words decreases performance and that bigrams yield better results than unigrams .


##W03-0428
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Named Entity Recognition<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> With <tag name="TECHNIQUE" value="start"/>Character-Level<tag name="TECHNIQUE" value="end"/> Models .
We discuss two <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>named-entity recognition<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> models which use <tag name="TECHNIQUE" value="start"/>characters and character a4 - grams<tag name="TECHNIQUE" value="end"/> either exclusively or as an important part of their data representation .
The first model is a <tag name="TECHNIQUE" value="start"/>character-level HMM<tag name="TECHNIQUE" value="end"/> with minimal context information , and the second model is a <tag name="TECHNIQUE" value="start"/>maximum-entropy conditional markov<tag name="TECHNIQUE" value="end"/> model with substantially richer context features .
Our best model achieves an overall Fa5 of 86.07 % on the English test data -LRB- 92.31 % on the development data -RRB- .
This number represents a 25 % error reduction over the same model without word-internal -LRB- substring -RRB- features .


##W06-3005
A <tag name="TECHNIQUE" value="start"/>Data Driven<tag name="TECHNIQUE" value="end"/> Approach To <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Relevancy Recognition<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> For <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Contextual Question Answering<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Contextual question answering<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> -LRB- <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>QA<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> -RRB- , in which users ' information needs are satisfied through an interactive <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>QA dialogue<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , has recently attracted more research attention .
One challenge of engaging dialogue into <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>QA<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> systems is to determine whether a question is relevant to the previous interaction context .
We refer to this task as <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>relevancy recognition<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
In this paper we propose a <tag name="TECHNIQUE" value="start"/>data driven<tag name="TECHNIQUE" value="end"/> approach for the task of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>relevancy recognition<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> and evaluate it on two data sets : the TREC data and the HandQA data .
The results show that we achieve better performance than a previous rule-based algorithm .
A detailed evaluation analysis is presented .


##I08-1031
Hypothesis Selection in <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Machine Transliteration<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> : A <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Web Mining<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> Approach .
We propose a new method of selecting hypotheses for  <tag name="DOMAIN" value="start"/>machine transliteration<tag name="DOMAIN" value="end"/> .
We generate a set of Chinese , Japanese , and Korean transliteration hypotheses for a given English word .
We then use the set of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>transliteration<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> hypotheses as a guide to finding relevant Web pages and <tag name="TECHNIQUE" value="start"/>mining contextual information<tag name="TECHNIQUE" value="end"/> for the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>transliteration<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> hypotheses from the Web page .
Finally , we use the mined information for <tag name="TECHNIQUE" value="start"/>machine-learning<tag name="TECHNIQUE" value="end"/> algorithms including <tag name="TECHNIQUE" value="start"/>support vector machines<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>maximum entropy<tag name="TECHNIQUE" value="end"/> model designed to select the correct <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>transliteration<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> hypothesis .
In our experiments , our proposed method based on <tag name="TECHNIQUE" value="start"/>Web mining<tag name="TECHNIQUE" value="end"/> consistently outperformed systems based on simple Web counts used in previous work , regardless of the language .


##C02-1009
A Robust Cross-Style <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Bilingual Sentences Alignment<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Model .
Most current <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>sentence alignment<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> approaches adopt sentence length and cognate as the alignment features ; and they are mostly trained and tested in the documents with the same style .
Since the length distribution , alignment-type distribution -LRB- used by length-based approaches -RRB- and cognate frequency vary significantly across texts with different styles , the length-based approaches fail to achieve similar performance when tested incorpora ofdifferent styles .
The experiments show that the performance in F-measure could drop from 98.2 % to 85.6 % when a length-based approach is trained by a technical manual and then tested on a general magazine .
Sincealargepercentageofcontentwordsinthesource text would be translated into the corresponding translation duals to preserve the meaning in the target text , <tag name="TECHNIQUE" value="start"/>transfer lexicons<tag name="TECHNIQUE" value="end"/> are usually regarded as more reliable cues for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>aligning sentences<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> when the alignment task is performed by human .
To enhance the robustness , a robust <tag name="TECHNIQUE" value="start"/>statistical<tag name="TECHNIQUE" value="end"/> model based on both <tag name="TECHNIQUE" value="start"/>transfer lexicons<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>sentence lengths<tag name="TECHNIQUE" value="end"/> are proposed in this paper .
After integrating the <tag name="TECHNIQUE" value="start"/>transfer lexicons<tag name="TECHNIQUE" value="end"/> into the model , a 60 % F-measure error reduction -LRB- from 14.4 % to 5.8 % -RRB- is observed .


##P94-1008
Common Topics And Coherent Situations : Interpreting <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Ellipsis<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> In The Context Of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Discourse Inference<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
It is claimed that a variety of facts concerning <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>ellipsis<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , event reference , and interclausal coherence can be explained by two features of the linguistic form in question : -LRB- 1 -RRB- whether the form leaves behind an empty constituent in the <tag name="TECHNIQUE" value="start"/>syntax<tag name="TECHNIQUE" value="end"/> , and -LRB- 2 -RRB- whether the form is <tag name="TECHNIQUE" value="start"/>anaphoric<tag name="TECHNIQUE" value="end"/> in the <tag name="TECHNIQUE" value="start"/>semantics<tag name="TECHNIQUE" value="end"/> .
It is proposed that these features interact with one of two types of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>discourse inference<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , namely Common Topic inference and Coherent Situation inference .
The differing ways in which these types of inference utilize <tag name="TECHNIQUE" value="start"/>syntactic and semantic<tag name="TECHNIQUE" value="end"/> representations predicts phenomena for which it is otherwise difficult to account .


##W05-1605
Generating And Selecting <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Grammatical Paraphrases<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
Natural language has a high paraphrastic power yet not all paraphrases are appropriate for all contexts .
In this paper , we present a <tag name="TECHNIQUE" value="start"/>TAG<tag name="TECHNIQUE" value="end"/> based <tag name="TECHNIQUE" value="start"/>surface realiser<tag name="TECHNIQUE" value="end"/> which supports both the generation and the selection of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>paraphrases<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
To deal with the combinatorial explosion typical of such an NP-complete task , we introduce a number of new optimisations in a tabular , <tag name="TECHNIQUE" value="start"/>bottom-up surface realisation <tag name="TECHNIQUE" value="end"/> algorithm .
We then show that one of these optimisations supports <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>paraphrase selection<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .


##P08-2052
<tag name="FOCUS" value="start"/>FastSum<tag name="FOCUS" value="end"/> : Fast and Accurate <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Query-based Multi-document Summarization<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
Wepresentafastquery-basedmulti-document <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>summarizer<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> called <tag name="FOCUS" value="start"/>FastSum<tag name="FOCUS" value="end"/> based solely on <tag name="TECHNIQUE" value="start"/>word-frequency features<tag name="TECHNIQUE" value="end"/> of clusters , documents and topics .
Summary sentences are ranked by a <tag name="TECHNIQUE" value="start"/>regression SVM<tag name="TECHNIQUE" value="end"/> .
The <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>summarizer<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> does not use any expensive NLP techniques such as parsing , tagging of names or even part of speech information .
Still , the achieved accuracy is comparable to the best systems presented in recent academic competitions -LRB- i.e. , Document Understanding Conference -LRB- DUC -RRB- -RRB- .
Because of a detailed feature analysis using <tag name="TECHNIQUE" value="start"/>Least Angle Regression<tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/>LARS<tag name="TECHNIQUE" value="end"/> -RRB- , <tag name="FOCUS" value="start"/>FastSum<tag name="FOCUS" value="end"/> can rely on a minimal set of featuresleading tofastprocessingtimes : 1250 news documents in 60 seconds .


##P07-1019
<tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Forest Rescoring<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> : Faster <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Decoding<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> with <tag name="TECHNIQUE" value="start"/>Integrated Language Models<tag name="TECHNIQUE" value="end"/> .
Efficient <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>decoding<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> has been a fundamental problem in <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>machine translation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> , especially with an <tag name="TECHNIQUE" value="start"/>integrated language model<tag name="TECHNIQUE" value="end"/> which is essential for achieving good translation quality .
We develop faster approaches for this problem based on <tag name="TECHNIQUE" value="start"/>k-best parsing<tag name="TECHNIQUE" value="end"/> algorithms and demonstrate their effectiveness on both <tag name="TECHNIQUE" value="start"/>phrase-based<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>syntax-based<tag name="TECHNIQUE" value="end"/>  <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/> MT<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> systems .
In both cases , our methods achieve significant speed improvements , often by more than a factor of ten , over the conventional beam-search method at the same levels of search error and translation accuracy .


##P98-2181
Building Accurate <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Semantic Taxonomies<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> from Monolingual <tag name="TECHNIQUE" value="start"/>MRDs<tag name="TECHNIQUE" value="end"/> .
This paper presents a method that conbines a set of <tag name="TECHNIQUE" value="start"/>unsupervised<tag name="TECHNIQUE" value="end"/> algorithms in order to accurately build large <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>taxonomies<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> from any <tag name="TECHNIQUE" value="start"/>machine-readable dictionary -LRB- MRD -RRB-<tag name="TECHNIQUE" value="end"/> .
Our aim is to profit from conventional <tag name="TECHNIQUE" value="start"/>MRDs<tag name="TECHNIQUE" value="end"/> , with no explicit semantic coding .
We propose a system that 1 -RRB- performs fully automatic extraction of <tag name="DOMAIN" value="start"/>taxonomic links<tag name="DOMAIN" value="end"/> from <tag name="TECHNIQUE" value="start"/>MRD<tag name="TECHNIQUE" value="end"/> entries and 2 -RRB- ranks the extracted relations in a way that selective manual refinement is allowed .
Tested accuracy can reach around 100 % depending on the degree of coverage selected , showing that taxonomy building is not limited to structured dictionaries such as LDOCE .


##C02-2004
A <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Linguistic Discovery<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Program That Verbalizes Its Discoveries .
We describe a <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>discovery<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> program , called <tag name="FOCUS" value="start"/>UNIVAUTO -LRB- UNIVersals AUthoringTOol -RRB-<tag name="FOCUS" value="end"/> , whose domain of application is the study of <tag name="DOMAIN" value="start"/>language universals<tag name="DOMAIN" value="end"/> , a classic trend in contemporary linguistics .
Accepting as input information about languages , presented in terms of <tag name="TECHNIQUE" value="start"/>feature-values<tag name="TECHNIQUE" value="end"/> , the discoveries of another human agent arising from the same data , as well as some additional data , the program discovers the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>universals<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> in the data , compares them with the discoveries of the human agent and , if appropriate , generates a report in English on its discoveries .
Running <tag name="FOCUS" value="start"/>UNIVAUTO<tag name="FOCUS" value="end"/> on the data from the seminal paper of Greenberg -LRB- 1966 -RRB- on <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>word order universals<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> , the system has produced several linguistically valuable texts , two of which are published in a refereed linguistic journal .


##W98-1421
Towards <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Multilingual Protocol Generation For Spontaneous Speech Dialogues<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
: This paper presents a novel <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>multi-lingual progress protocol generation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> module .
The module is used within the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>speech-to -- speech translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> system VERBMOBIL .
The task of the protocol is to give the dialogue partners a brief description of the content of their dialogue .
We utilize an .
abstract representation describing , for instance , <tag name="TECHNIQUE" value="start"/>thematic<tag name="TECHNIQUE" value="end"/>  information and <tag name="TECHNIQUE" value="start"/>dialogue acts<tag name="TECHNIQUE" value="end"/> of the dialogue utterances .
From this representation we generate simplified <tag name="TECHNIQUE" value="start"/>paraphrases<tag name="TECHNIQUE" value="end"/> of the individual turns of the dialogue which together make up the protocol .
Instead of writing completely new software , the protocol generation component is almost exclusively composed of already existing modules in the system which are extended by planning and formatting routines for protocol formulations .
We describe how the abstract information is extracted from user utterances in different languages and how the <tag name="TECHNIQUE" value="start"/>abstract thematic<tag name="TECHNIQUE" value="end"/> representation is used to <tag name="DOMAIN" value="start"/>generate a protocol<tag name="DOMAIN" value="end"/> in one specific language .
Future directions are given .


##P06-1017
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Relation Extraction<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Using <tag name="TECHNIQUE" value="start"/>Label Propagation Based Semi-Supervised Learning<tag name="TECHNIQUE" value="end"/> .
Shortage of manually labeled data is an obstacle to supervised relation extraction methods .
In this paper we investigate a <tag name="TECHNIQUE" value="start"/>graph based semi-supervised learning<tag name="TECHNIQUE" value="end"/> algorithm , a <tag name="TECHNIQUE" value="start"/>label propagation -LRB- LP -RRB-<tag name="TECHNIQUE" value="end"/> algorithm , for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>relation extraction<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
It represents labeled and unlabeled examples and their distances as the nodes and the weights of edges of a graph , and tries to obtain a labeling function to satisfy two constraints : 1 -RRB- it should be fixed on the labeled nodes , 2 -RRB- it should be smooth on the whole graph .
Experiment results on the ACE corpus showed that this <tag name="TECHNIQUE" value="start"/>LP<tag name="TECHNIQUE" value="end"/> algorithm achieves better performance than SVM when only very few labeled examples are available , and it also performs better than bootstrapping for the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>relation extraction<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> task .


##W00-0712
Knowledge-Free <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Induction Of Morphology<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Using <tag name="TECHNIQUE" value="start"/>Latent Semantic Analysis<tag name="TECHNIQUE" value="end"/> .
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Morphology induction<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> is a subproblem of important tasks like automatic learning of machine-readable dictionaries and grammar induction .
Previous <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>morphology induction<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> approaches have relied solely on statistics of hypothesized stems and affixes to choose which affixes to consider legitimate .
Relying on stemand-affix statistics rather than semantic knowledge leads to a number of problems , such as the inappropriate use of valid affixes -LRB- `` ally '' stemming to `` all '' -RRB- .
We introduce a <tag name="TECHNIQUE" value="start"/>semantic-based<tag name="TECHNIQUE" value="end"/> algorithm for learning <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>morphology<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> which only proposes affixes when the stem and stem-plusaffix are sufficiently similar semantically .
We implement our approach using <tag name="TECHNIQUE" value="start"/>Latent Semantic Analysis<tag name="TECHNIQUE" value="end"/> and show that our <tag name="TECHNIQUE" value="start"/>semantics-only<tag name="TECHNIQUE" value="end"/> approach provides <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>morphology induction<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> results that rival a current state-of-the-art system .


##N09-1017
The Role of <tag name="TECHNIQUE" value="start"/>Implicit Argumentation<tag name="TECHNIQUE" value="end"/> in <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Nominal SRL<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Nominals<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> frequently surface without overtly expressed arguments .
In order to measure the potential benefit of <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>nominal SRL<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> for downstream processes , such <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>nominals<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> must be accounted for .
In this paper , we show that a state-of-the-art <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>nominal SRL<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> system with an overall argument F1 of 0.76 suffers a performance loss of more than 9 % when nominals with implicit arguments are included in the evaluation .
We then develop a system that takes <tag name="TECHNIQUE" value="start"/>implicit argumentation<tag name="TECHNIQUE" value="end"/> into account , improving overall performance by nearly 5 % .
Our results indicate that the degree of <tag name="TECHNIQUE" value="start"/>implicit argumentation<tag name="TECHNIQUE" value="end"/> varies widely across <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>nominals<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , making automated detection of <tag name="TECHNIQUE" value="start"/>implicit argumentation<tag name="TECHNIQUE" value="end"/> an important step for <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>nominal SRL<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .


##W94-0108
The Automatic Construction Of A <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Symbolic <tag name="TECHNIQUE" value="end"/><tag name="DOMAIN" value="start"/>Parser<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> Via <tag name="TECHNIQUE" value="start"/>Statistical<tag name="TECHNIQUE" value="end"/> Techniques .
We report on the development of a robust <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> device which aims to provide a partial explanation for child language acquisition and help in the construction of better natural language processing systems .
The backbone of the new approach is the synthesis of <tag name="TECHNIQUE" value="start"/>statistical<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>symbolic<tag name="TECHNIQUE" value="end"/> approaches to natural language .
Motivation We report on the progress we have made towards developing a robust ` self-constructing ' <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> device that uses indirect negative evidence -LRB- Kapur ,


##W97-0124
Analysis Of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Unknown Lexical Items<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Using <tag name="TECHNIQUE" value="start"/>Morphological<tag name="TECHNIQUE" value="end"/> And <tag name="TECHNIQUE" value="start"/>Syntactic<tag name="TECHNIQUE" value="end"/> Information With The TIMIT Corpus .
The importance of dealing with <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>unknown words<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> in Natural Language Processing -LRB- NLP -RRB- is growing as NLP systems are used in more and more applications .
One aid in predicting the lexical class of words that do not appear in the lexicon -LRB- referred to as <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>unknown words<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> -RRB- is the use of <tag name="TECHNIQUE" value="start"/>syntactic<tag name="TECHNIQUE" value="end"/> parsing rules .
The distinction between closed-class and open-class words together with <tag name="TECHNIQUE" value="start"/>morphological<tag name="TECHNIQUE" value="end"/> recognition appears to be pivotal in increasing the ability of the system to predict the lexical categories of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>unknown words<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
An experiment is performed to investigate the ability of a parser to parse unknown words using <tag name="TECHNIQUE" value="start"/>morphology<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>syntactic<tag name="TECHNIQUE" value="end"/> parsing rules without human intervention .
This experiment shows that the performance of the <tag name="DOMAIN" value="start"/>parser<tag name="DOMAIN" value="end"/> is enhanced greatly when <tag name="TECHNIQUE" value="start"/>morphological<tag name="TECHNIQUE" value="end"/> recognition is used in conjunction with <tag name="TECHNIQUE" value="start"/>syntactic rules<tag name="TECHNIQUE" value="end"/> to <tag name="DOMAIN" value="start"/>parse<tag name="DOMAIN" value="end"/> sentences containing <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>unknown words<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> from the TIMIT corpus .


##A88-1006
From Water To Wine : <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Generating Natural Language Text<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> From Today 's Applications Programs .
In this paper we present a means of compensating for the semantic deficits of linguistically naive underlying application programs without compromising principled grammatical treatments in <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>natural language generation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We present a method for building an interface from today 's underlying application programs to the linguistic realization component <tag name="TECHNIQUE" value="start"/>Mumble-86<tag name="TECHNIQUE" value="end"/> .
The goal of the paper is not to discuss how Mumble works , but to describe how one exploits its capabilities .
We provide examples from current generation projects using <tag name="TECHNIQUE" value="start"/>Mumble<tag name="TECHNIQUE" value="end"/> as their linguistic component .


##P07-1039
<tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Bootstrapping<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>Word Alignment<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> via <tag name="TECHNIQUE" value="start"/>Word Packing<tag name="TECHNIQUE" value="end"/> .
We introduce a simple method to <tag name="TECHNIQUE" value="start"/>pack words<tag name="TECHNIQUE" value="end"/> for <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>statistical<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>word alignment<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
Our goal is to simplify the task of automatic <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>word alignment<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> by <tag name="TECHNIQUE" value="start"/>packing<tag name="TECHNIQUE" value="end"/> several consecutive words together when we believe they correspond to a single word in the opposite language .
This is done using the word aligner itself , i.e. by <tag name="TECHNIQUE" value="start"/>bootstrapping<tag name="TECHNIQUE" value="end"/> on its output .
We evaluate the performance of our approach on a Chinese-to-English <tag name="DOMAIN" value="start"/>machine translation<tag name="DOMAIN" value="end"/> task , and report a 12.2 % relative increase in BLEU score over a state-of-the art phrasebased SMT system .


##J99-4005
Decoding <tag name="FOCUS" value="start"/>Complexity<tag name="FOCUS" value="end"/> In <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Word-Replacement Translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Models .
<tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Statistical<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>machine translation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> is a relatively new approach to the long-standing problem of translating human languages by computer .
Current statistical techniques uncover translation rules from bilingual training texts and use those rules to translate new texts .
The general architecture is the <tag name="TECHNIQUE" value="start"/>source-channel<tag name="TECHNIQUE" value="end"/> model : an English string is statistically generated -LRB- source -RRB- , then statistically transformed into French -LRB- channel -RRB- .
In order to translate -LRB- or `` decode '' -RRB- a French string , we look for the most likely English source .
We show that for the simplest form of <tag name="TECHNIQUE" value="start"/>statistical<tag name="TECHNIQUE" value="end"/> models , this problem is NP-complete , i.e. , probably exponential in the length of the observed sentence .
We trace this <tag name="FOCUS" value="start"/>complexity<tag name="FOCUS" value="end"/> to factors not present in other decoding problems .


##W96-0412
An Evaluation Of <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Anaphor Generation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> In Chinese .
In this paper , we present an evaluation of <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>anaphors generated<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> by a Chinese <tag name="DOMAIN" value="start"/>natural language generation<tag name="DOMAIN" value="end"/> system .
In the evaluation work , the anaphors in five test texts generated by three test systems employing generation <tag name="TECHNIQUE" value="start"/>rules<tag name="TECHNIQUE" value="end"/> with different complexities ~ vere compared with the ones in the same texts created by twelve native speakers of Chinese .
We took the average number of <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>anaphors<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> matching between the machine and human texts as a measure of the quality of anaphors generated by the test systems .
The results suggest that the one we have chosen and which has the most complex <tag name="TECHNIQUE" value="start"/>rule<tag name="TECHNIQUE" value="end"/> is better than the other two .
There axe , however , real difficulties in establishing the significance of the results because of the degree of disagreement among the native speakers .


##C94-1082
<tag name="FOCUS" value="start"/>LHIP<tag name="FOCUS" value="end"/> : Extended <tag name="TECHNIQUE" value="start"/>DCGs<tag name="TECHNIQUE" value="end"/> For Configurable Robust <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We present <tag name="FOCUS" value="start"/>LHIP<tag name="FOCUS" value="end"/> , a system for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>incremental grammar development<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> using an extended <tag name="TECHNIQUE" value="start"/>DCG<tag name="TECHNIQUE" value="end"/> formalism .
` rite system uses a robust <tag name="TECHNIQUE" value="start"/>island-based<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> method controlled by user-defined performance thresholds .
Keywords : I -RRB- CG , head , island parsing , robust parsing , Prolog


##P96-1014
Computing <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Optimal Descriptions For Optimality Theory Grammars<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> With <tag name="TECHNIQUE" value="start"/>Context-Free Position Structures<tag name="TECHNIQUE" value="end"/> .
This paper describes an algorithm for computing <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>optimal structural descriptions for Optimality Theory grammars<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> with <tag name="TECHNIQUE" value="start"/>context-free position structures<tag name="TECHNIQUE" value="end"/> .
This algorithm extends Tesar 's <tag name="TECHNIQUE" value="start"/>dynamic programming<tag name="TECHNIQUE" value="end"/> approach -LRB- Tesar , 1994 -RRB- -LRB- Tesar , 1995 @ to computing <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>optimal structural descriptions<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> from regular to context-free structures .
The generalization to <tag name="TECHNIQUE" value="start"/>contextfree structures<tag name="TECHNIQUE" value="end"/> creates several complications , all of which are overcome without compromising the core <tag name="TECHNIQUE" value="start"/>dynamic programming<tag name="TECHNIQUE" value="end"/> approach .
The resulting algorithm has a time complexity cubic in the length of the input , and is applicable to grammars with universal constraints that exhibit context-free locality .


##W05-0801
<tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Association-Based<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>Bilingual Word Alignment<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Bilingual word alignment<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> forms the foundation of current work on statistical machine translation .
Standard <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>wordalignment<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> methods involve the use of probabilistic generative models that are complex to implement and slow to train .
In this paper we show that it is possible to approach the alignment accuracy of the standard models using algorithms that are much faster , and in some ways simpler , based on basic <tag name="TECHNIQUE" value="start"/>word-association statistics<tag name="TECHNIQUE" value="end"/> .


##W04-0851
<tag name="TECHNIQUE" value="start"/>Regularized Least-Squares Classification<tag name="TECHNIQUE" value="end"/> For <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Word Sense Disambiguation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
The paper describes <tag name="FOCUS" value="start"/>RLSC-LIN<tag name="FOCUS" value="end"/> and <tag name="FOCUS" value="start"/>RLSCCOMB<tag name="FOCUS" value="end"/> systems which participated in the Senseval-3 English lexical sample task .
These systems are based on <tag name="TECHNIQUE" value="start"/>Regularized Least-Squares Classification<tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/>RLSC<tag name="TECHNIQUE" value="end"/> -RRB- <tag name="TECHNIQUE" value="start"/>learning<tag name="TECHNIQUE" value="end"/> method .
We describe the reasons of choosing this method , how we applied it to <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>word sense disambiguation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , what results we obtained on Senseval1 , Senseval-2 and Senseval-3 data and discuss some possible improvements .


##P09-1053
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Paraphrase<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Identification as <tag name="TECHNIQUE" value="start"/>Probabilistic Quasi-Synchronous<tag name="TECHNIQUE" value="end"/> Recognition .
We present a novel approach to deciding whether two sentences hold a <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>paraphrase<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> relationship .
We employ a <tag name="TECHNIQUE" value="start"/>generative<tag name="TECHNIQUE" value="end"/> model that generates a <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>paraphrase<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> of a given sentence , and we use <tag name="TECHNIQUE" value="start"/>probabilistic inference<tag name="TECHNIQUE" value="end"/> to reason about whether two sentences share the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>paraphrase<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> relationship .
The model cleanly incorporates both <tag name="TECHNIQUE" value="start"/>syntax and lexical semantics<tag name="TECHNIQUE" value="end"/> using <tag name="TECHNIQUE" value="start"/>quasi-synchronous dependency grammars<tag name="TECHNIQUE" value="end"/> -LRB- Smith and Eisner , 2006 -RRB- .
Furthermore , using a product of experts -LRB- Hinton , 2002 -RRB- , we combine the model with a complementary <tag name="TECHNIQUE" value="start"/>logistic regression<tag name="TECHNIQUE" value="end"/> model based on state-of-the-art <tag name="TECHNIQUE" value="start"/>lexical overlap features<tag name="TECHNIQUE" value="end"/> .
We evaluate our models on the task of distinguishing true <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>paraphrase<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> pairs from false ones on a standard corpus , giving competitive state-of-the-art performance .


##P03-1018
<tag name="TECHNIQUE" value="start"/>Orthogonal Negation In Vector Spaces<tag name="TECHNIQUE" value="end"/> For Modelling <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Word-Meanings And Document Retrieval<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
Standard <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>IR<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> systems can process queries such as `` web NOT internet '' , enabling users who are interested in arachnids to avoid documents about computing .
The documents retrieved for such a query should be irrelevant to the negated query term .
Most systems implement this by reprocessing results after retrieval to remove documents containing the unwanted string of letters .
This paper describes and evaluates a theoretically motivated method for removing unwanted meanings directly from the original query in <tag name="TECHNIQUE" value="start"/>vector models<tag name="TECHNIQUE" value="end"/> , with the same <tag name="TECHNIQUE" value="start"/>vector negation<tag name="TECHNIQUE" value="end"/> operator as used in quantum logic .
Irrelevance in vector spaces is modelled using <tag name="TECHNIQUE" value="start"/>orthogonality<tag name="TECHNIQUE" value="end"/> , so query vectors are made orthogonal to the negated term or terms .
As well as removing unwanted terms , this form of <tag name="TECHNIQUE" value="start"/>vector negation<tag name="TECHNIQUE" value="end"/> reduces the occurrence of synonyms and neighbors of the negated terms by as much as 76 % compared with standard Boolean methods .
By altering the query vector itself , <tag name="TECHNIQUE" value="start"/>vector negation<tag name="TECHNIQUE" value="end"/> removes not only unwanted strings but unwanted meanings .


##W96-0103
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Hierarchical Clustering Of Words<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> And Application To NLP Tasks .
This paper describes a <tag name="TECHNIQUE" value="start"/>data-driven<tag name="TECHNIQUE" value="end"/> method for  <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>hierarchical clustering of words<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> and <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>clustering of multiword compounds<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
A large vocabulary of English words -LRB- 70,000 words -RRB- is <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>clustered<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> <tag name="TECHNIQUE" value="start"/>bottom-up<tag name="TECHNIQUE" value="end"/> , with respect to corpora ranging in size from 5 million to 50 million words , using <tag name="TECHNIQUE" value="start"/>mutual information<tag name="TECHNIQUE" value="end"/> as an objective function .
The resulting <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>hierarchical clusters of words<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> are then naturally transformed to a bit-string representation of -LRB- i.e. word bits for -RRB- all the words in the vocabulary .
Evaluation of the word bits is carried out through the measurement of the error rate of the ATR <tag name="TECHNIQUE" value="start"/>Decision-Tree<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>Part-Of-Speech Tagger<tag name="DOMAIN" value="end"/> .
The same <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>clustering<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> technique is then applied to the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>classification of multiword compounds<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
In order to avoid the explosion of the number of compounds to be handled , compounds in a small subclass are bundled and treated as a single compound .
Another merit of this approach is that we can avoid the data sparseness problem which is ubiquitous in corpus statistics .
The quality of one of the obtained compound classes is examined and compared to a conventional approach .


##P03-1065
An <tag name="TECHNIQUE" value="start"/>Expert Lexicon<tag name="TECHNIQUE" value="end"/> Approach To Identifying English <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Phrasal Verbs<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Phrasal Verbs<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> are an important feature of the English language .
Properly identifying them provides the basis for an English parser to decode the related structures .
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Phrasal verbs<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> have been a challenge to Natural Language Processing -LRB- NLP -RRB- because they sit at the borderline between lexicon and syntax .
Traditional NLP frameworks that separate the lexicon module from the parser make it difficult to handle this problem properly .
This paper presents a <tag name="TECHNIQUE" value="start"/>finite state<tag name="TECHNIQUE" value="end"/> approach that integrates a <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>phrasal verb<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> <tag name="TECHNIQUE" value="start"/>expert lexicon<tag name="TECHNIQUE" value="end"/> between shallow parsing and deep parsing to handle morpho-syntactic interaction .
With precision\/recall combined performance benchmarked consistently at 95.8 % -97.5 % , the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Phrasal Verb<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> identification problem has basically been solved with the presented method .


##P90-1024
<tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Zero Morphemes<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> In <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Unification-Based Combinatory Categorial Grammar<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
In this paper , we report on our use of <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>zero morphemes<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> in <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Unification-Based Combinatory Categorial Grammar<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
After illustrating the benefits of this approach with several examples , we describe the algorithm for compiling <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>zero morphemes<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> into <tag name="TECHNIQUE" value="start"/>unary rules<tag name="TECHNIQUE" value="end"/> , which allows us to use <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>zero morphemes<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> more efficiently in natural language processing .
1 Then , we discuss the question of equivalence of a grammar with these <tag name="TECHNIQUE" value="start"/>unary rules<tag name="TECHNIQUE" value="end"/> to the original grammar .
Lastly , we compare our approach to <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>zero morphemes<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> with possible alternatives .


##W07-1417
The Role of <tag name="TECHNIQUE" value="start"/>Sentence Structure<tag name="TECHNIQUE" value="end"/> in Recognizing <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Textual Entailment<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
Recent research suggests that <tag name="TECHNIQUE" value="start"/>sentence structure<tag name="TECHNIQUE" value="end"/> can improve the accuracy of recognizing <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>textual entailments<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> and paraphrasing .
Although background knowledge such as gazetteers , WordNet and custom built knowledge bases are also likely to improve performance , our goal in this paper is to characterize the <tag name="TECHNIQUE" value="start"/>syntactic features<tag name="TECHNIQUE" value="end"/> alone that aid in accurate <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>entailment<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> prediction .
We describe candidate features , the role of <tag name="TECHNIQUE" value="start"/>machine learning<tag name="TECHNIQUE" value="end"/> , and two final <tag name="TECHNIQUE" value="start"/>decision rules<tag name="TECHNIQUE" value="end"/> .
These rules resulted in an accuracy of 60.50 and 65.87 % and average precision of 58.97 and 60.96 % in RTE3 Test and suggest that <tag name="TECHNIQUE" value="start"/>sentence structure<tag name="TECHNIQUE" value="end"/> alone can improve <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>entailment<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> accuracy by 9.25 to 14.62 % over the baseline majority class .


##D07-1065
Recovery of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Empty Nodes in Parse Structures<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
In this paper , we describe a new algorithm for recovering <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>WH-trace empty nodes<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
Our approach combines a set of <tag name="TECHNIQUE" value="start"/>hand-written patterns<tag name="TECHNIQUE" value="end"/> together with a <tag name="TECHNIQUE" value="start"/>probabilistic<tag name="TECHNIQUE" value="end"/> model .
Because the <tag name="TECHNIQUE" value="start"/>patterns<tag name="TECHNIQUE" value="end"/> heavily utilize <tag name="TECHNIQUE" value="start"/>regular expressions<tag name="TECHNIQUE" value="end"/> , the pertinent tree structures are covered using a limited number of <tag name="TECHNIQUE" value="start"/>patterns<tag name="TECHNIQUE" value="end"/> .
The <tag name="TECHNIQUE" value="start"/>probabilistic<tag name="TECHNIQUE" value="end"/> model is essentially a <tag name="TECHNIQUE" value="start"/>probabilistic context-free grammar -LRB- PCFG -RRB-<tag name="TECHNIQUE" value="end"/> approach with the <tag name="TECHNIQUE" value="start"/>patterns<tag name="TECHNIQUE" value="end"/> acting as the terminals in production rules .
We evaluate the algorithm 's performance on gold trees and parser output using three different metrics .
Our method compares favorably with state-of-the-art algorithms that recover <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>WH-traces<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .


##P00-1070
Importance Of <tag name="TECHNIQUE" value="start"/>Pronominal Anaphora Resolution<tag name="TECHNIQUE" value="end"/> In <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Question Answering<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Systems .
The main aim of this paper is to analyze the e # 0Bects of applying <tag name="TECHNIQUE" value="start"/>pronominal anaphora resolution<tag name="TECHNIQUE" value="end"/> to <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Question Answering<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> # 28QA # 29 systems .
For this task a complete <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>QA<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> system has been implemented .
System evaluation measures performance improvements obtained when information that is referenced anaphorically in documents is not ignored .


##P04-1080
Learning <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Word Sense<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> With <tag name="TECHNIQUE" value="start"/>Feature Selection<tag name="TECHNIQUE" value="end"/> And <tag name="TECHNIQUE" value="start"/>Order Identification<tag name="TECHNIQUE" value="end"/> Capabilities .
This paper presents an <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>unsupervised<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>word sense<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> learning algorithm , which induces senses of target word by grouping its occurrences into a `` natural '' number of clusters based on the similarity of their contexts .
For removing noisy words in feature set , <tag name="TECHNIQUE" value="start"/>feature selection<tag name="TECHNIQUE" value="end"/> is conducted by optimizing a cluster validation criterion subject to some constraint in an <tag name="TECHNIQUE" value="start"/>unsupervised<tag name="TECHNIQUE" value="end"/> manner .
<tag name="TECHNIQUE" value="start"/>Gaussian mixture<tag name="TECHNIQUE" value="end"/> model and <tag name="TECHNIQUE" value="start"/>Minimum Description Length<tag name="TECHNIQUE" value="end"/> criterion are used to estimate cluster structure and cluster number .
Experimental results show that our algorithm can find important feature subset , estimate model order -LRB- cluster number -RRB- and achieve better performance than another algorithm which requires cluster number to be provided .


##W07-2216
On the Complexity of <tag name="TECHNIQUE" value="start"/>Non-Projective Data-Driven<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Dependency Parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
In this paper we investigate several <tag name="TECHNIQUE" value="start"/>nonprojective<tag name="TECHNIQUE" value="end"/> <tag name="FOCUS" value="start"/> <tag name="DOMAIN" value="start"/>parsing<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> algorithms for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>dependency parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , providing novel polynomial time solutions under the assumption that each dependency decision is independent of all the others , called here the <tag name="TECHNIQUE" value="start"/>edge-factored<tag name="TECHNIQUE" value="end"/> model .
We also investigate algorithms for <tag name="TECHNIQUE" value="start"/>non-projective<tag name="TECHNIQUE" value="end"/>  <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>parsing<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> that account for nonlocal information , and present several hardness results .
This suggests that it is unlikely that exact <tag name="TECHNIQUE" value="start"/>non-projective<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>dependency parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> is tractable for any model richer than the <tag name="TECHNIQUE" value="start"/>edge-factored<tag name="TECHNIQUE" value="end"/> model .


##P05-2008
Using <tag name="TECHNIQUE" value="start"/>Emoticons<tag name="TECHNIQUE" value="end"/> To Reduce Dependency In <tag name="TECHNIQUE" value="start"/>Machine Learning<tag name="TECHNIQUE" value="end"/> Techniques For <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Sentiment Classification<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Sentiment Classification<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> seeks to identify a piece of text according to its author 's general feeling toward their subject , be it positive or negative .
Traditional machine learning techniques have been applied to this problem with reasonable success , but they have been shown to work well only when there is a good match between the training and test data with respect to topic .
This paper demonstrates that match with respect to domain and time is also important , and presents preliminary experiments with training data labeled with <tag name="TECHNIQUE" value="start"/>emoticons<tag name="TECHNIQUE" value="end"/> , which has the potential of being independent of domain , topic and time .


##P08-1016
Lexicalized <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Phonotactic Word Segmentation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
This paper presents a new <tag name="TECHNIQUE" value="start"/>unsupervised<tag name="TECHNIQUE" value="end"/> algorithm -LRB-<tag name="FOCUS" value="start"/> WordEnds<tag name="FOCUS" value="end"/> -RRB- for inferring <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>word boundaries<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> from transcribed adult conversations .
<tag name="TECHNIQUE" value="start"/>Phone ngrams<tag name="TECHNIQUE" value="end"/> before and after observed pauses are used to <tag name="TECHNIQUE" value="start"/>bootstrap<tag name="TECHNIQUE" value="end"/> a simple <tag name="TECHNIQUE" value="start"/>discriminative<tag name="TECHNIQUE" value="end"/> model of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>boundary marking<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
This fast algorithm delivers high performance even on morphologically complex words in English and Arabic , and promising results on accurate phonetic transcriptions with extensive pronunciation variation .
Expanding training data beyond the traditional miniature datasets pushes performance numbers well above those previously reported .
This suggests that <tag name="FOCUS" value="start"/>WordEnds<tag name="FOCUS" value="end"/> is a viable model of child language acquisition and might be useful in speech understanding .


##N09-1010
Adding More Languages Improves <tag name="TECHNIQUE" value="start"/>Unsupervised<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Multilingual Part-of-Speech Tagging<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> : a <tag name="TECHNIQUE" value="start"/>Bayesian Non-Parametric<tag name="TECHNIQUE" value="end"/> Approach .
We investigate the problem of <tag name="TECHNIQUE" value="start"/>unsupervised<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>part-of-speech tagging<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> when raw parallel data is available in a large number of languages .
Patterns of ambiguity vary greatly across languages and therefore even unannotated multilingual data can serve as a learning signal .
We propose a <tag name="TECHNIQUE" value="start"/>non-parametric Bayesian<tag name="TECHNIQUE" value="end"/> model that connects related tagging decisions across languages through the use of <tag name="TECHNIQUE" value="start"/>multilingual latent variables<tag name="TECHNIQUE" value="end"/> .
Our experiments show that performance improves steadily as the number of languages increases .


##P07-1111
A Re-examination of <tag name="TECHNIQUE" value="start"/>Machine Learning<tag name="TECHNIQUE" value="end"/> Approaches for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Sentence-Level MT Evaluation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
Recent studies suggest that <tag name="TECHNIQUE" value="start"/>machine learning<tag name="TECHNIQUE" value="end"/> can be applied to develop good automatic <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>evaluation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> metrics for machine translated sentences .
This paper further analyzes aspects of learning that impact performance .
We argue that previously proposed approaches of training a HumanLikeness classifier is not as well correlated with human judgments of translation quality , but that <tag name="TECHNIQUE" value="start"/>regression-based learning<tag name="TECHNIQUE" value="end"/> produces more reliable metrics .
We demonstrate the feasibility of <tag name="TECHNIQUE" value="start"/>regression-based<tag name="TECHNIQUE" value="end"/> metrics through empirical analysis of learning curves and generalization studies and show that they can achieve higher correlations with human judgments than standard automatic metrics .



##D07-1126
A <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Multilingual Dependency<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Analysis System Using <tag name="TECHNIQUE" value="start"/>Online Passive-Aggressive Learning<tag name="TECHNIQUE" value="end"/> .
This paper presents an <tag name="TECHNIQUE" value="start"/>online<tag name="TECHNIQUE" value="end"/> algorithm for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>dependency parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> problems .
We propose an adaptation of the <tag name="TECHNIQUE" value="start"/>passive and aggressive online learning<tag name="TECHNIQUE" value="end"/> algorithm to the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>dependency parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> domain .
We evaluate the proposed algorithms on the 2007 CONLL Shared Task , and report errors analysis .
Experimental results show that the system score is better than the average score among the participating systems .


##N07-3003
Creating a <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Knowledge Base<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> from a Collaboratively Generated <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Encyclopedia<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> .
We present our work on using <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Wikipedia<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> as a knowledge source for Natural Language Processing .
We first describe our previous work on computing semantic relatedness from <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Wikipedia<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> , and its application to a <tag name="TECHNIQUE" value="start"/>machine learning<tag name="TECHNIQUE" value="end"/> based <tag name="DOMAIN" value="start"/>coreference resolution<tag name="DOMAIN" value="end"/> system .
Our results suggest that <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Wikipedia<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> represents a semantic resource to be treasured for NLP applications , and accordingly present the work directions to be explored in the future .


##W93-0105
Identifying <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Unknown Proper Names<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> In Newswire Text .
The identification of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>unknown proper names<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> in text is a significant challenge for NLP systems operating on unrestricted text .
A system which indexes documents according to name references can be useful for information retrieval or as a preprocessor for more knowledge intensive tasks such as database extraction .
This paper describes a system which uses <tag name="TECHNIQUE" value="start"/>text skimming<tag name="TECHNIQUE" value="end"/> techniques for deriving <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>proper names<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> and their semantic attributes automatically from newswire text , without relying on any listing of name elements .
In order to identify new names , the system treats proper names as -LRB- potentially -RRB- context-dependent linguistic expressions .
In addition to using information in the <tag name="TECHNIQUE" value="start"/>local context<tag name="TECHNIQUE" value="end"/> , the system exploits a <tag name="TECHNIQUE" value="start"/>computational model of discourse<tag name="TECHNIQUE" value="end"/> which identifies individuals based on the way they are described in the text , instead of relying on their description in a pre-existing knowledge base .


##P09-2033
English-Chinese <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Bi-Directional OOV Translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> based on <tag name="TECHNIQUE" value="start"/>Web Mining<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>Supervised Learning<tag name="TECHNIQUE" value="end"/> .
In Cross-Language Information Retrieval -LRB- CLIR -RRB- , <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Out-of-Vocabulary -LRB- OOV -RRB- detection and translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> pair relevance evaluation still remain as key problems .
In this paper , an English-Chinese <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Bi-Directional OOV translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> model is presented , which utilizes <tag name="TECHNIQUE" value="start"/>Web mining<tag name="TECHNIQUE" value="end"/> as the corpus source to collect translation pairs and combines <tag name="TECHNIQUE" value="start"/>supervised learning<tag name="TECHNIQUE" value="end"/> to evaluate their association degree .
The experimental results show that the proposed model can successfully filter the most possible translation candidate with the lower computational cost , and improve the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>OOV translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> ranking effect , especially for popular new words .


##W08-0302
Rich Source-Side <tag name="TECHNIQUE" value="start"/>Context<tag name="TECHNIQUE" value="end"/> for <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Statistical<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>Machine Translation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
We explore the augmentation of <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>statistical <tag name="TECHNIQUE" value="end"/><tag name="DOMAIN" value="start"/>machine translation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> models with features of the context of each phrase to be translated .
This work extends several existing threads of research in <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>statistical<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>MT<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> , including the use of context in example-based machine translation -LRB- Carl and Way , 2003 -RRB- and the incorporation of word sense disambiguation into a translation model -LRB- Chan et al. , 2007 -RRB- .
The context features we consider use <tag name="TECHNIQUE" value="start"/>surrounding words<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>part-of-speech tags<tag name="TECHNIQUE" value="end"/> , <tag name="TECHNIQUE" value="start"/>local syntactic structure<tag name="TECHNIQUE" value="end"/> , and other properties of the source language sentence to help predict each phrase 's translation .
Our approach requires very little computation beyond the standard <tag name="TECHNIQUE" value="start"/>phrase extraction<tag name="TECHNIQUE" value="end"/> algorithm and scales well to large data scenarios .
We report significant improvements in automatic evaluation scores for Chineseto-EnglishandEnglish-to-Germantranslation , and also describe our entry in the WMT-08 shared task based on this approach .


##P06-2032
<tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Coreference Handling In XMG<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> .
We claim that existing specification languages for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>tree based grammars<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> fail to adequately support identifier managment .
We then show that <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>XMG -LRB- eXtensible MetaGrammar -RRB-<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> provides a sophisticated treatment of identifiers which is effective in supporting a <tag name="DOMAIN" value="start"/>linguist-friendly grammar design<tag name="DOMAIN" value="end"/> .


##N06-1057
<tag name="FOCUS" value="start"/>ParaEval<tag name="FOCUS" value="end"/> : Using <tag name="TECHNIQUE" value="start"/>Paraphrases<tag name="TECHNIQUE" value="end"/> To <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Evaluate Summaries<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Automatically .
<tag name="FOCUS" value="start"/>ParaEval<tag name="FOCUS" value="end"/> is an automated <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>evaluation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> method for comparing reference and peer <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>summaries<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
It facilitates a tieredcomparison strategy where recall-oriented global optimal and local greedy searches for <tag name="TECHNIQUE" value="start"/>paraphrase<tag name="TECHNIQUE" value="end"/> matching are enabled in the top tiers .
We utilize a domainindependent <tag name="TECHNIQUE" value="start"/>paraphrase<tag name="TECHNIQUE" value="end"/> table extracted from a large bilingual parallel corpus using methods from <tag name="TECHNIQUE" value="start"/>Machine Translation<tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/>MT<tag name="TECHNIQUE" value="end"/> -RRB- .
We show that the quality of <tag name="FOCUS" value="start"/>ParaEval<tag name="FOCUS" value="end"/> 's evaluations , measured by correlating with human judgments , closely resembles that of ROUGE 's .


##W09-0805
<tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Unsupervised<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>Concept Discovery<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> In Hebrew Using Simple <tag name="TECHNIQUE" value="start"/>Unsupervised Word Prefix Segmentation<tag name="TECHNIQUE" value="end"/> for Hebrew and Arabic .
Fully unsupervised pattern-based methods for discovery of word categories have been proven to be useful in several languages .
The majority of these methods rely on the existence of function words as separate text units .
However , in morphology-rich languages , in particular Semitic languages such as Hebrew and Arabic , the equivalents of such function words are usually written as morphemes attached as prefixes to other words .
As a result , they are missed by word-based pattern discovery methods , causing many useful patterns to be undetected and a drastic deterioration in performance .
To enable high quality <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>lexical category<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> acquisition , we propose a simple <tag name="TECHNIQUE" value="start"/>unsupervised word segmentation<tag name="TECHNIQUE" value="end"/> algorithm that separates these morphemes .
We study the performance of the algorithm for Hebrew and Arabic , and show that it indeed improves a state-of-art <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/> unsupervised<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>concept acquisition<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> algorithm in Hebrew .


##N07-1065
Automatic <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Answer Typing for How-Questions<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We introduce an <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>answer typing strategy specific to quantifiable how questions<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
Using the <tag name="TECHNIQUE" value="start"/>web<tag name="TECHNIQUE" value="end"/> as a data source , we automatically collect answer units appropriate to a given <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>how-question<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> type .
Experimental results show <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>answer typing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> with these units outperforms traditional fixedcategory <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>answer typing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> and other strategies based on the occurrences of numerical entities in text .


##C08-1117
Using Three Way Data for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Word Sense Discrimination<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
In this paper , an extension of a <tag name="TECHNIQUE" value="start"/>dimensionality reduction<tag name="TECHNIQUE" value="end"/> algorithm called <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>NONNEGATIVE MATRIX FACTORIZATION<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> is presented that combines both ` <tag name="TECHNIQUE" value="start"/>bag of words<tag name="TECHNIQUE" value="end"/> ' data and <tag name="TECHNIQUE" value="start"/>syntactic<tag name="TECHNIQUE" value="end"/> data , in order to find semantic dimensions according to which both words and syntactic relations can be classified .
The use of three way data allows one to determine which dimension -LRB- s -RRB- are responsible for a certain <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>sense of a word<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , and adapt the corresponding feature vector accordingly , ` subtracting ' one sense to discover another one .
The intuition in this is that the <tag name="TECHNIQUE" value="start"/>syntactic features of the syntax-based<tag name="TECHNIQUE" value="end"/> approach can be disambiguated by the semantic dimensions found by the <tag name="TECHNIQUE" value="start"/>bag of words<tag name="TECHNIQUE" value="end"/> approach .
The novel approach is embedded into <tag name="TECHNIQUE" value="start"/>clustering<tag name="TECHNIQUE" value="end"/> algorithms , to make it fully automatic .
The approach is carried out for Dutch , and evaluated against EuroWordNet .


##I08-2131
Language Independent <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Text Correction<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> using <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Finite State Automata<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> .
Many natural language applications , like machine translation and information extraction , are required to operate on text with <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>spelling errors<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
Those <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>spelling mistakes<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> have to be corrected automatically to avoid deteriorating the performance of such applications .
In this work , we introduce a novel approach for automatic <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>correction of spelling mistakes<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> by deploying <tag name="TECHNIQUE" value="start"/>finite state automata<tag name="TECHNIQUE" value="end"/> to propose candidates corrections withinaspecifiededitdistancefromthemisspelled word .
After choosing candidate corrections , a <tag name="TECHNIQUE" value="start"/>language model<tag name="TECHNIQUE" value="end"/> is used to assign scores the candidate corrections and choose best correction in the given context .
The proposed approach is language independent and requires only a <tag name="TECHNIQUE" value="start"/>dictionary<tag name="TECHNIQUE" value="end"/> and text data for building a <tag name="TECHNIQUE" value="start"/>language model<tag name="TECHNIQUE" value="end"/> .
The approach have been tested on both Arabic and English text and achieved accuracy of 89 % .


##J85-4001
On The <tag name="FOCUS" value="start"/>Complexity Of <tag name="TECHNIQUE" value="start"/>ID\/LP<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>Parsing<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
Modern linguistic theory attributes surface complexity to interacting subsystems of constraints .
For instance , the <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>ID\/LP<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>grammar formalism<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> separates constraints on immediate dominance from those on linear order .
An <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>ID\/LP<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>parsing<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> algorithm by Shieber shows how to use <tag name="TECHNIQUE" value="start"/>ID and LP constraints<tag name="TECHNIQUE" value="end"/> directly in language processing , without expanding them into an intermediate context-free `` object grammar '' .
However , Shieber 's purported runtime bound underestimates the difficulty of <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>ID\/LP<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>parsing<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
<tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>ID\/LP<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>parsing<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> is actually NP-complete , and the worst-case runtime of Shieber 's algorithm is actually exponential in grammar size .
The growth of parser data structures causes the difficulty .
Some computational and linguistic implications follow ; in particular , it is important to note that , desplte its potential for combinatorial explosion , Shieber 's algorithm remains better than the alternative of parsing an expanded object grammar .


##P00-1037
An Improved Error Model For <tag name="TECHNIQUE" value="start"/>Noisy Channel<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Spelling Correction<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
The <tag name="TECHNIQUE" value="start"/>noisy channel<tag name="TECHNIQUE" value="end"/> model has been applied to a wide range of problems , including <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>spelling correction<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
These models consist of two components : a source model and a <tag name="TECHNIQUE" value="start"/>channel model<tag name="TECHNIQUE" value="end"/> .
Very little research has gone into improving the <tag name="TECHNIQUE" value="start"/>channel model<tag name="TECHNIQUE" value="end"/> for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>spelling correction<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
This paper describes a new <tag name="TECHNIQUE" value="start"/>channel model<tag name="TECHNIQUE" value="end"/> for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>spelling correction<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , based on generic string to <tag name="TECHNIQUE" value="start"/>string edits<tag name="TECHNIQUE" value="end"/> .
Using this model gives significant performance improvements compared to previously proposed models .


##W06-0906
Extending <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>TimeML<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> With Typical <tag name="FOCUS" value="start"/>Durations Of Events<tag name="FOCUS" value="end"/> .
In this paper , we demonstrate how to extend <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>TimeML<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> , a rich specification language for <tag name="DOMAIN" value="start"/>event and temporal expressions in text<tag name="DOMAIN" value="end"/> , with the implicit typical <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>durations of events<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>temporal information<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> in text that has hitherto been largely unexploited .
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Event duration<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> information can be very important in applications in which the time course of events is to be extracted from text .
For example , whether two events overlap or are in sequence often depends very much on their durations .


##N03-2027
<tag name="TECHNIQUE" value="start"/>Bayesian Nets<tag name="TECHNIQUE" value="end"/> For <tag name="DOMAIN" value="start"/>Syntactic Categorization<tag name="DOMAIN" value="end"/> Of Novel Words .
This paper presents an application of a <tag name="TECHNIQUE" value="start"/>Dynamic Bayesian Network<tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/>DBN<tag name="TECHNIQUE" value="end"/> -RRB- to the task of assigning <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Part-of-Speech<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> -LRB- <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>PoS<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> -RRB- tags to novel text .
This task is particularly challenging for non-standard corpora , such as Internet lingo , where a large proportion of words are unknown .
Previous work reveals that <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>PoS tags<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> depend on a variety of <tag name="TECHNIQUE" value="start"/>morphological<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>contextual features<tag name="TECHNIQUE" value="end"/> .
Representing these dependencies in a <tag name="TECHNIQUE" value="start"/>DBN<tag name="TECHNIQUE" value="end"/> results into an elegant and effective <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>PoS tagger<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .


##C94-2191
An Integrated Model For <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Anaphora Resolution<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
The paper discusses a new <tag name="TECHNIQUE" value="start"/>knowledgebased and sublanguage-oriented<tag name="TECHNIQUE" value="end"/> model for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>anaphora resolution<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , which integrates <tag name="TECHNIQUE" value="start"/>syntactic , semantic , discourse , domain and heuristical<tag name="TECHNIQUE" value="end"/> knowledge for the sublanguage of computer science .
Special attention is paid to a new approach for tracking the center throughout a discourse segment , which plays an imtx ~ rtant role in proposing the most likely antecedent to the anaphor in case of ambiguity .


##P04-1058
Alternative Approaches For <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Generating Bodies Of Grammar Rules<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We compare two approaches for describing and <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>generating bodies of rules<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> used for natural language <tag name="DOMAIN" value="start"/>parsing<tag name="DOMAIN" value="end"/> .
In today 's parsers rule bodies do not exist a priori but are generated on the fly , usually with methods based on n-grams , which are one particular way of inducing probabilistic regular languages .
We compare two approaches for inducing such languages .
One is based on <tag name="TECHNIQUE" value="start"/>n-grams<tag name="TECHNIQUE" value="end"/> , the other on minimization of the <tag name="TECHNIQUE" value="start"/>Kullback-Leibler divergence<tag name="TECHNIQUE" value="end"/> .
The inferred regular languages are used for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>generating bodies of rules<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> inside a <tag name="DOMAIN" value="start"/>parsing<tag name="DOMAIN" value="end"/> procedure .
We compare the two approaches along two dimensions : the quality of the <tag name="TECHNIQUE" value="start"/>probabilistic<tag name="TECHNIQUE" value="end"/> regular language they produce , and the performance of the <tag name="DOMAIN" value="start"/>parser<tag name="DOMAIN" value="end"/> they were used to build .
The second approach outperforms the first one along both dimensions .


##P08-2027
A <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Unified Syntactic<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> Model for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Parsing Fluent and Disfluent Speech<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
This paper describes a <tag name="TECHNIQUE" value="start"/>syntactic representation<tag name="TECHNIQUE" value="end"/> for modeling <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>speech repairs<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
This representation makes use of a right corner transform of <tag name="TECHNIQUE" value="start"/>syntax trees<tag name="TECHNIQUE" value="end"/> to produce a tree representation in which <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>speech repairs<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> require very few special <tag name="TECHNIQUE" value="start"/>syntax rules<tag name="TECHNIQUE" value="end"/> , making better use of training data .
<tag name="TECHNIQUE" value="start"/>PCFGs<tag name="TECHNIQUE" value="end"/> trained on <tag name="TECHNIQUE" value="start"/>syntax trees<tag name="TECHNIQUE" value="end"/> using this model achieve high accuracy on the standard Switchboard <tag name="DOMAIN" value="start"/>parsing<tag name="DOMAIN" value="end"/> task .


##C96-1041
<tag name="TECHNIQUE" value="start"/>Markov Random Field<tag name="TECHNIQUE" value="end"/> Based English <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Part-Of-Speech Tagging<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> System .
Probabilistic models have been widely used for natural language processing .
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Part-of-speech tagging<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , which assigns the most likely tag to each word in a given sentence , is one .
of tire problems which can be solved by statisticM approach .
Many researchers haw ~ tried to solve the problem by hidden Marker model -LRB- HMM -RRB- , which is well known as one of the statistical models .
But it has many difficulties : integrating heterogeneous information , coping with data sparseness prohlem , and adapting to new environments .
In this paper , we propose a <tag name="TECHNIQUE" value="start"/>Markov radom field<tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/>MRF<tag name="TECHNIQUE" value="end"/> -RRB- model based approach to the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>tagging<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> problem .
The <tag name="TECHNIQUE" value="start"/>MRF<tag name="TECHNIQUE" value="end"/> provides the base frame to combine various statistical information with <tag name="TECHNIQUE" value="start"/>maximum entropy<tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/>ME<tag name="TECHNIQUE" value="end"/> -RRB- method .
As <tag name="TECHNIQUE" value="start"/>Gibbs distribution<tag name="TECHNIQUE" value="end"/> can be used to describe a posteriori probability of tagging , we use it in ma .
ximum a posteriori -LRB- MAP -RRB- estimation of optimizing process .
Besides , several <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>tagging<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> models are developed to show the effect of adding information .
Experimental results show that the performance of the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>tagger<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> gets improved as we add more <tag name="TECHNIQUE" value="start"/>statistical<tag name="TECHNIQUE" value="end"/> information , and that Mt -LCB- F-based <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>tagging<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> model is better than ttMM based <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>tagging<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> model in data sparseness problem .


##W04-3228
<tag name="TECHNIQUE" value="start"/>Dependencies<tag name="TECHNIQUE" value="end"/> Vs. <tag name="TECHNIQUE" value="start"/>Constituents<tag name="TECHNIQUE" value="end"/> For <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Tree-Based Alignment<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
Given a parallel parsed corpus , <tag name="TECHNIQUE" value="start"/>statistical<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>treeto-tree alignment<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> attempts to match nodes in the syntactic trees for a given sentence in two languages .
We train a <tag name="TECHNIQUE" value="start"/>probabilistic tree transduction<tag name="TECHNIQUE" value="end"/> model on a large automatically parsed Chinese-English corpus , and evaluate results against human-annotated word level alignments .
We find that a <tag name="TECHNIQUE" value="start"/>constituent-based<tag name="TECHNIQUE" value="end"/> model performs better than a similar probability model trained on the same trees converted to a <tag name="TECHNIQUE" value="start"/>dependency<tag name="TECHNIQUE" value="end"/> representation .


##C00-2117
Text <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Genre<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Detection Using <tag name="TECHNIQUE" value="start"/>Common Word Frequencies<tag name="TECHNIQUE" value="end"/> .
In this paper we present a method for detecting the text <tag name="DOMAIN" value="start"/>genre<tag name="DOMAIN" value="end"/> quickly and easily following an approach originally proposed in authorship attribution studies which uses as style markers the frequencies of occurrence of the most frequent words in a training corpus -LRB- Burrows , 1992 -RRB- .
In contrast to this approach we use the frequencies of occurrence of the most <tag name="TECHNIQUE" value="start"/>frequent words<tag name="TECHNIQUE" value="end"/> of the entire written language .
Using as testing ground a part of the Wall Street Journal corpus , we show that the most <tag name="TECHNIQUE" value="start"/>frequent words<tag name="TECHNIQUE" value="end"/> of the British National Corpus , representing the most <tag name="TECHNIQUE" value="start"/>frequent words<tag name="TECHNIQUE" value="end"/> of the written English language , are more reliable discriminators of text <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>genre<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> in comparison to the most frequent words of the training corpus .
Moreover , the fi'equencies of occurrence of the most common punctuation marks play an important role in terms of accurate text categorization as well as when dealing with training data of limited size .


##H91-1049
A <tag name="TECHNIQUE" value="start"/>Dynamical<tag name="TECHNIQUE" value="end"/> System Approach To <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Continuous Speech Recognition<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
A <tag name="TECHNIQUE" value="start"/>dynamical<tag name="TECHNIQUE" value="end"/> system model is proposed for better representing the <tag name="TECHNIQUE" value="start"/>spectral dynamics<tag name="TECHNIQUE" value="end"/> of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>speech for recognition<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We assume that the observed feature vectors of a phone segment are the output of a <tag name="TECHNIQUE" value="start"/>stochastic linear dynamical<tag name="TECHNIQUE" value="end"/> system and consider two alternative assumptions regarding the relationship of the segment length and the evolution of the dynamics .
Training is equivalent to the identification of a <tag name="TECHNIQUE" value="start"/>stochastic linear<tag name="TECHNIQUE" value="end"/> system , and we follow a nontraditional approach based on the <tag name="TECHNIQUE" value="start"/>Estlmate-Maximize<tag name="TECHNIQUE" value="end"/> algorithm .
We evaluate this model on a <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>phoneme classification<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> task using the TIMIT database .


##C04-1021
Modern <tag name="DOMAIN" value="start"/>Natural Language Interfaces To Databases<tag name="DOMAIN" value="end"/> : Composing <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Statistical Parsing<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> With <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Semantic Tractability<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> .
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Natural Language Interfaces to Databases<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> -LRB- <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>NLIs<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> -RRB- can benefit from the advances in <tag name="TECHNIQUE" value="start"/>statistical parsing<tag name="TECHNIQUE" value="end"/> over the last fifteen years or so .
However , statistical parsers require training on a massive , labeled corpus , and manually creating such a corpus for each database is prohibitively expensive .
To address this quandary , this paper reports on the <tag name="FOCUS" value="start"/>PRECISE NLI<tag name="FOCUS" value="end"/> , which uses a <tag name="TECHNIQUE" value="start"/>statistical parser<tag name="TECHNIQUE" value="end"/> as a `` plug in '' .
The paper shows how a strong <tag name="TECHNIQUE" value="start"/>semantic<tag name="TECHNIQUE" value="end"/> model coupled with ``<tag name="TECHNIQUE" value="start"/> light re-training<tag name="TECHNIQUE" value="end"/> '' enables <tag name="FOCUS" value="start"/>PRECISE<tag name="FOCUS" value="end"/> to overcome <tag name="TECHNIQUE" value="start"/>parser<tag name="TECHNIQUE" value="end"/> errors , and correctly map from parsed questions to the corresponding SQL queries .
We discuss the issues in using <tag name="TECHNIQUE" value="start"/>statistical parsers<tag name="TECHNIQUE" value="end"/> to build <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>database-independent NLIs<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , and report on experimental results with the benchmark ATIS data set where PRECISE achieves 94 % accuracy .


##W99-0706
Learning <tag name="TECHNIQUE" value="start"/>Transformation Rules<tag name="TECHNIQUE" value="end"/> To Find <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Grammatical Relations<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Grammatical relationships<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> are an important level of natural language processing .
We present a trainable approach to find these relationships through <tag name="TECHNIQUE" value="start"/>transformation sequences and-error-driven learning<tag name="TECHNIQUE" value="end"/> .
Our approach finds <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>grammatical relationships<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> between core syntax groups and bypasses much of the parsing phase .
On our training and test set , our procedure achieves 63.6 % recall and 77.3 % precision -LRB- f-score = 69.8 -RRB- .


##N04-2002
Identifying <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Chemical Names In Biomedical Text<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> : An Investigation Of <tag name="TECHNIQUE" value="start"/>Substring Co-Occurrence<tag name="TECHNIQUE" value="end"/> Based Approaches .
We investigate various strategies for finding <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>chemicals in biomedical text<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> using <tag name="TECHNIQUE" value="start"/>substring co-occurrence<tag name="TECHNIQUE" value="end"/> information .
The goal is to build a system from readily available data with minimal human involvement .
Our models are trained from a <tag name="TECHNIQUE" value="start"/>dictionary<tag name="TECHNIQUE" value="end"/> of chemical names and general biomedical text .
We investigated several strategies including <tag name="TECHNIQUE" value="start"/>NaÃ¯ve Bayes classifiers<tag name="TECHNIQUE" value="end"/> and several types of <tag name="TECHNIQUE" value="start"/>N-gram models<tag name="TECHNIQUE" value="end"/> .
We introduced a new way of interpolating <tag name="TECHNIQUE" value="start"/>N-grams<tag name="TECHNIQUE" value="end"/> that does not require tuning any parameters .
We also found the task to be similar to Language Identification .


##A00-2039
<tag name="TECHNIQUE" value="start"/>Finite-State <tag name="TECHNIQUE" value="end"/> Reduplication In One-Level <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Prosodic Morphology<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
Reduplication , a central instance of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>prosodic morphology<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , is particularly challenging for state-ofthe-art <tag name="DOMAIN" value="start"/>computational morphology<tag name="DOMAIN" value="end"/> , since it involves copying of some part of a phonological string .
In this paper I advocate a <tag name="TECHNIQUE" value="start"/>finite-state<tag name="TECHNIQUE" value="end"/> method that combines enriched <tag name="TECHNIQUE" value="start"/>lexical representations<tag name="TECHNIQUE" value="end"/> via intersection to implement the copying .
The proposal includes a resource-conscious variant of <tag name="TECHNIQUE" value="start"/>automata<tag name="TECHNIQUE" value="end"/> and can benefit from the existence of <tag name="TECHNIQUE" value="start"/>lazy<tag name="TECHNIQUE" value="end"/> algorithms .
Finally , the implementation of a complex case from Koasati is presented .


##P98-2170
A Procedure for <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Multi-Class Discrimination<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> and some Linguistic Applications .
The paper describes a novel computational tool for <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>multiple concept learning<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> .
Unlike previous approaches , whose major goal is prediction on unseen instances rather than the legibility of the output , our <tag name="TECHNIQUE" value="start"/>MPD<tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/>Maximally Parsimonious Discrimination<tag name="TECHNIQUE" value="end"/> -RRB- program emphasizes the conciseness and intelligibility of the resultant class descriptions , using three intuitive simplicity criteria to this end .
We illustrate <tag name="TECHNIQUE" value="start"/>MPD<tag name="TECHNIQUE" value="end"/> with applications in <tag name="DOMAIN" value="start"/>componential analysis<tag name="DOMAIN" value="end"/> -LRB- in <tag name="DOMAIN" value="start"/>lexicology and phonology<tag name="DOMAIN" value="end"/> -RRB- , <tag name="DOMAIN" value="start"/>language typology<tag name="DOMAIN" value="end"/> , and <tag name="DOMAIN" value="start"/>speech pathology<tag name="DOMAIN" value="end"/> .


##I05-1083
An Empirical Study on <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Language Model Adaptation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Using a <tag name="TECHNIQUE" value="start"/>Metric of Domain Similarity<tag name="TECHNIQUE" value="end"/> .
This paper presents an empirical study on four techniques of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>language model adaptation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , including a <tag name="TECHNIQUE" value="start"/>maximum a posteriori<tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/>MAP<tag name="TECHNIQUE" value="end"/> -RRB- method and three <tag name="TECHNIQUE" value="start"/>discriminative training<tag name="TECHNIQUE" value="end"/> models , in the application of Japanese Kana-Kanji conversion .
We compare the performance of these methods from various angles by <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>adapting<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> the baseline model to four adaptation domains .
In particular , we attempt to interpret the results given in terms of the character error rate -LRB- CER -RRB- by correlating them with the characteristics of the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>adaptation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> domain measured using the <tag name="TECHNIQUE" value="start"/>information-theoretic<tag name="TECHNIQUE" value="end"/> notion of <tag name="TECHNIQUE" value="start"/>cross entropy<tag name="TECHNIQUE" value="end"/> .
We show that such a metric correlates well with the CER performance of the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>adaptation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> methods , and also show that the <tag name="TECHNIQUE" value="start"/>discriminative<tag name="TECHNIQUE" value="end"/> methods are not only superior to a <tag name="TECHNIQUE" value="start"/>MAP-based<tag name="TECHNIQUE" value="end"/> method in terms of achieving larger CER reduction , but are also more robust against the similarity of background and adaptation domains .


##N07-2041
Simultaneous Identification of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Biomedical Named-Entity<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> and <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Functional Relation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Using <tag name="TECHNIQUE" value="start"/>Statistical Parsing<tag name="TECHNIQUE" value="end"/> Techniques .
In this paper we propose a <tag name="TECHNIQUE" value="start"/>statistical parsing<tag name="TECHNIQUE" value="end"/> technique that simultaneously identifies <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>biomedical named-entities<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>NEs<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> -RRB- and extracts subcellular <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>localization relations<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> for bacterial proteins from the text in MEDLINE articles .
We build a <tag name="TECHNIQUE" value="start"/>parser<tag name="TECHNIQUE" value="end"/> that derives both <tag name="TECHNIQUE" value="start"/>syntactic<tag name="TECHNIQUE" value="end"/> and domain-dependent <tag name="TECHNIQUE" value="start"/>semantic<tag name="TECHNIQUE" value="end"/> information and achieves an F-score of 48.4 % for the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>relation extraction<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> task .
We then propose a <tag name="TECHNIQUE" value="start"/>semi-supervised<tag name="TECHNIQUE" value="end"/> approach that incorporates noisy automatically labeled data to improve the F-score of our parser to 83.2 % .
Our key contributions are : learning from noisy data , and building an annotated corpus that can benefit <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>relation extraction<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> research .
1 Introduction Relation extraction from text is a step beyond Named-Entity Recognition -LRB- NER -RRB- and generally demands adequate domain knowledge to build relations among domain-specific concepts .
A Biomedical Functional Relation -LRB- relation for short -RRB- states interactions among biomedical substances .
In this paper we focus on one such relation : Bacterial Protein Localization -LRB- BPL -RRB- , and introduce our approach for identifying BPLs from MEDLINE1 articles .
BPL is a key functional characteristic of proteins .
It is essential to the understanding of the function of different proteins and the discovery of suitable drugs , vaccines and diagnostic targets .
We are collaborating with researchers in molecular biology with the goal of automatically extracting BPLs from â This research was partially supported by NSERC , Canada .


##W04-1808
Discovering <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Synonyms<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> And Other Related Words .
Discovering <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>synonyms<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> and other related words among the words in a document collection can be seen as a <tag name="TECHNIQUE" value="start"/>clustering<tag name="TECHNIQUE" value="end"/> problem , where we expect the words in a cluster to be closely related to one another .
The intuition is that words occurring in similar contexts tend to convey similar meaning .
We introduce a way to use <tag name="TECHNIQUE" value="start"/>translation dictionaries<tag name="TECHNIQUE" value="end"/> for several languages to evaluate the rate of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>synonymy<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> found in the word clusters .
We also apply the <tag name="TECHNIQUE" value="start"/>information radius<tag name="TECHNIQUE" value="end"/> to calculating similarities between words using a full <tag name="TECHNIQUE" value="start"/>dependency syntactic feature space<tag name="TECHNIQUE" value="end"/> , and introduce a method for similarity recalculation during <tag name="TECHNIQUE" value="start"/>clustering<tag name="TECHNIQUE" value="end"/> as a fast approximation of the high-dimensional feature space .
Finally , we show that 69-79 % of the words in the clusters we discover are useful for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>thesaurus construction<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .


##I05-1086
A <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Case-Based Reasoning<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> Approach for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Speech Corpus Generation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
Corpus-based stochastic language models have achieved significant success in speech recognition , but construction of a corpus pertaining to a specific application is a difficult task .
This paper introduces a <tag name="TECHNIQUE" value="start"/>Case-Based Reasoning<tag name="TECHNIQUE" value="end"/> system to <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>generate natural language<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> corpora .
In comparison to traditional <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>natural language generation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> approaches , this system overcomes the inflexibility of template-based methods while avoiding the linguistic sophistication of rule-based packages .
The evaluation of the system indicates our approach is effective in <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>generating<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> users ' specifications or queries as 98 % of the generated sentences are grammatically correct .
The study result also shows that the <tag name="TECHNIQUE" value="start"/>language model<tag name="TECHNIQUE" value="end"/> derived from the generated corpus can significantly outperform a general language model or a dictation grammar .


##W08-0120
A <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Frame-Based Probabilistic<tag name="FOCUS" value="end"/> <tag name="TECHNIQUE" value="end"/> Framework for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Spoken Dialog Management<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Using Dialog  <tag name="TECHNIQUE" value="start"/>Examples<tag name="TECHNIQUE" value="end"/> .
This paper proposes a <tag name="TECHNIQUE" value="start"/>probabilistic<tag name="TECHNIQUE" value="end"/> framework for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>spoken dialog management<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> using dialog examples .
To overcome the complexity problems of the classic partially observable Markov decision processes -LRB- POMDPs -RRB- based dialog manager , we use a <tag name="TECHNIQUE" value="start"/>frame-based belief state representation<tag name="TECHNIQUE" value="end"/> that reduces the complexity of belief update .
We also used dialog <tag name="TECHNIQUE" value="start"/>examples<tag name="TECHNIQUE" value="end"/> to maintain a reasonable number of system actions to reduce the complexity of the optimizing policy .
We developed <tag name="DOMAIN" value="start"/>weather information<tag name="DOMAIN" value="end"/> and <tag name="DOMAIN" value="start"/>car navigation dialog<tag name="DOMAIN" value="end"/> system that employed a <tag name="TECHNIQUE" value="start"/>frame-based probabilistic<tag name="TECHNIQUE" value="end"/> framework .
This framework enables people to develop a <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>spoken dialog<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> system using a <tag name="TECHNIQUE" value="start"/>probabilistic<tag name="TECHNIQUE" value="end"/> approach without complexity problem of POMDP .


##C02-1064
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Text Generation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> From Keywords .
We describe a method for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>generating sentences<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> from `` keywords '' or `` headwords '' .
This method consists of two main parts , candidate-text construction and evaluation .
The construction part <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>generates text sentences<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> in the form of <tag name="TECHNIQUE" value="start"/>dependency trees<tag name="TECHNIQUE" value="end"/> by using complementary information to replace information that is missing because of a `` knowledge gap '' and other missing function words to <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>generate natural text sentences<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> based on a particular monolingual corpus .
The evaluation part consists of a model for generating an appropriate text when given keywords .
This model considers not only <tag name="TECHNIQUE" value="start"/>word n-gram<tag name="TECHNIQUE" value="end"/> information , but also <tag name="TECHNIQUE" value="start"/>dependency<tag name="TECHNIQUE" value="end"/> information between words .
Furthermore , it considers both string information and <tag name="TECHNIQUE" value="start"/>morphological<tag name="TECHNIQUE" value="end"/> information .


##P04-1075
<tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Multi-Criteria-Based Active Learning<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> For <tag name="DOMAIN" value="start"/>Named Entity Recognition<tag name="DOMAIN" value="end"/> .
In this paper , we propose a <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>multi-criteria based active learning<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> approach and effectively apply it to <tag name="DOMAIN" value="start"/>named entity recognition<tag name="DOMAIN" value="end"/> .
<tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Active learning<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> targets to minimize the human annotation efforts by selecting examples for labeling .
To maximize the contribution of the selected examples , we consider the multiple criteria : informativeness , representativeness and diversity and propose measures to quantify them .
More comprehensively , we incorporate all the criteria using two selection strategies , both of which result in less labeling cost than single - criterion-based method .
The results of the <tag name="DOMAIN" value="start"/>named entity recognition<tag name="DOMAIN" value="end"/> in both MUC-6 and GENIA show that the labeling cost can be reduced by at least 80 % without degrading the performance .


##P06-2071
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Discriminating Image Senses<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> By <tag name="TECHNIQUE" value="start"/>Clustering With Multimodal Features<tag name="TECHNIQUE" value="end"/> .
We discuss <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Image Sense Discrimination<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> -LRB- <tag name="DOMAIN" value="start"/>ISD<tag name="DOMAIN" value="end"/> -RRB- , and apply a method based on <tag name="TECHNIQUE" value="start"/>spectral clustering<tag name="TECHNIQUE" value="end"/> , using <tag name="TECHNIQUE" value="start"/>multimodal features<tag name="TECHNIQUE" value="end"/> from the image and text of the embedding web page .
We evaluate our method on a new data set of annotated web images , retrieved with ambiguous query terms .
Experiments investigate different levels of sense granularity , as well as the impact of text and image features , and global versus local text features .


##P98-1066
A <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Layered<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> Approach to <tag name="TECHNIQUE" value="start"/>NLP-Based<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Information Retrieval<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
A <tag name="TECHNIQUE" value="start"/>layered<tag name="TECHNIQUE" value="end"/> approach to <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>information retrieval<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> permits the inclusion of multiple search engines as well as multiple databases , with a natural language layer to convert English queries for use by the various search engines .
The <tag name="TECHNIQUE" value="start"/>NLP layer<tag name="TECHNIQUE" value="end"/> incorporates <tag name="TECHNIQUE" value="start"/>morphological<tag name="TECHNIQUE" value="end"/> analysis , <tag name="TECHNIQUE" value="start"/>noun phrase syntax<tag name="TECHNIQUE" value="end"/> , and <tag name="TECHNIQUE" value="start"/>semantic<tag name="TECHNIQUE" value="end"/> expansion based on <tag name="TECHNIQUE" value="start"/>WordNet<tag name="TECHNIQUE" value="end"/> .


##P07-2049
Measuring Importance and Query Relevance in <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Topic-focused Multi-document Summarization<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
The increasing complexity of summarization systems makes it difficult to analyze exactly which modules make a difference in performance .
We carried out a principled comparison between the two most commonly used schemes for assigning importance to words in the context of <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>query focused multi-document summarization<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> : <tag name="TECHNIQUE" value="start"/>raw frequency<tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/>word probability<tag name="TECHNIQUE" value="end"/> -RRB- and <tag name="TECHNIQUE" value="start"/>log-likelihood ratio<tag name="TECHNIQUE" value="end"/> .
We demonstrate that the advantages of <tag name="TECHNIQUE" value="start"/>log-likelihood ratio<tag name="TECHNIQUE" value="end"/> come from its known <tag name="TECHNIQUE" value="start"/>distributional<tag name="TECHNIQUE" value="end"/> properties which allow for the identification of a set of words that in its entirety defines the aboutness of the input .
We also find that <tag name="TECHNIQUE" value="start"/>LLR<tag name="TECHNIQUE" value="end"/> is more suitable for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>query-focused summarization<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> since , unlike raw frequency , it is more sensitive to the integration of the information need defined by the user .


##W97-1202
<tag name="FOCUS" value="start"/>Message-To-Speech<tag name="FOCUS" value="end"/> : High Quality <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Speech Generation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> For <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Messaging And Dialogue Systems<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
In this paper , we present a <tag name="FOCUS" value="start"/>Message-toSpeech<tag name="FOCUS" value="end"/> -LRB- <tag name="FOCUS" value="start"/>MTS<tag name="FOCUS" value="end"/> -RRB- system that offers the linguistic flexibility desired for <tag name="DOMAIN" value="start"/>spoken dialogue and message generating<tag name="DOMAIN" value="end"/> systems .
The use of <tag name="TECHNIQUE" value="start"/>prosody transplantation<tag name="TECHNIQUE" value="end"/> and special purpose <tag name="TECHNIQUE" value="start"/>prosody<tag name="TECHNIQUE" value="end"/> models results in highly natural prosody for the synthesised speech .


##P04-1088
<tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>FLSA<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> : Extending <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Latent Semantic Analysis<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> With Features For <tag name="DOMAIN" value="start"/>Dialogue Act Classification<tag name="DOMAIN" value="end"/> .
We discuss <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Feature Latent Semantic Analysis<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>FLSA<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> -RRB- , an extension to <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Latent Semantic Analysis<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>LSA<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> -RRB- .
<tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>LSA<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> is a <tag name="TECHNIQUE" value="start"/>statistical<tag name="TECHNIQUE" value="end"/> method that is ordinarily trained on words only ; <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>FLSA<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> adds to <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>LSA<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> the richness of the many other linguistic features that a corpus may be labeled with .
We applied <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>FLSA<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> to <tag name="DOMAIN" value="start"/>dialogue act classification<tag name="DOMAIN" value="end"/> with excellent results .
We report results on three corpora : CallHome Spanish , MapTask , and our own corpus of tutoring dialogues .


##P04-1014
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> The WSJ Using <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>CCG<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> And <tag name="TECHNIQUE" value="start"/>Log-Linear<tag name="TECHNIQUE" value="end"/> Models .
This paper describes and evaluates <tag name="TECHNIQUE" value="start"/>log-linear<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> models for <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Combinatory Categorial Grammar -LRB- CCG -RRB-<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
A parallel implementation of the <tag name="TECHNIQUE" value="start"/>L-BFGS optimisation<tag name="TECHNIQUE" value="end"/> algorithm is described , which runs on a Beowulf cluster allowing the complete Penn Treebank to be used for estimation .
We also develop a new efficient <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> algorithm for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>CCG<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> which maximises expected recall of dependencies .
We compare models which use all <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>CCG<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> derivations , including nonstandard derivations , with normal-form models .
The performances of the two models are comparable and the results are competitive with existing wide-coverage <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>CCG parsers<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .


##C02-1070
Inducing <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Information Extraction<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Systems For New Languages Via <tag name="TECHNIQUE" value="start"/>Cross-Language Projection<tag name="TECHNIQUE" value="end"/> .
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Information extraction<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> -LRB- <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>IE<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> -RRB- systems are costly to build because they require development texts , parsing tools , and specialized dictionaries for each application domain and each natural language that needs to be processed .
We present a novel method for rapidly creating <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>IE<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> systems for new languages by exploiting existing <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>IE<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> systems via <tag name="TECHNIQUE" value="start"/>crosslanguage projection<tag name="TECHNIQUE" value="end"/> .
Given an <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>IE<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> system for a source language -LRB- e.g. , English -RRB- , we can transfer its annotations to corresponding texts in a target language -LRB- e.g. , French -RRB- and learn <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>information extraction<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> <tag name="TECHNIQUE" value="start"/>rules<tag name="TECHNIQUE" value="end"/> for the new language automatically .
In this paper , we explore several ways of realizing both the transfer and learning processes using off-theshelf <tag name="TECHNIQUE" value="start"/>machine translation<tag name="TECHNIQUE" value="end"/> systems , <tag name="TECHNIQUE" value="start"/>induced word alignment<tag name="TECHNIQUE" value="end"/> , <tag name="TECHNIQUE" value="start"/>attribute projection<tag name="TECHNIQUE" value="end"/> , and <tag name="TECHNIQUE" value="start"/>transformationbased learning<tag name="TECHNIQUE" value="end"/> .
We present a variety of experiments that show how an English <tag name="DOMAIN" value="start"/><tag name="TECHNIQUE" value="start"/>IE<tag name="TECHNIQUE" value="end"/><tag name="DOMAIN" value="end"/> system for a plane crash domain can be leveraged to automatically create a French <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>IE<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> system for the same domain .


##W09-0203
<tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Unsupervised<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>Classification<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> with <tag name="TECHNIQUE" value="start"/>Dependency Based Word Spaces<tag name="TECHNIQUE" value="end"/> .
We present the results of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>clustering<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> experiments with a number of different evaluation sets using <tag name="TECHNIQUE" value="start"/>dependency based word spaces<tag name="TECHNIQUE" value="end"/> .
Contrary to previous results we found a clear advantage using a <tag name="TECHNIQUE" value="start"/>parsed corpus over word spaces<tag name="TECHNIQUE" value="end"/> constructed with the help of simple <tag name="TECHNIQUE" value="start"/>patterns<tag name="TECHNIQUE" value="end"/> .
We achieve considerable gains in performance over these spaces ranging between 9 and 13 % in absolute terms of cluster purity .


##D07-1102
<tag name="TECHNIQUE" value="start"/>Log-Linear<tag name="TECHNIQUE" value="end"/> Models of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Non-Projective Trees<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , <tag name="TECHNIQUE" value="start"/>$ k $ - best MST<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> and <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Tree-Ranking<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We present our system used in the CoNLL 2007 shared task on <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>multilingual parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
The system is composed of three components : a <tag name="TECHNIQUE" value="start"/>k-best maximum spanning tree -LRB- MST -RRB-<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>parser<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , a <tag name="TECHNIQUE" value="start"/>tree labeler<tag name="TECHNIQUE" value="end"/> , and a <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>reranker<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> that orders the k-best labeled trees .
We present two techniques for training the <tag name="TECHNIQUE" value="start"/>MST<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>parser<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> : <tag name="TECHNIQUE" value="start"/>tree-normalized<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>graphnormalized conditional training<tag name="TECHNIQUE" value="end"/> .
The <tag name="TECHNIQUE" value="start"/>treebased<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>reranking<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> model allows us to explicitly model global syntactic phenomena .
We describe the reranker features which include <tag name="TECHNIQUE" value="start"/>non-projective edge attributes<tag name="TECHNIQUE" value="end"/> .
We provide an analysis of the errors made by our system and suggest changes to the models and features that might rectify the current system .


##I08-1056
<tag name="TECHNIQUE" value="start"/>Cluster-Based Query Expansion<tag name="TECHNIQUE" value="end"/> for <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Statistical<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/> Question Answering<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
Document retrieval is a critical component of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>question answering<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> -LRB- <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>QA<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> -RRB- , yet little work has been done towards statistical modeling of queries and towards automatic generation of high quality query content for QA .
This paper introduces a new , <tag name="TECHNIQUE" value="start"/>cluster-based query expansion<tag name="TECHNIQUE" value="end"/> method that learns queries known to be successful when applied to similar questions .
We show that <tag name="TECHNIQUE" value="start"/>cluster-based expansion<tag name="TECHNIQUE" value="end"/> improves the retrieval performance of a <tag name="FOCUS" value="start"/> <tag name="TECHNIQUE" value="start"/>statistical<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>question answering<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> system when used in addition to existing query expansion methods .
This paper presents experiments with several <tag name="TECHNIQUE" value="start"/>feature selection<tag name="TECHNIQUE" value="end"/> methods used individually and in combination .
We show that documents retrieved using the <tag name="TECHNIQUE" value="start"/>cluster-based<tag name="TECHNIQUE" value="end"/> approach are inherently different than documents retrieved using existing methods and provide a higher data diversity to answers extractors .


##W09-0206
<tag name="DOMAIN" value="start"/>Positioning for Conceptual Development<tag name="DOMAIN" value="end"/> using <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Latent Semantic Analysis<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> .
With increasing opportunities to learn online , the problem of positioning learners in an educational network of content offers new possibilities for the utilisation of geometry-based natural language processing techniques .
In this article , the adoption of <tag name="TECHNIQUE" value="start"/>latent semantic analysis<tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/>LSA<tag name="TECHNIQUE" value="end"/> -RRB- for guiding learners in their conceptual development is investigated .
We propose five new algorithmic derivations of <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>LSA<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> and test their validity for positioning in an experiment in order to draw back conclusions on the suitability of machine learning from previously accredited evidence .
Special attention is thereby directed towards the role of distractors and the calculation of thresholds when using similarities as a proxy for assessing conceptual closeness .
Results indicate that <tag name="TECHNIQUE" value="start"/>learning<tag name="TECHNIQUE" value="end"/> improves <tag name="DOMAIN" value="start"/>positioning<tag name="DOMAIN" value="end"/> .
Distractors are of low value and seem to be replaceable by generic noise to improve threshold calculation .
Furthermore , new ways to flexibly calculate thresholds could be identified .


##P06-1079
Exploiting <tag name="TECHNIQUE" value="start"/>Syntactic Patterns<tag name="TECHNIQUE" value="end"/> As Clues In <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Zero-Anaphora Resolution<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We approach the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>zero-anaphora resolution<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> problem by decomposing it into intra-sentential and inter-sentential <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>zeroanaphora resolution<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
For the former problem , <tag name="TECHNIQUE" value="start"/>syntactic patterns<tag name="TECHNIQUE" value="end"/> of the appearance of zero-pronouns and their antecedents are useful clues .
Taking Japanese as a target language , we empirically demonstrate that incorporating rich <tag name="TECHNIQUE" value="start"/>syntactic pattern<tag name="TECHNIQUE" value="end"/> features in a state-of-the-art <tag name="TECHNIQUE" value="start"/>learning-based<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>anaphora resolution<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> model dramatically improves the accuracy of intra-sentential <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>zero-anaphora<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , which consequently improves the overall performance of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>zeroanaphora resolution<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .


##I08-3020
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Speech to speech machine translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> : Biblical chatter from Finnish to English .
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Speech-to-speech machine translation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> is in some ways the peak of natural language processing , in that it deals directly with our original , oral mode of communication -LRB- as opposed to derived written language -RRB- .
As such , it presents challenges that are not to be taken lightly .
Although existing technology covers each of the steps in the process , from speech recognition to synthesis , deriving a model of translation that is effective in the domain of spoken language is an interesting and challenging task .
If we could teach our algorithms to learn as children acquire language , the result would be useful both for language technology and cognitive science .
We propose several potential approaches , an implementation of a <tag name="TECHNIQUE" value="start"/>multi-path<tag name="TECHNIQUE" value="end"/> model that <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>translates<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> recognized morphemes alongside words,andaweb-interfacetotestourspeech translation tool as trained for Finnish to English .
We also discuss current approaches to <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>machine translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> and the problems they face in adapting simultaneously to morphologically rich languages and to the spoken modality .


##C92-1027
Compiling And Using <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Finite-State<tag name="TECHNIQUE" value="end"/><tag name="DOMAIN" value="start"/> Syntactic Rules<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
A language-independent framework for <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>syntactic finlte-state<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> is discussed .
The article presents a framework , a formalism , a compiler and a parser for grammars written in this forrealism .
As a substantial example , fragments from a nontrivial <tag name="TECHNIQUE" value="start"/>finite-state<tag name="TECHNIQUE" value="end"/> grammar of English are discussed .
The linguistic framework of the present approach is based on a <tag name="TECHNIQUE" value="start"/>surface syntactic tagging<tag name="TECHNIQUE" value="end"/> scheme by F. Karlsson .
This representation is slightly less powerful than phrase structure tree notation , letUng some ambiguous constructions be described more concisely .
The <tag name="TECHNIQUE" value="start"/>finite-state<tag name="TECHNIQUE" value="end"/> rule compiler implements what was briefly sketched by Koskenniemi -LRB- 1990 -RRB- .
It is based on the <tag name="TECHNIQUE" value="start"/>calculus of finite-state machines<tag name="TECHNIQUE" value="end"/> .
The compiler transforms rules into <tag name="TECHNIQUE" value="start"/>rule-automata<tag name="TECHNIQUE" value="end"/> .
The run-time parser exploits one of certain alternative strategies in performing the effective intersection of the <tag name="TECHNIQUE" value="start"/>rule automata<tag name="TECHNIQUE" value="end"/> and the <tag name="TECHNIQUE" value="start"/>sentence automaton<tag name="TECHNIQUE" value="end"/> .
Fragments of a fairly comprehensive <tag name="TECHNIQUE" value="start"/>finite-state<tag name="TECHNIQUE" value="end"/> granmmr of English axe presented here , including samples from non-finite constructions as a demonstration of the capacity of the present formalism , which goes far beyond plain disamblguation or part of speech tagging .
The grammar itself is directly related to a parser and tagging system for English created as a part of project SIMPR I using Karlsson 's <tag name="TECHNIQUE" value="start"/>CG<tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/>Constraint Grammar<tag name="TECHNIQUE" value="end"/> -RRB- formalism .


##I05-1018
Adapting a <tag name="TECHNIQUE" value="start"/>Probabilistic Disambiguation<tag name="TECHNIQUE" value="end"/> Model of an <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>HPSG<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Parser<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> to a New Domain .
This paper describes a method of adapting a domain-independent <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>HPSG<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>parser<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> to a <tag name="DOMAIN" value="start"/>biomedical<tag name="DOMAIN" value="end"/> domain .
Without modifying the grammar and the <tag name="TECHNIQUE" value="start"/>probabilistic<tag name="TECHNIQUE" value="end"/> model of the original <tag name="FOCUS" value="start"/> <tag name="TECHNIQUE" value="start"/>HPSG<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>parser<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> , we develop a <tag name="TECHNIQUE" value="start"/>log-linear<tag name="TECHNIQUE" value="end"/> model with additional features on a treebank of the <tag name="DOMAIN" value="start"/>biomedical<tag name="DOMAIN" value="end"/> domain .
Since the treebank of the target domain is limited , we need to exploit an original disambiguation model that was trained on a larger treebank .
Our model incorporates the original model as a reference <tag name="TECHNIQUE" value="start"/>probabilistic<tag name="TECHNIQUE" value="end"/> distribution .
The experimental results for our model trained with a small amount of a treebank demonstrated an improvement in <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> accuracy .


##C00-1002
Learning <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Word Clusters<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> From <tag name="TECHNIQUE" value="start"/>Data Types<tag name="TECHNIQUE" value="end"/> .
The paper illustrates a <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>linguistic knowledge acquisition<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> model making use of <tag name="TECHNIQUE" value="start"/>data types<tag name="TECHNIQUE" value="end"/> , <tag name="TECHNIQUE" value="start"/>infinite nlenlory<tag name="TECHNIQUE" value="end"/> , and an <tag name="TECHNIQUE" value="start"/>inferential mechanism<tag name="TECHNIQUE" value="end"/> tbr inducing new intbrmation Dora known data .
The mode -RRB- is colnpared with standard stochastic lnethods applied to data tokens , and tested on a task of <tag name="DOMAIN" value="start"/>lexico semantic classification<tag name="DOMAIN" value="end"/> .


##N03-2029
Automatic Derivation Of <tag name="TECHNIQUE" value="start"/>Surface Text Patterns<tag name="TECHNIQUE" value="end"/> For A <tag name="TECHNIQUE" value="start"/>Maximum Entropy<tag name="TECHNIQUE" value="end"/> Based <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Question Answering<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> System .
In this paper we investigate the use of <tag name="TECHNIQUE" value="start"/>surface text patterns<tag name="TECHNIQUE" value="end"/> for a <tag name="TECHNIQUE" value="start"/>Maximum Entropy<tag name="TECHNIQUE" value="end"/> based <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Question Answering<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> -LRB- <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>QA<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> -RRB- system .
These <tag name="TECHNIQUE" value="start"/>text patterns<tag name="TECHNIQUE" value="end"/> are collected automatically in an <tag name="TECHNIQUE" value="start"/>unsupervised<tag name="TECHNIQUE" value="end"/> fashion using a collection of trivia question and answer pairs as seeds .
These <tag name="TECHNIQUE" value="start"/>patterns<tag name="TECHNIQUE" value="end"/> are used to generate features for a <tag name="TECHNIQUE" value="start"/>statistical<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>question answering<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> system .
We report our results on the TREC-10 question set .


##W97-0312
<tag name="TECHNIQUE" value="start"/>Learning<tag name="TECHNIQUE" value="end"/> To <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Tag Multilingual Texts<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Through <tag name="TECHNIQUE" value="start"/>Observation<tag name="TECHNIQUE" value="end"/> .
This paper describes <tag name="FOCUS" value="start"/>RoboTag<tag name="FOCUS" value="end"/> , an advanced prototype for a <tag name="TECHNIQUE" value="start"/>machine learningbased<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>multilingual information extraction<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> system .
First , we describe a general client\/server architecture used in <tag name="TECHNIQUE" value="start"/>learning from observation<tag name="TECHNIQUE" value="end"/> .
Then we give a detailed description of our novel <tag name="TECHNIQUE" value="start"/>decision-tree<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>tagging<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> approach .
<tag name="FOCUS" value="start"/>RoboTag<tag name="FOCUS" value="end"/> performance for the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>proper noun tagging<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> task in English and Japanese is compared against humantagged keys and to the best hand-coded pattern performance -LRB- as reported in the MUC and MET evaluation results -RRB- .
Related work and future directions are presented .


##N04-1017
<tag name="TECHNIQUE" value="start"/>Lattice-Based Search<tag name="TECHNIQUE" value="end"/> For <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Spoken Utterance Retrieval<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
Recent work on <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>spoken document retrieval<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> has suggested that it is adequate to take the singlebest output of ASR , and perform text retrieval on this output .
This is reasonable enough for the task of retrieving broadcast news stories , where word error rates are relatively low , and the stories are long enough to contain much redundancy .
But it is patently not reasonable if one 's task is to retrieve a short snippet of speech in a domain where WER 's can be as high as 50 % ; such would be the situation with teleconference speech , where one 's task is to find if and when a participant uttered a certain phrase .
In this paper we propose an <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>indexing<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> procedure for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>spoken utterance retrieval<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> that works on <tag name="TECHNIQUE" value="start"/>lattices<tag name="TECHNIQUE" value="end"/> rather than just single-best text .
We demonstrate that this procedure can improve F scores by over five points compared to singlebest retrieval on tasks with poor WER and low redundancy .
The representation is flexible so that we can represent both <tag name="TECHNIQUE" value="start"/>word lattices<tag name="TECHNIQUE" value="end"/> , as well as <tag name="TECHNIQUE" value="start"/>phone lattices<tag name="TECHNIQUE" value="end"/> , the latter being important for improving performance when searching for phrases containing OOV words .


##W03-1308
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Bio-Medical Entity Extraction<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> Using <tag name="TECHNIQUE" value="start"/>Support Vector Machines<tag name="TECHNIQUE" value="end"/> .
<tag name="TECHNIQUE" value="start"/>Support Vector Machines<tag name="TECHNIQUE" value="end"/> have achieved state of the art performance in several classification tasks .
In this article we apply them to the identification and semantic annotation of scientific and technical terminology in the domain of molecular biology .
This illustrates the extensibility of the traditional <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>named entity<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> task to special domains with extensive terminologies such as those in medicine and related disciplines .
We illustrate <tag name="TECHNIQUE" value="start"/>SVM<tag name="TECHNIQUE" value="end"/> 's capabilities using a sample of 100 journal abstracts texts taken from the fhuman , blood cell , transcription factorg domain of MEDLINE .
Approximately 3400 terms are annotated and the model performs at about 74 % F-score on cross-validation tests .
A detailed analysis based on empirical evidence shows the contribution of various feature sets to performance .


##P07-1119
<tag name="TECHNIQUE" value="start"/>Substring-Based<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Transliteration<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Transliteration<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> is the task of converting a word from one alphabetic script to another .
We present a novel , <tag name="TECHNIQUE" value="start"/>substring-based<tag name="TECHNIQUE" value="end"/> approach to <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>transliteration<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , inspired by phrasebased models of machine translation .
We investigate two implementations of <tag name="TECHNIQUE" value="start"/>substringbased<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>transliteration<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> : a <tag name="TECHNIQUE" value="start"/>dynamic programming<tag name="TECHNIQUE" value="end"/> algorithm , and a <tag name="TECHNIQUE" value="start"/>finite-state transducer<tag name="TECHNIQUE" value="end"/> .
We show that our <tag name="TECHNIQUE" value="start"/>substring-based transducer<tag name="TECHNIQUE" value="end"/> not only outperforms a state-of-the-art letterbased approach by a significant margin , but is also orders of magnitude faster .


##W09-2103
Inferring <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Tutorial Dialogue Structure<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> with <tag name="TECHNIQUE" value="start"/>Hidden Markov<tag name="TECHNIQUE" value="end"/> Modeling .
The field of intelligent <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>tutoring<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> systems has seen many successes in recent years .
A significant remaining challenge is the automatic creation of corpus-based <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>tutorial dialogue management<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> models .
This paper reports on early work toward this goal .
We identify <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>tutorial dialogue<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> modes in an <tag name="TECHNIQUE" value="start"/>unsupervised<tag name="TECHNIQUE" value="end"/> fashion using <tag name="TECHNIQUE" value="start"/>hidden Markov<tag name="TECHNIQUE" value="end"/> models -LRB- <tag name="TECHNIQUE" value="start"/>HMMs<tag name="TECHNIQUE" value="end"/> -RRB- trained on input sequences of manually-labeled dialogue acts and adjacency pairs .
The two best-fit <tag name="TECHNIQUE" value="start"/>HMMs<tag name="TECHNIQUE" value="end"/> are presented and compared with respect to the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>dialogue structure<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> they suggest ; we also discuss potential uses of the methodology for future work .


##W09-1106
Efficient Linearization of <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Tree Kernel Functions<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> .
The combination of Support Vector Machines with very high dimensional <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>kernels<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> , such as string or tree kernels , suffers from two major drawbacks : first , the implicit representation of feature spaces does not allow us to understand which features actually triggered the generalization ; second , the resulting computational burden may in some cases render unfeasible to use large data sets for training .
We propose an approach based on <tag name="TECHNIQUE" value="start"/>feature space<tag name="TECHNIQUE" value="end"/> reverse engineering to tackle both problems .
Our experiments with <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Tree Kernels<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> on a <tag name="DOMAIN" value="start"/>Semantic Role Labeling<tag name="DOMAIN" value="end"/> data set show that the proposed approach can drastically reduce the computational footprint while yielding almost unaffected accuracy .


##P08-1090
<tag name="TECHNIQUE" value="start"/>Unsupervised Learning<tag name="TECHNIQUE" value="end"/> of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Narrative Event Chains<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
Hand-coded scripts were used in the 1970-80s as knowledge backbones that enabled inference and other NLP tasks requiring deep semantic knowledge .
We propose <tag name="TECHNIQUE" value="start"/>unsupervised<tag name="TECHNIQUE" value="end"/> induction of similar schemata called  <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>narrative event chains<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> from raw newswire text .
A <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>narrative event chain<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> is a partially ordered set of events related by a common protagonist .
We describe a three step process to learning <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>narrative event chains<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
The first uses<tag name="TECHNIQUE" value="start"/> unsupervised distributional<tag name="TECHNIQUE" value="end"/> methods to learn <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>narrative relations<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> between events sharing coreferring arguments .
The second applies a <tag name="TECHNIQUE" value="start"/>temporal classifier<tag name="TECHNIQUE" value="end"/> to partially order the connected events .
Finally , the third prunes and <tag name="TECHNIQUE" value="start"/>clusters<tag name="TECHNIQUE" value="end"/> self-contained chains from the space of events .
We introduce two evaluations : the narrative cloze to evaluate event relatedness , and an order coherence task to evaluate narrative order .
We show a 36 % improvement over baseline for narrative prediction and 25 % for temporal coherence .


##W02-1106
<tag name="TECHNIQUE" value="start"/>Translating Lexical Semantic Relations<tag name="TECHNIQUE" value="end"/> : The First Step Towards <tag name="DOMAIN" value="start"/>Multilingual Wordnets<tag name="DOMAIN" value="end"/> .
Establishing correspondences between <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>wordnets<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> of different languages is essential to both multilingual knowledge processing and for bootstrapping wordnets of low-density languages .
We claim that such correspondences must be based on <tag name="TECHNIQUE" value="start"/>lexical semantic relations<tag name="TECHNIQUE" value="end"/> , rather than top ontology or word translations .
In particular , we define a translation equivalence relation as a bilingual <tag name="TECHNIQUE" value="start"/>lexical semantic relation<tag name="TECHNIQUE" value="end"/> .
Such relations can then be part of a <tag name="TECHNIQUE" value="start"/>logical entailment<tag name="TECHNIQUE" value="end"/> predicting whether source language semantic relations will hold in a target language or not .
Our claim is tested with a study of 210 Chinese lexical lemmas and their possible semantic relations links bootstrapped from the Princeton WordNet .
The results show that <tag name="TECHNIQUE" value="start"/>lexical semantic relation translations<tag name="TECHNIQUE" value="end"/> are indeed highly precise when they are logically inferable .


##A00-3005
Corpus-Based <tag name="DOMAIN" value="start"/>Syntactic Error Detection<tag name="DOMAIN" value="end"/> Using <tag name="TECHNIQUE" value="start"/>Syntactic Patterns<tag name="TECHNIQUE" value="end"/> .
This paper presents a <tag name="TECHNIQUE" value="start"/>parsing<tag name="TECHNIQUE" value="end"/> system for the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>detection of syntactic errors<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
It combines a robust <tag name="TECHNIQUE" value="start"/>partial parser<tag name="TECHNIQUE" value="end"/> which obtains the main sentence components and a <tag name="TECHNIQUE" value="start"/>finite-state parser<tag name="TECHNIQUE" value="end"/> used for the description of <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>syntactic error<tag name="DOMAIN" value="end"/><tag name="TECHNIQUE" value="start"/> patterns<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> .
The system has been tested on a corpus of real texts , containing both correct and incorrect sentences , with promising results .


##W09-2206
<tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Latent Dirichlet Allocation<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> with <tag name="TECHNIQUE" value="start"/>Topic-in-Set Knowledge<tag name="TECHNIQUE" value="end"/> .
<tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Latent Dirichlet Allocation<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> is an <tag name="TECHNIQUE" value="start"/>unsupervised graphical<tag name="TECHNIQUE" value="end"/> model which can discover <tag name="DOMAIN" value="start"/>latent topics in unlabeled data<tag name="DOMAIN" value="end"/> .
We propose a mechanism for adding <tag name="TECHNIQUE" value="start"/>partial supervision<tag name="TECHNIQUE" value="end"/> , called <tag name="TECHNIQUE" value="start"/>topic-in-set knowledge<tag name="TECHNIQUE" value="end"/> , to <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>latent topic modeling<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> .
This type of supervision can be used to encourage the recovery of topics which are more relevant to user modeling goals than the topics which would be recovered otherwise .
Preliminary experiments on text datasets are presented to demonstrate the potential effectiveness of this method .


##D08-1073
<tag name="TECHNIQUE" value="start"/>Jointly Combining Implicit Constraints<tag name="TECHNIQUE" value="end"/> Improves <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Temporal Ordering<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
Previous work on <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>ordering events<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> in text has typically focused on local pairwise decisions , ignoring globally inconsistent labels .
However , temporal ordering is the type of domain in which global constraints should be relatively easy to represent and reason over .
This paper presents a framework that informs local decisions with two types of implicit <tag name="TECHNIQUE" value="start"/>global constraints<tag name="TECHNIQUE" value="end"/> : <tag name="TECHNIQUE" value="start"/>transitivity<tag name="TECHNIQUE" value="end"/> -LRB- A before B and B beforeC implies AbeforeC -RRB- and <tag name="TECHNIQUE" value="start"/>time expression normalization<tag name="TECHNIQUE" value="end"/> -LRB- e.g. last month is before yesterday -RRB- .
We show how these constraints can be used to create a more densely-connected network of events , and how global consistency can be enforced by incorporating these constraints into an <tag name="TECHNIQUE" value="start"/>integer linear programming<tag name="TECHNIQUE" value="end"/> framework .
We present results on two <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>event ordering<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> tasks , showing a 3.6 % absolute increase in the accuracy of before\/after classification over a pairwise model .


##W06-2607
<tag name="TECHNIQUE" value="start"/>Tree Kernel<tag name="TECHNIQUE" value="end"/> Engineering In <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Semantic Role Labeling<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Systems .
Recent work on the design of automatic systems for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>semantic role labeling<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> has shown that feature engineering is a complex task from a modeling and implementation point of view .
<tag name="TECHNIQUE" value="start"/>Tree kernels<tag name="TECHNIQUE" value="end"/> alleviate suchcomplexityaskernelfunctionsgenerate features automatically and require less software development for data extraction .
In this paper , we study several <tag name="TECHNIQUE" value="start"/>tree kernel<tag name="TECHNIQUE" value="end"/> approaches for both <tag name="DOMAIN" value="start"/>boundary detection<tag name="DOMAIN" value="end"/> and <tag name="DOMAIN" value="start"/>argument classification<tag name="DOMAIN" value="end"/> .
The comparative experiments on <tag name="TECHNIQUE" value="start"/>Support Vector Machines<tag name="TECHNIQUE" value="end"/> with such <tag name="TECHNIQUE" value="start"/>kernels<tag name="TECHNIQUE" value="end"/> on the CoNLL 2005 dataset show that very simple tree manipulations trigger automatic feature engineering that highly improves accuracy and efficiency in both phases .
Moreover , the use of different classifiers for internal andpre-terminalnodesmaintainsthesame accuracy and highly improves efficiency .


##W04-1119
A <tag name="TECHNIQUE" value="start"/>Semi-Supervised<tag name="TECHNIQUE" value="end"/> Approach To Build Annotated Corpus For Chinese  <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Named Entity Recognition<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
1 This paper presents a <tag name="TECHNIQUE" value="start"/>semi-supervised<tag name="TECHNIQUE" value="end"/> approach to reduce human effort in building an annotated Chinese corpus .
One of the disadvantages of many statistical Chinese <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>named entity recognition<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> systems is that training data may be in short supply , and manually building annotated corpus is expensive .
In the proposed approach , we construct an 80M handannotated corpus in three steps : -LRB- 1 -RRB- Automatically annotate training corpus ; -LRB- 2 -RRB- Manually refine small subsets of the automatically annotated corpus ; -LRB- 3 -RRB- Combine small subsets and whole corpus in a <tag name="TECHNIQUE" value="start"/>bootstrapping<tag name="TECHNIQUE" value="end"/> process .
Our approach is tested on a state-ofthe-art Chinese <tag name="DOMAIN" value="start"/>word segmentation<tag name="DOMAIN" value="end"/> system -LRB- Gao et al. , 2003 , 2004 -RRB- .
Experiments show that only a small subset of hand-annotated corpus is sufficient to achieve a satisfying performance of the named entity component in this system .


##I08-5014
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Named Entity Recognition<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> for Indian Languages  .
Abstract Stub This paper talks about a new approach to recognize <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>named entities<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> for Indian languages .
<tag name="TECHNIQUE" value="start"/>Phonetic matching<tag name="TECHNIQUE" value="end"/> technique is used to match the strings of different languages on the basis of their similar sounding property .
We have tested our system with a comparable corpus of English and Hindi language data .
This approach is language independent and requires only a set of <tag name="TECHNIQUE" value="start"/>rules<tag name="TECHNIQUE" value="end"/> appropriate for a language .


##C88-2118
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Parsing Noisy Sentences<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
This paper describes a method to <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>parse and understand a `` noisy '' sentence<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> that possibly includes errors caused by a speech recognition device .
Our parser is connected to a speech recognition device which takes a continuously spoken sentence in Japanese and produces a sequence of phonemes .
The output sequence of phonemes can quite possibly include errors : altered phonemes , extra phonemes and missing phonemes .
The task is to <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>parse the noisy phoneme sequence<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> and understand the meaning of the original input sentence , given an augmented context-free grammar whose terminal symbols are phonemes .
A very efficient <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> method is required , as the task 's search space is much larger than that of parsing un-noisy sentences .
We adopt the <tag name="TECHNIQUE" value="start"/>generalized LR<tag name="TECHNIQUE" value="end"/><tag name="DOMAIN" value="start"/> <tag name="FOCUS" value="start"/>parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> algorithm , and a certain scoring scheme to select the most likely sentence o ~ t of multiple sentence candidates .
The use of a <tag name="TECHNIQUE" value="start"/>confusion matrix<tag name="TECHNIQUE" value="end"/> , which is created in advance by analyzing a large set of input\/output pairs , is discussed to improve the scoring accuracy .
The system has been integrated into CMU 's knowledge-based <tag name="DOMAIN" value="start"/>machine translation<tag name="DOMAIN" value="end"/> system .


##C08-1110
A Framework for Identifying <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Textual Redundancy<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
The task of identifying <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>redundant<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> information in documents that are generated from multiple sources provides a significant challenge for summarization and QA systems .
Traditional clustering techniques detect redundancy at the sentential level and do not guarantee the preservation of all information within the document .
We discuss an algorithm that generates a novel <tag name="TECHNIQUE" value="start"/>graph-based<tag name="TECHNIQUE" value="end"/> representation for a document and then utilizes a <tag name="TECHNIQUE" value="start"/>set cover approximation<tag name="TECHNIQUE" value="end"/> algorithm to remove <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>redundant<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> text from it .
Our experiments show that this approach offers a significant performance advantage over clustering when evaluated over an annotated dataset .


##J87-3003
Tools And Methods For <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Computational Lexicology<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
This paper presents a set of tools and methods for acquiring , manipulating , and analyzing <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>machinereadable dictionaries<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We give several detailed examples of the use of these tools and methods for particular analyses .
A novel aspect of our work is that it allows the combined processing of multiple <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>machine-readable dictionaries<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
Our examples describe analyses of data from Webster 's Seventh Collegiate Dictionary , the Longman Dictionary of Contemporary English , the Collins bilingual dictionaries , the Collins Thesaurus , and the Zingarelli Italian dictionary .
We describe existing facilities and results they have produced as well as planned enhancements to those facilities , particularly in the area of managing associations involving the <tag name="TECHNIQUE" value="start"/>senses of polysemous words<tag name="TECHNIQUE" value="end"/> .
We show how these enhancements expand the ways in which we can exploit <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>machine-readable dictionaries<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> in the construction of large <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>lexicons<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> for natural language processing systems .


##C96-2213
Using A <tag name="TECHNIQUE" value="start"/>Hybrid<tag name="TECHNIQUE" value="end"/> System Of <tag name="TECHNIQUE" value="start"/>Corpus - And Knowledge-Based<tag name="TECHNIQUE" value="end"/> Techniques To Automate The Induction Of A <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Lexical Sublanguage Grammar<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
Porting a Natural Language Processing -LRB- NLP -RRB- system to a new donmin renmins one of the bottlenecks in syntactic parsing , because of the amount of effort required to fix gaps in the lexicon , and to attune the existing grammar to the idiosyncracics of the new sublanguage .
This paper shows how thc process of fitting a <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>lexicalizcd grammar<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> to a domain can be automated to a great extent by using a <tag name="TECHNIQUE" value="start"/>hybrid<tag name="TECHNIQUE" value="end"/> system that combines traditimml knowledgebased techniques with a corpus-based approach .


##P09-3011
<tag name="TECHNIQUE" value="start"/>Clustering<tag name="TECHNIQUE" value="end"/> Technique in <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Multi-Document Personal Name Disambiguation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
Focusing on <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>multi-document personal name disambiguation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> , this paper develops an <tag name="TECHNIQUE" value="start"/>agglomerative clustering<tag name="TECHNIQUE" value="end"/> approach to resolving this problem .
We start from an analysis of <tag name="TECHNIQUE" value="start"/>pointwise mutual information<tag name="TECHNIQUE" value="end"/> between feature and the ambiguous name , which brings about a novel weight computing method for feature in <tag name="TECHNIQUE" value="start"/>clustering<tag name="TECHNIQUE" value="end"/> .
Then a trade-off measure between within-cluster compactness and among-cluster separation is proposed for stopping <tag name="TECHNIQUE" value="start"/>clustering<tag name="TECHNIQUE" value="end"/> .
After that , we apply a <tag name="TECHNIQUE" value="start"/>labeling<tag name="TECHNIQUE" value="end"/> method to find representative feature for each cluster .
Finally , experiments are conducted on word-based clustering in Chinese dataset and the result shows a good effect .


##D07-1095
Inducing <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Search Keys for Name Filtering<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
This paper describes <tag name="TECHNIQUE" value="start"/>ETK<tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/>Ensemble of Transformation-based Keys<tag name="TECHNIQUE" value="end"/> -RRB- a new algorithm for inducing <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>search keys for name filtering<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
<tag name="TECHNIQUE" value="start"/>ETK<tag name="TECHNIQUE" value="end"/> has the low computational cost and ability to <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>filter<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> by <tag name="TECHNIQUE" value="start"/>phonetic similarity<tag name="TECHNIQUE" value="end"/> characteristic of phonetic keys such as Soundex , but is adaptable to alternative similarity models .
The accuracy of <tag name="TECHNIQUE" value="start"/>ETK<tag name="TECHNIQUE" value="end"/> in a preliminary empirical evaluation suggests that it is well-suited for phonetic <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>filtering<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> applications such as recognizing alternative <tag name="DOMAIN" value="start"/>cross-lingual transliterations<tag name="DOMAIN" value="end"/> .


##I08-2138
A Punjabi <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Grammar Checker<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
This article provides description about the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>grammar checking<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> software developed for detecting the  <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>grammatical errors<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> in Punjabi texts and providing suggestions wherever appropriate to rectify those errors .
This system utilizes a full-form  <tag name="TECHNIQUE" value="start"/>lexicon<tag name="TECHNIQUE" value="end"/> for <tag name="TECHNIQUE" value="start"/>morphology<tag name="TECHNIQUE" value="end"/> analysis and <tag name="TECHNIQUE" value="start"/>rule-based<tag name="TECHNIQUE" value="end"/> systems for <tag name="TECHNIQUE" value="start"/>part of speech tagging<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>phrase chunking<tag name="TECHNIQUE" value="end"/> .
The system supported by a set of carefully devised <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>error detection<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> <tag name="TECHNIQUE" value="start"/>rules<tag name="TECHNIQUE" value="end"/> can detect and suggest rectifications for a number of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>grammatical errors<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , resulting from lack of agreement , order of words in various phrases etc. , in literary style Punjabi texts .


##P06-2108
Using <tag name="TECHNIQUE" value="start"/>Word Support<tag name="TECHNIQUE" value="end"/> Model To Improve Chinese <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Input<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> System .
This paper presents a <tag name="TECHNIQUE" value="start"/>word support<tag name="TECHNIQUE" value="end"/> model -LRB- <tag name="TECHNIQUE" value="start"/>WSM<tag name="TECHNIQUE" value="end"/> -RRB- .
The <tag name="TECHNIQUE" value="start"/>WSM<tag name="TECHNIQUE" value="end"/> can effectively perform <tag name="TECHNIQUE" value="start"/>homophone<tag name="TECHNIQUE" value="end"/> selection and <tag name="TECHNIQUE" value="start"/>syllable-word segmentation<tag name="TECHNIQUE" value="end"/> to improve Chinese <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>input<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> systems .
The experimental results show that : -LRB- 1 -RRB- the <tag name="TECHNIQUE" value="start"/>WSM<tag name="TECHNIQUE" value="end"/> is able to achieve tonal -LRB- <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>syllables input<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> with four tones -RRB- and toneless -LRB- <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>syllables input<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> without four tones -RRB- syllable-to-word -LRB- STW -RRB- accuracies of 99 % and 92 % , respectively , among the converted words ; and -LRB- 2 -RRB- while applying the <tag name="TECHNIQUE" value="start"/>WSM<tag name="TECHNIQUE" value="end"/> as an adaptation processing , together with the Microsoft Input Method Editor 2003 -LRB- MSIME -RRB- and an optimized <tag name="TECHNIQUE" value="start"/>bigram<tag name="TECHNIQUE" value="end"/> model , the average tonal and toneless STW improvements are 37 % and 35 % , respectively .


##W09-2201
Coupling <tag name="TECHNIQUE" value="start"/>Semi-Supervised Learning<tag name="TECHNIQUE" value="end"/> of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Categories and Relations<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We consider <tag name="TECHNIQUE" value="start"/>semi-supervised learning<tag name="TECHNIQUE" value="end"/> of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>information extraction<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> methods , especially for extracting instances of noun categories -LRB- e.g. , ` athlete , ' ` team ' -RRB- and relations -LRB- e.g. , ` playsForTeam -LRB- athlete , team -RRB- ' -RRB- .
<tag name="TECHNIQUE" value="start"/>Semisupervised<tag name="TECHNIQUE" value="end"/> approaches using a small number of labeled examples together with many unlabeled examples are often unreliable as they frequently produce an internally consistent , but nevertheless incorrect set of extractions .
We propose that this problem can be overcome by <tag name="TECHNIQUE" value="start"/>simultaneously learning classifiers<tag name="TECHNIQUE" value="end"/> for many different <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>categories and relations<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> in the presence of an <tag name="TECHNIQUE" value="start"/>ontology<tag name="TECHNIQUE" value="end"/> defining <tag name="TECHNIQUE" value="start"/>constraints<tag name="TECHNIQUE" value="end"/> that couple the training of these <tag name="TECHNIQUE" value="start"/>classifiers<tag name="TECHNIQUE" value="end"/> .
Experimental results show that <tag name="TECHNIQUE" value="start"/>simultaneously learning<tag name="TECHNIQUE" value="end"/> a coupled collection of <tag name="TECHNIQUE" value="start"/>classifiers<tag name="TECHNIQUE" value="end"/> for 30 <tag name="DOMAIN" value="start"/>categories and relations<tag name="DOMAIN" value="end"/> results in much more accurate extractions than training classifiers individually .


##W93-0110
Acquiring <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Predicate-Argument Mapping<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Information From Multilingual Texts .
This paper discusses automatic acquisition of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>predicate-argument mapping<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> information from multilingual texts .
The <tag name="TECHNIQUE" value="start"/>lexicon<tag name="TECHNIQUE" value="end"/> of our NLP system abstracts the language-dependent portion of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>predicate-argument mapping<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> information from the core meaning of verb senses -LRB- i.e. semantic concepts as defined in the knowledge base -RRB- .
We represent this mapping information in terms of cross-linguistically generalized mapping types called <tag name="TECHNIQUE" value="start"/>situation types<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>word sense-specific idiosyncrasies<tag name="TECHNIQUE" value="end"/> .
This representation has enabled us to automatically acquire <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>predicate-argument mapping<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> information , specifically <tag name="TECHNIQUE" value="start"/>situation types<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>idiosyncrasies<tag name="TECHNIQUE" value="end"/> , for verbs in English , Spanish , and Japanese texts .


##W04-3007
Robustness Issues In A <tag name="TECHNIQUE" value="start"/>Data-Driven<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Spoken Language Understanding<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> System .
Robustness is a key requirement in <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>spoken language understanding<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> -LRB- <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>SLU<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> -RRB- systems .
Human speech is often ungrammatical and ill-formed , and there will frequently be a mismatch between training and test data .
This paper discusses robustness and adaptation issues in a <tag name="TECHNIQUE" value="start"/>statistically-based<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>SLU<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> system which is entirely <tag name="TECHNIQUE" value="start"/>data-driven<tag name="TECHNIQUE" value="end"/> .
To test robustness , the system has been tested on data from the Air Travel Information Service -LRB- ATIS -RRB- domain which has been artificially corrupted with varying levels of additive noise .
Although the speech recognition performance degraded steadily , the system did not fail catastrophically .
Indeed , the rate at which the end-to-end performance of the complete system degraded was significantly slower than that of the actual recognition component .
In a second set of experiments , the ability to rapidly adapt the core understanding component of the system to a different application within the same broad domain has been tested .
Using only a small amount of training data , experiments have shown that a semantic parser based on the <tag name="TECHNIQUE" value="start"/>Hidden Vector State<tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/>HVS<tag name="TECHNIQUE" value="end"/> -RRB- model originally trained on the ATIS corpus can be straightforwardly adapted to the somewhat different DARPA Communicator task using standard <tag name="TECHNIQUE" value="start"/>adaptation<tag name="TECHNIQUE" value="end"/> algorithms .
The paper concludes by suggesting that the results presented provide initial support to the claim that an <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>SLU<tag name="FOCUS" value="end"/> <tag name="DOMAIN" value="end"/>system which is <tag name="TECHNIQUE" value="start"/>statistically-based<tag name="TECHNIQUE" value="end"/> and trained entirely from data is intrinsically robust and can be readily adapted to new applications .


##I05-1013
Automatic <tag name="DOMAIN" value="start"/>Partial Parsing Rule<tag name="DOMAIN" value="end"/> Acquisition Using <tag name="TECHNIQUE" value="start"/>Decision Tree<tag name="TECHNIQUE" value="end"/> Induction .
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Partial parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> techniques try to recover syntactic information eï¬ciently and reliably by sacrificing completeness and depth of analysis .
One of the diï¬culties of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>partial parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> is finding a means to extract the grammar involved automatically .
In this paper , we present a method for automatically extracting <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>partial parsing rules<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> from a tree-annotated corpus using <tag name="TECHNIQUE" value="start"/>decision tree<tag name="TECHNIQUE" value="end"/> induction .
We define the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>partial parsing rules<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> as those that can decide the structure of a substring in an input sentence deterministically .
This decision can be considered as a <tag name="TECHNIQUE" value="start"/>classification<tag name="TECHNIQUE" value="end"/> ; as such , for a substring in an input sentence , a proper structure is chosen among the structures occurred in the corpus .
For the classification , we use <tag name="TECHNIQUE" value="start"/>decision tree<tag name="TECHNIQUE" value="end"/> induction , and induce <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>partial parsing rules<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> from the <tag name="TECHNIQUE" value="start"/>decision tree<tag name="TECHNIQUE" value="end"/> .
The acquired grammar is similar to a phrase structure grammar , with contextual and lexical information , but it allows building structures of depth one or more .
Our experiments showed that the proposed <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>partial parser<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> using the automatically extracted rules is not only accurate and eï¬cient , but also achieves reasonable coverage for Korean .


##W97-0811
An Experiment In <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Semantic Tagging<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Using <tag name="TECHNIQUE" value="start"/>Hidden Markov Model<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Tagging<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
The same word can have many different meanings depending on the context in which it is used .
Discovering the meaning of a word , given the text around it , has been an interesting problem for both the psychology and the artificial intelligence research communities .
In this article , we present a series of experiments , using methods which have proven to be useful for eliminating part-of-speech ambiguity , to see if such simple methods can be used to resolve <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>semantic ambiguities<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
Using a publicly available semantic <tag name="TECHNIQUE" value="start"/>lexicon<tag name="TECHNIQUE" value="end"/> , we find the <tag name="TECHNIQUE" value="start"/>Hidden Markov<tag name="TECHNIQUE" value="end"/> Models work surprising well at choosing the right <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>semantic categories<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , once the sentence has been stripped of purely functional words .


##E06-1002
Using <tag name="TECHNIQUE" value="start"/>Encyclopedic<tag name="TECHNIQUE" value="end"/> Knowledge For <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Named Entity<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Disambiguation .
We present a new method for detecting and disambiguating  <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>named entities<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> in open domain text .
A disambiguation <tag name="TECHNIQUE" value="start"/>SVM kernel<tag name="TECHNIQUE" value="end"/> is trained to exploit the high coverage and rich structure of the knowledge encoded in an online <tag name="TECHNIQUE" value="start"/>encyclopedia<tag name="TECHNIQUE" value="end"/> .
The resulting model significantly outperforms a less informed baseline .


##N04-3006
Open Text <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Semantic Parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Using <tag name="TECHNIQUE" value="start"/>FrameNet<tag name="TECHNIQUE" value="end"/> And <tag name="TECHNIQUE" value="start"/>WordNet<tag name="TECHNIQUE" value="end"/> .
This paper describes a <tag name="TECHNIQUE" value="start"/>rule-based<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>semantic parser<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> that relies on a <tag name="TECHNIQUE" value="start"/>frame<tag name="TECHNIQUE" value="end"/> dataset -LRB- <tag name="TECHNIQUE" value="start"/>FrameNet<tag name="TECHNIQUE" value="end"/> -RRB- , and a <tag name="TECHNIQUE" value="start"/>semantic network<tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/>WordNet<tag name="TECHNIQUE" value="end"/> -RRB- , to identify semantic relations between words in open text , as well as shallow semantic features associated with concepts in the text .
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Parsing semantic structures<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> allows semantic units and constituents to be accessed and processed in a more meaningful way than syntactic parsing , moving the automation of understanding natural language text to a higher level .


##P06-2107
<tag name="TECHNIQUE" value="start"/>Statistical Phrase-Based<tag name="TECHNIQUE" value="end"/> Models For Interactive <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Computer-Assisted Translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
Obtaining high-quality machine translations is still a long way off .
A postediting phase is required to improve the output of a machine translation system .
An alternative is the so called <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>computerassisted translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
In this framework , a human translator interacts with the system in order to obtain high-quality translations .
A <tag name="TECHNIQUE" value="start"/>statistical phrase-based<tag name="TECHNIQUE" value="end"/> approach to <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>computer-assisted translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> is described in this article .
A new decoder algorithm for interactive search is also presented , that combines monotone and nonmonotone search .
The system has been assessed in the TransType-2 project for the translation of several printer manuals , from -LRB- to -RRB- English to -LRB- from -RRB- Spanish , German and French .


##P98-1068
Japanese <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Morphological Analyzer<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> using <tag name="TECHNIQUE" value="start"/>Word Co-occurrence<tag name="TECHNIQUE" value="end"/> - JTAG .
We developed a Japanese <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>morphological analyzer<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> that uses the <tag name="TECHNIQUE" value="start"/>co-occurrence of words<tag name="TECHNIQUE" value="end"/> to select the correct sequence of words in an unsegmented Japanese sentence .
The <tag name="TECHNIQUE" value="start"/>co-occurrence<tag name="TECHNIQUE" value="end"/> information can be obtained from cases where the system incorrectly analyzes sentences .
As the amount of information increases , the accuracy of the system increases with a small risk of degradation .
Experimental results show that the proposed system assigns the correct phonological representations to unsegmented Japanese sentences more precisely than do other popular systems .


##D07-1082
<tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Active Learning<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Word Sense Disambiguation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> with Methods for Addressing the Class Imbalance Problem .
In this paper , we analyze the effect of <tag name="TECHNIQUE" value="start"/>resampling<tag name="TECHNIQUE" value="end"/> techniques , including undersampling and over-sampling used in <tag name="TECHNIQUE" value="start"/> active learning<tag name="TECHNIQUE" value="end"/> for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>word sense disambiguation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> -LRB- <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>WSD<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> -RRB- .
Experimental results show that under-sampling causes negative effects on active learning , but over-sampling is a relatively good choice .
To alleviate the withinclass imbalance problem of over-sampling , we propose a <tag name="TECHNIQUE" value="start"/>bootstrap-based oversampling<tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/>BootOS<tag name="TECHNIQUE" value="end"/> -RRB- method that works better than ordinary over-sampling in <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>active learning<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>WSD<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
Finally , we investigate when to stop <tag name="TECHNIQUE" value="start"/>active learning<tag name="TECHNIQUE" value="end"/> , and adopt two strategies , max-confidence and min-error , as stopping conditions for <tag name="TECHNIQUE" value="start"/>active learning<tag name="TECHNIQUE" value="end"/> .
According to experimental results , we suggest a prediction solution by considering max-confidence as the upper bound and min-error as the lower bound for stopping conditions .


##P98-2138
Combining <tag name="TECHNIQUE" value="start"/>Trigram<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>Winnow<tag name="TECHNIQUE" value="end"/> in Thai <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>OCR Error Correction<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
For languages that have no explicit word boundary such as Thai , Chinese and Japanese , correcting words in text is harder than in English because of additional ambiguities in locating error words .
The traditional method handles this by hypothesizing that every substrings in the input sentence could be error words and trying to correct all of them .
In this paper , we propose the idea of reducing the scope of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>spelling correction<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> by focusing only on dubious areas in the input sentence .
Boundaries of these dubious areas could be obtained approximately by applying <tag name="TECHNIQUE" value="start"/>word segmentation<tag name="TECHNIQUE" value="end"/> algorithm and finding word sequences with low probability .
To generate the candidate correction words , we used a modified <tag name="TECHNIQUE" value="start"/>edit distance<tag name="TECHNIQUE" value="end"/> which reflects the characteristic of Thai <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>OCR errors<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
Finally , a <tag name="TECHNIQUE" value="start"/>part-ofspeech trigram<tag name="TECHNIQUE" value="end"/> model and <tag name="TECHNIQUE" value="start"/>Winnow<tag name="TECHNIQUE" value="end"/> algorithm are combined to determine the most probable correction .


##E06-1049
A <tag name="TECHNIQUE" value="start"/>Machine Learning<tag name="TECHNIQUE" value="end"/> Approach To Extract <tag name="DOMAIN" value="start"/> <tag name="FOCUS" value="start"/>Temporal Information<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> From Texts In Swedish And Generate Animated 3D Scenes .
Carsim is a program that automatically converts narratives into 3D scenes .
Carsim considers authentic texts describing road accidents , generally collected from web sitesofSwedishnewspapers ortranscribed from hand-written accounts by victims of accidents .
One of the program 's key featuresisthatitanimatesthegenerated scene to visualize events .
To create a consistent animation , Carsim extracts the participants mentioned in a text and identifies what they do .
In this paper , we focus on the extraction of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>temporal relations<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> between actions .
We first describe how we detect <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>time expressions and events<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We then present a <tag name="TECHNIQUE" value="start"/>machine learning<tag name="TECHNIQUE" value="end"/> technique to <tag name="DOMAIN" value="start"/>order the sequence of events<tag name="DOMAIN" value="end"/> identified in the narratives .
We finally report the results we obtained .


##W00-0309
<tag name="TECHNIQUE" value="start"/>Task-Based<tag name="TECHNIQUE" value="end"/>  <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Dialog Management<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Using An <tag name="TECHNIQUE" value="start"/>Agenda<tag name="TECHNIQUE" value="end"/> .
Dialog man tigement addresses two specific problems : -LRB- 1 -RRB- providing a coherent overall structure to interaction that extends beyond the single turn , -LRB- 2 -RRB- correctly managing mixedinitiative interaction .
We propose a <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>dialog management<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> architecture based on the following elements : handlers that manage interaction focussed on tightly coupled sets of information , a product that reflects mutually agreed-upon information and an <tag name="TECHNIQUE" value="start"/>agenda<tag name="TECHNIQUE" value="end"/> that orders the topics relevant to task completion .


##W02-0713
Sharing Problems And Solutions For <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Machine Translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Of Spoken And Written Interaction .
Examples from chat interaction are presented to demonstrate that <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>machine translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> of written interaction shares many problems with translation of spoken interaction .
The potential for common solutions to the problems is illustrated by describing operations that <tag name="TECHNIQUE" value="start"/>normalize<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>tag<tag name="TECHNIQUE" value="end"/> input before <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
<tag name="TECHNIQUE" value="start"/>Segmenting utterances<tag name="TECHNIQUE" value="end"/> into small translation units and processing short turns separately are also motivated using data from chat .


##C96-2204
Constructing <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Verb Semantic Classes<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> For French : Methods And Evaluation .
In this paper , we study a reformulation , which is better adapted to NLP , of the alternation system developed for English by B. Levin .
We have studied a set of 1700 verbs from which we explain how <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>verb semantic classes<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> can be built in a systematic way .
The quality of the results w.r. t , semantic chLssifications such as WordNet is then evaluated .


##P02-1060
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Named Entity Recognition<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Using An <tag name="TECHNIQUE" value="start"/>HMM-Based Chunk Tagger<tag name="TECHNIQUE" value="end"/> .
This paper proposes a <tag name="TECHNIQUE" value="start"/>Hidden Markov Model<tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/>HMM<tag name="TECHNIQUE" value="end"/> -RRB- and an <tag name="TECHNIQUE" value="start"/>HMM-based chunk tagger<tag name="TECHNIQUE" value="end"/> , from which a <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>named entity -LRB- NE -RRB- recognition -LRB- NER -RRB- <tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> system is built to recognize and classify names , times and numerical quantities .
Through the <tag name="TECHNIQUE" value="start"/>HMM<tag name="TECHNIQUE" value="end"/> , our system is able to apply and integrate four types of internal and external evidences : 1 -RRB- simple deterministic internal feature of the words , such as <tag name="TECHNIQUE" value="start"/>capitalization<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>digitalization<tag name="TECHNIQUE" value="end"/> ; 2 -RRB- internal <tag name="TECHNIQUE" value="start"/>semantic<tag name="TECHNIQUE" value="end"/> feature of important triggers ; 3 -RRB- internal <tag name="TECHNIQUE" value="start"/>gazetteer<tag name="TECHNIQUE" value="end"/> feature ; 4 -RRB- external <tag name="TECHNIQUE" value="start"/>macro context<tag name="TECHNIQUE" value="end"/> feature .
In this way , the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>NER<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> problem can be resolved effectively .
Evaluation of our system on MUC-6 and MUC-7 English <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>NE<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> tasks achieves F-measures of 96.6 % and 94.1 % respectively .
It shows that the performance is significantly better than reported by any other machine-learning system .
Moreover , the performance is even consistently better than those based on handcrafted rules .


##W98-1415
<tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Clause Aggregation<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> Using <tag name="TECHNIQUE" value="start"/>Linguistic Knowledge<tag name="TECHNIQUE" value="end"/> .
By combining multiple clauses into one single sentence , a <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>text generation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> system can express the same amount of information in fewer words and at the same time , produce a great variety of complex constructions .
In this paper , we describe <tag name="TECHNIQUE" value="start"/>hypotactic and paratactic operators<tag name="TECHNIQUE" value="end"/> for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>generating complex sentences<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> from clause-sized semantic representations .
These two types of operators are portable and reusable because they are based on general resources such as the <tag name="TECHNIQUE" value="start"/>lexicon<tag name="TECHNIQUE" value="end"/> and the <tag name="TECHNIQUE" value="start"/>grammar<tag name="TECHNIQUE" value="end"/> .


##W08-0403
<tag name="TECHNIQUE" value="start"/>Prior Derivation<tag name="TECHNIQUE" value="end"/> Models For Formally <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Syntax-Based<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/> Translation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> Using <tag name="TECHNIQUE" value="start"/>Linguistically Syntactic Parsing<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>Tree Kernels<tag name="TECHNIQUE" value="end"/> .
This paper presents an improved formally <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>syntax-based<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>SMT<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> model , which is enriched by linguistically syntactic knowledge obtained from <tag name="TECHNIQUE" value="start"/>statistical constituent parsers<tag name="TECHNIQUE" value="end"/> .
We propose a linguistically-motivated <tag name="TECHNIQUE" value="start"/>prior derivation<tag name="TECHNIQUE" value="end"/> model to score hypothesis derivations on top of the baseline model during the translation decoding .
Moreover , we devise a fast <tag name="TECHNIQUE" value="start"/>training<tag name="TECHNIQUE" value="end"/> algorithm to achieve such improved models based on <tag name="TECHNIQUE" value="start"/>tree kernel<tag name="TECHNIQUE" value="end"/> methods .
Experiments on an English-to-Chinese task demonstrate that our proposed models outperformed the baseline formally syntaxbased models , while both of them achieved signi cant improvements over a state-of-theart phrase-based SMT system .


##W03-0416
An Efficient <tag name="TECHNIQUE" value="start"/>Clustering<tag name="TECHNIQUE" value="end"/> Algorithm For <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/> Class-Based<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>Language Models<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
This paper defines a general form for <tag name="FOCUS" value="start"/> <tag name="TECHNIQUE" value="start"/>classbased probabilistic<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>language models<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> and proposes an efficient algorithm for <tag name="TECHNIQUE" value="start"/>clustering<tag name="TECHNIQUE" value="end"/> based on this .
Our evaluation experiments revealed that our method decreased computation time drastically , while retaining accuracy .


##X98-1016
Transforming <tag name="TECHNIQUE" value="start"/>Examples Into Patterns<tag name="TECHNIQUE" value="end"/> For <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Information Extraction<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Information Extraction<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> -LRB- <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>IE<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> -RRB- systems today are commonly based on pattern matching .
The patterns are regular expressions stored in a customizable knowledge base .
Adapting an IE system to a new subject domain entails the construction of a new pattern base - a time-consuming and expensive task .
We describe a strategy for building <tag name="TECHNIQUE" value="start"/>patterns from examples<tag name="TECHNIQUE" value="end"/> .
To adapt the IE system to a new domain quickly , the user chooses a set of examples in a training text , and for each example gives the logical form entries which the example induces .
The system transforms these examples into patterns and then applies <tag name="TECHNIQUE" value="start"/>meta-rules<tag name="TECHNIQUE" value="end"/> to generalize these patterns .


##C08-2011
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Word Sense Disambiguation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> for All Words using <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Tree-Structured Conditional Random Fields<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> .
We propose a <tag name="TECHNIQUE" value="start"/>supervised<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>word sense disambiguation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> -LRB- <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>WSD<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> -RRB- method using <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>tree-structured conditional random fields<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/>TCRFs<tag name="TECHNIQUE" value="end"/> -RRB- .
By applying <tag name="TECHNIQUE" value="start"/>TCRFs<tag name="TECHNIQUE" value="end"/> to a sentence described as a dependency tree structure , we conduct <tag name="DOMAIN" value="start"/>WSD<tag name="DOMAIN" value="end"/> as a labeling problem on tree structures .
To incorporate dependencies between word senses , we introduce a set of features on tree edges , in combination with coarse-grained tagsets , and show that these contribute to an improvement in WSD accuracy .
We also show that the <tag name="TECHNIQUE" value="start"/>tree-structured<tag name="TECHNIQUE" value="end"/> model outperforms the linear-chain model .
Experiments on the SENSEVAL-3 data set show that our TCRF model performs comparably with state-of-the-art WSD systems .


##D09-1084
A <tag name="TECHNIQUE" value="start"/>Relational<tag name="TECHNIQUE" value="end"/> Model of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Semantic Similarity<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> between Words using Automatically Extracted <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Lexical Pattern Clusters from the Web<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> .
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Semantic similarity<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> is a central concept that extends across numerous fields such as artificial intelligence , natural language processing , cognitive science and psychology .
Accurate measurement of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>semantic similarity<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> between words is essential for various tasks such as , document clustering , information retrieval , and synonym extraction .
We propose a novel model of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>semantic similarity<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> using the <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>semantic relations<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> that exist among words .
Given two words , first , we represent the <tag name="TECHNIQUE" value="start"/>semantic relations<tag name="TECHNIQUE" value="end"/> that hold between those words using automatically extracted <tag name="TECHNIQUE" value="start"/>lexical pattern clusters<tag name="TECHNIQUE" value="end"/> .
Next , the semantic similarity between the two words is computed using a <tag name="TECHNIQUE" value="start"/>Mahalanobis distance measure<tag name="TECHNIQUE" value="end"/> .
We compare the proposed similarity measure against previously proposed <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>semantic similarity<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> measures on Miller-Charles benchmark dataset and WordSimilarity353 collection .
The proposed method outperforms all existing web-based <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>semantic similarity<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> measures , achieving a Pearson correlation coefficient of 0.867 on the Millet-Charles dataset .


##P08-1117
Extraction of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Entailed Semantic Relations<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Through <tag name="TECHNIQUE" value="start"/>Syntax-Based Comma Resolution<tag name="TECHNIQUE" value="end"/> .
This paper studies <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>textual inference<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> by investigating <tag name="TECHNIQUE" value="start"/>comma structures<tag name="TECHNIQUE" value="end"/> , which are highly frequent elements whose major role in the extraction of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>semantic relations<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> has not been hitherto recognized .
We introduce the problem of <tag name="TECHNIQUE" value="start"/>comma resolution<tag name="TECHNIQUE" value="end"/> , defined as understanding the role of commas and extracting the relations they imply .
We show the importance of the problem using examples from Textual Entailment tasks , and present A <tag name="TECHNIQUE" value="start"/>Sentence Transformation Rule Learner<tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/>ASTRL<tag name="TECHNIQUE" value="end"/> -RRB- , a <tag name="TECHNIQUE" value="start"/>machine learning<tag name="TECHNIQUE" value="end"/> algorithm that uses a <tag name="TECHNIQUE" value="start"/>syntactic analysis<tag name="TECHNIQUE" value="end"/> of the sentence to learn <tag name="TECHNIQUE" value="start"/>sentence transformation rules<tag name="TECHNIQUE" value="end"/> that can then be used to extract <tag name="DOMAIN" value="start"/>relations<tag name="DOMAIN" value="end"/> .
We have manually annotated a corpus identifying <tag name="TECHNIQUE" value="start"/>comma structures<tag name="TECHNIQUE" value="end"/> and relations they entail and experimented with both gold standard parses and parses created by a leading statistical parser , obtaining F-scores of 80.2 % and 70.4 % respectively .


##W98-0719
<tag name="DOMAIN" value="start"/>Lexical Discovery<tag name="DOMAIN" value="end"/> With An Enriched <tag name="TECHNIQUE" value="start"/>Semantic Network<tag name="TECHNIQUE" value="end"/> .
The study of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>lexical semantics<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> has produced a systematic analysis of binary relationships between content words that has greatly benefited lexical search tools and natural language processing algorithms .
We first introduce a database system called <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>FreeNet<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> that facilitates the description and exploration of finite binary relations .
We then describe the design and implementation of Lexical <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>FreeNet<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> , a <tag name="TECHNIQUE" value="start"/>semantic network<tag name="TECHNIQUE" value="end"/> that mixes <tag name="TECHNIQUE" value="start"/>WordNet-derived<tag name="TECHNIQUE" value="end"/> semantic relations with <tag name="TECHNIQUE" value="start"/>data-derived<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>phonetically-derived<tag name="TECHNIQUE" value="end"/> relations .
We discuss how <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Lexical FreeNet<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> has aided in <tag name="DOMAIN" value="start"/>lexical discovery<tag name="DOMAIN" value="end"/> , the pursuit of linguistic and factual knowledge by the computer-aided exploration of lexical relations .


##I08-1008
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Name Origin Recognition<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Using <tag name="TECHNIQUE" value="start"/>Maximum Entropy Model<tag name="TECHNIQUE" value="end"/> and Diverse Features .
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Name origin recognition<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> is to identify the source language of a personal or location name .
Some early work used either rulebased or statistical methods with single knowledge source .
In this paper , we cast the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>name origin recognition<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> as a <tag name="TECHNIQUE" value="start"/>multi-class classification<tag name="TECHNIQUE" value="end"/> problem and approach the problem using <tag name="TECHNIQUE" value="start"/>Maximum Entropy<tag name="TECHNIQUE" value="end"/> method .
In doing so , we investigate the use of different features , including <tag name="TECHNIQUE" value="start"/>phonetic rules<tag name="TECHNIQUE" value="end"/> , <tag name="TECHNIQUE" value="start"/>ngram statistics<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>character position<tag name="TECHNIQUE" value="end"/> information for name origin recognition .
Experiments on a publicly available personal name database show that the proposed approach achieves an overall accuracy of 98.44 % for names written in English and 98.10 % for names written in Chinese , which are significantly and consistently better than those in reported work .


##J86-2002
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Summarizing Natural Language Database Responses<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
In a human dialogue it is usually considered inappropriate if one conversant monopolizes the conversation .
Similarly it can be inappropriate for a natural language database interface to respond with a lengthy list of data .
A non-enumerative `` summary '' response is less verbose and often avoids misleading the user where an extensional response might .
In this paper we investigate the problem of generating such <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>discourse-oriented concise responses<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We present details of the design and implementation of a system that produces <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>summary responses<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> to queries of a relational data base .
The system employs a set of <tag name="TECHNIQUE" value="start"/>heuristics<tag name="TECHNIQUE" value="end"/> that work in conjunction with a <tag name="TECHNIQUE" value="start"/>knowledge base<tag name="TECHNIQUE" value="end"/> to discover underlying regularities that form the basis of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>summary responses<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
The system is largely domain-independent , and hence can be ported relatively easily from one data base to another .
It can handle a wide variety of situations requiring a <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>summary response<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> and can be readily extended .
It also has a number of shortcomings which are discussed thoroughly and which form the basis for a number of suggested research directions .


##P06-1008
<tag name="DOMAIN" value="start"/>Acceptability Prediction<tag name="DOMAIN" value="end"/> By Means Of <tag name="TECHNIQUE" value="start"/>Grammaticality Quantification<tag name="TECHNIQUE" value="end"/> .
We propose in this paper a method for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>quantifying sentence grammaticality<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
The approach based on <tag name="TECHNIQUE" value="start"/>Property Grammars<tag name="TECHNIQUE" value="end"/> , a <tag name="TECHNIQUE" value="start"/>constraint-based syntactic<tag name="TECHNIQUE" value="end"/> formalism , makes it possible to evaluate a <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>grammaticality index<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> for any kind of sentence , including ill-formed ones .
We compare on a sample of sentences the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>grammaticality indices<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> obtained from <tag name="TECHNIQUE" value="start"/>PG<tag name="TECHNIQUE" value="end"/> formalism and the acceptability judgements measured by means of a psycholinguistic analysis .
The results show that the derived <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>grammaticality index<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> is a fairly good tracer of acceptability scores .


##W00-1427
Robust , Applied <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Morphological Generation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
In practical natural language generation systems it is often advantageous to have a separate component that deals purely with morphological processing .
We present such a component : a fast and robust <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>morphological generator<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> for English based on <tag name="TECHNIQUE" value="start"/>finite-state<tag name="TECHNIQUE" value="end"/> techniques that generates a word form given a specification of the <tag name="TECHNIQUE" value="start"/>lemma<tag name="TECHNIQUE" value="end"/> , <tag name="TECHNIQUE" value="start"/>part-of-speech<tag name="TECHNIQUE" value="end"/> , and the type of <tag name="TECHNIQUE" value="start"/>inflection<tag name="TECHNIQUE" value="end"/> required .
We describe how this <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>morphological generator<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> is used in a prototype system for automatic simplification of English newspaper text , and discuss practical morphological and orthographic issues we have encountered in generation of unrestricted text within this application .


##W09-1216
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Multilingual Semantic Parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> with a Pipeline of <tag name="TECHNIQUE" value="start"/>Linear Classifiers<tag name="TECHNIQUE" value="end"/> .
I describe a fast <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>multilingual parser<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>semantic dependencies<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
The parser is implemented as a pipeline of <tag name="TECHNIQUE" value="start"/>linear classifiers<tag name="TECHNIQUE" value="end"/> trained with <tag name="TECHNIQUE" value="start"/>support vector machines<tag name="TECHNIQUE" value="end"/> .
I use only <tag name="TECHNIQUE" value="start"/>first order features<tag name="TECHNIQUE" value="end"/> , and no pair-wise feature combinations in order to reduce training and prediction times .
Hyper-parameters are carefully tuned for each language and sub-problem .
The system is evaluated on seven different languages : Catalan , Chinese , Czech , English , German , Japanese and Spanish .
An analysis of learning rates and of the reliance on syntactic parsing quality shows that only modest improvements could be expected for most languages given more training data ; Better syntactic parsing quality , on the other hand , could greatly improve the results .
Individual tuning of hyper-parameters is crucial for obtaining good <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>semantic parsing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> quality .


##W05-0902
On The <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Subjectivity Of Human Authored Summaries<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We address the issue of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>human subjectivity when authoring summaries<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , aiming at a simple , robust evaluation of machine generated summaries .
Applying a <tag name="TECHNIQUE" value="start"/>cross comprehension test<tag name="TECHNIQUE" value="end"/> on human authored short summaries from broadcast news , the level of subjectivity is gauged among four authors .
The instruction set is simple , thus there is enough room for subjectivity .
However the approach is robust because the test does not use the absolute score , relying instead on relative comparison , effectively alleviating the subjectivity .
Finally we illustrate the application of the above scheme when evaluating the informativeness of machine generated summaries .


##W06-3501
Pragmatic <tag name="DOMAIN" value="start"/>Information Extraction<tag name="DOMAIN" value="end"/> From <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Subject Ellipsis<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> In Informal English .
<tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Subject ellipsis<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> is one of the characteristics of informal English .
The investigation of <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>subject ellipsis<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> in corpora thus reveals an abundance of pragmatic and extralinguistic information associated with <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>subject ellipsis<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> that enhances natural language understanding .
In essence , the presence of <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>subject elipsis<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> conveys an ` informal ' conversation involving 1 -RRB- an informal ` Topic ' as well as familiar\/close ` Participants ' , 2 -RRB- specific ` Conotations ' that are different from the corresponding ful sentences : interruptive -LRB- ending discourse coherence -RRB- , polite , intimate , friendly , and less determinate implicatures .
This paper also construes linguistic environments that triger the use of <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>subject ellipsis<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> and resolve <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>subject elipsis<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> .


##C02-1081
<tag name="TECHNIQUE" value="start"/>Data-Driven<tag name="TECHNIQUE" value="end"/>Classification Of  <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Linguistic Styles In Spoken Dialogues<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
Language users have individual <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>linguistic styles<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
A <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>spoken dialogue<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> system may benefit from adapting to the linguistic style of a user in input analysis and output generation .
To investigate the possibility to automatically classify speakers according to their <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>linguistic style<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> three corpora of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>spoken dialogues<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> were analyzed .
Several numerical parameters were computed for every speaker .
These parameters were reduced to linguistically interpretable components by means of a <tag name="TECHNIQUE" value="start"/>principal component analysis<tag name="TECHNIQUE" value="end"/> .
Classes were established from these components by <tag name="TECHNIQUE" value="start"/>cluster analysis<tag name="TECHNIQUE" value="end"/> .
Unseen input was classified by trained <tag name="TECHNIQUE" value="start"/>neural networks<tag name="TECHNIQUE" value="end"/> with varying error rates depending on corpus type .
A first investigation in using special <tag name="TECHNIQUE" value="start"/>language models<tag name="TECHNIQUE" value="end"/> for speaker classes was carried out .


##I05-1048
A <tag name="TECHNIQUE" value="start"/>Lexicon-Constrained Character<tag name="TECHNIQUE" value="end"/> Model for Chinese  <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Morphological Analysis<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
This paper proposes a <tag name="TECHNIQUE" value="start"/>lexicon-constrained character<tag name="TECHNIQUE" value="end"/> model that combines both <tag name="TECHNIQUE" value="start"/>word and character features<tag name="TECHNIQUE" value="end"/> to solve complicated issues in Chinese  <tag name="DOMAIN" value="start"/>morphological analysis<tag name="DOMAIN" value="end"/> .
A Chinese  <tag name="TECHNIQUE" value="start"/>character-based<tag name="TECHNIQUE" value="end"/> model constrained by a <tag name="TECHNIQUE" value="start"/>lexicon<tag name="TECHNIQUE" value="end"/> is built to acquire word building rules .
Each character in a Chinese sentence is assigned a tag by the proposed model .
The <tag name="DOMAIN" value="start"/>word segmentation<tag name="DOMAIN" value="end"/> and <tag name="DOMAIN" value="start"/>partof-speech tagging<tag name="DOMAIN" value="end"/> results are then generated based on the character tags .
The proposed method solves such problems as unknown word identification , data sparseness , and estimation bias in an integrated , unified framework .
Preliminary experiments indicate that the proposed method outperforms the best SIGHAN word segmentation systems in the open track on 3 out of the 4 test corpora .
Additionally , our method can be conveniently integrated with any other Chinese morphological systems as a post-processing module leading to significant improvement in performance .


##P09-2039
Extracting <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Comparative Sentences<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> from Korean Text Documents Using Comparative <tag name="TECHNIQUE" value="start"/>Lexical Patterns<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>Machine Learning<tag name="TECHNIQUE" value="end"/> Techniques .
This paper proposes how to automatically identify Korean <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>comparative sentences<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> from text documents .
This paper first investigates many comparative sentences referring to previous studies and then defines a set of comparative keywords from them .
A sentence which contains one or more elements of the keyword set is called a comparative-sentence candidate .
Finally , we use <tag name="TECHNIQUE" value="start"/>machine learning<tag name="TECHNIQUE" value="end"/> techniques to <tag name="DOMAIN" value="start"/>eliminate non-comparative sentences from the candidates<tag name="DOMAIN" value="end"/> .
As a result , we achieved significant performance , an F1-score of 88.54 % , in our experiments using various web documents .


##P08-2028
The Good , the Bad , and the Unknown : Morphosyllabic  <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Sentiment Tagging of Unseen Words<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
The omnipresence of unknown words is a problem that any NLP component needs to address in some form .
While there exist many established techniques for dealing with unknown words in the realm of POS-tagging , for example , guessing unknown words ' semantic properties is a less-explored area with greater challenges .
In this paper , we study the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>semantic field of sentiment<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> and propose five methods for assigning prior <tag name="DOMAIN" value="start"/>sentiment polarities to unknown words<tag name="DOMAIN" value="end"/> based on known <tag name="TECHNIQUE" value="start"/>sentiment carriers<tag name="TECHNIQUE" value="end"/> .
Tested on 2000 cases , the methods mirror human judgements closely in threeand twoway polarity classification tasks , and reach accuracies above 63 % and 81 % , respectively .


##N06-1004
<tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Segment Choice<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> Models : <tag name="TECHNIQUE" value="start"/>Feature-Rich<tag name="TECHNIQUE" value="end"/> Models For Global Distortion In <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Statistical<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/> Machine Translation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
This paper presents a new approach to <tag name="TECHNIQUE" value="start"/>distortion<tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/>phrase reordering<tag name="TECHNIQUE" value="end"/> -RRB- in <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>phrasebased<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>machine translation -LRB- MT -RRB-<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
Distortion is modeled as a sequence of choices during translation .
The approach yields trainable , <tag name="TECHNIQUE" value="start"/>probabilistic distortion<tag name="TECHNIQUE" value="end"/> models that are global : they assign a probability to each possible <tag name="TECHNIQUE" value="start"/>phrase reordering<tag name="TECHNIQUE" value="end"/> .
These <tag name="TECHNIQUE" value="start"/>`` segment choice ''<tag name="TECHNIQUE" value="end"/> models -LRB- <tag name="TECHNIQUE" value="start"/>SCMs<tag name="TECHNIQUE" value="end"/> -RRB- can be trained on `` segment-aligned '' sentence pairs ; they can be applied during decoding or rescoring .
The approach yields a metric called `` <tag name="TECHNIQUE" value="start"/>distortion perplexity<tag name="TECHNIQUE" value="end"/> '' -LRB- `` disperp '' -RRB- for comparing SCMs offline on test data , analogous to perplexity for language models .
A <tag name="TECHNIQUE" value="start"/>decision-tree-based SCM<tag name="TECHNIQUE" value="end"/> is tested on Chinese-to-English <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , and outperforms a baseline distortion penalty approach at the 99 % confidence level .


##P06-1004
<tag name="TECHNIQUE" value="start"/>Minimum Cut<tag name="TECHNIQUE" value="end"/> Model For <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Spoken Lecture Segmentation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We consider the task of <tag name="TECHNIQUE" value="start"/>unsupervised<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>lecture segmentation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We formalize <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>segmentation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> as a <tag name="TECHNIQUE" value="start"/>graph-partitioning<tag name="TECHNIQUE" value="end"/> task that optimizes the <tag name="TECHNIQUE" value="start"/>normalized cut<tag name="TECHNIQUE" value="end"/> criterion .
Our approach moves beyond localized comparisons and takes into account <tag name="TECHNIQUE" value="start"/>longrange cohesion dependencies<tag name="TECHNIQUE" value="end"/> .
Our results demonstrate that global analysis improves the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>segmentation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> accuracy and is robust in the presence of speech recognition errors .


##W99-0906
A <tag name="TECHNIQUE" value="start"/>Computational<tag name="TECHNIQUE" value="end"/> Approach To <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Deciphering Unknown Scripts<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We propose and evaluate <tag name="TECHNIQUE" value="start"/>computational <tag name="TECHNIQUE" value="end"/>techniques for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>deciphering unknown scripts<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We focus on the case in which an unfamiliar script encodes a known language .
The decipherment of a brief document or inscription is driven by data about the spoken language .
We consider which scripts are easy or hard to decipher , how much data is required , and whether the techniques are robust against language change over time .


##W08-1511
A Small-Vocabulary Shared Task for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Medical Speech Translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We outline a possible small-vocabulary shared task for the emerging <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>medical speech translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> community .
Data would consist of about 2000 recorded and transcribed utterances collected during an evaluation of an English â Spanish version of the Open Source MedSLT system ; the vocabulary covered consisted of about 450 words in English , and 250 in Spanish .
The key problem in defining the task is to agree on a scoring system which is acceptable both to medical professionals and to the speech and language community .
We suggest a framework for defining and administering a scoring system of this kind .


##D08-1058
Using <tag name="TECHNIQUE" value="start"/>Bilingual Knowledge<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>Ensemble<tag name="TECHNIQUE" value="end"/> Techniques for <tag name="TECHNIQUE" value="start"/>Unsupervised<tag name="TECHNIQUE" value="end"/> Chinese<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/> Sentiment<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Analysis .
It is a challenging task to identify <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>sentiment polarity<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> of Chinese reviews because the resources for Chinese <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>sentiment<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> analysis are limited .
Instead of leveraging only monolingual Chinese knowledge , this study proposes a novel approach to leverage reliable English resources to improve Chinese  <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>sentiment<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> analysis .
Rather than simply projecting English resources onto Chinese resources , our approach first <tag name="TECHNIQUE" value="start"/>translates<tag name="TECHNIQUE" value="end"/> Chinese reviews into English reviews by <tag name="TECHNIQUE" value="start"/>machine translation<tag name="TECHNIQUE" value="end"/> services , and then identifies the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>sentiment polarity<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> of English reviews by directly leveraging English resources .
Furthermore , our approach performs <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>sentiment<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> analysis for both Chinese reviews and English reviews , and then uses <tag name="TECHNIQUE" value="start"/>ensemble<tag name="TECHNIQUE" value="end"/> methods to combine the individual analysis results .
Experimental results on a dataset of 886 Chinese product reviews demonstrate the effectiveness of the proposed approach .
The individual analysis of the translated English reviews outperforms the individual analysis of the original Chinese reviews , and the combination of the individual analysis results further improves the performance .


##P08-2011
<tag name="TECHNIQUE" value="start"/>Coreference-inspired<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Coherence Modeling<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
Research on <tag name="TECHNIQUE" value="start"/>coreference resolution<tag name="TECHNIQUE" value="end"/> and summarization has modeled the way entities are realized as concrete phrases in discourse .
In particular there exist models of the noun phrase syntax used for discourse-new versus discourse-old referents , and models describing the likely distance between a pronoun and its antecedent .
However , models of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>discourse coherence<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , as applied to <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>information ordering<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> tasks , have ignored these kinds of information .
We apply a <tag name="TECHNIQUE" value="start"/>discourse-new classifier<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>pronoun coreference<tag name="TECHNIQUE" value="end"/> algorithm to the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>information ordering<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> task , and show significant improvements in performance over the entity grid , a popular model of local coherence .


##P98-1021
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Spoken Dialogue Interpretation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> with the <tag name="TECHNIQUE" value="start"/>DOP<tag name="TECHNIQUE" value="end"/> Model .
We show how the <tag name="TECHNIQUE" value="start"/>DOP<tag name="TECHNIQUE" value="end"/> model can be used for fast and robust processing of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>spoken input<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> in a practical <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>spoken dialogue<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> system called OVIS .
OVIS , Openbaar Vervoer Informatie Systeem -LRB- `` Public Transport Information System '' -RRB- , is a Dutch <tag name="DOMAIN" value="start"/>spoken language information<tag name="DOMAIN" value="end"/> system which operates over ordinary telephone lines .
The prototype system is the immediate goal of the NWO 1 Priority Programme `` Language and Speech Technology '' .
In this paper , we extend the original <tag name="TECHNIQUE" value="start"/>DOP<tag name="TECHNIQUE" value="end"/> model to <tag name="TECHNIQUE" value="start"/>context-sensitive<tag name="TECHNIQUE" value="end"/>interpretation of  <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>spoken input<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
The system we describe uses the OVIS corpus -LRB- 10,000 trees enriched with compositional semantics -RRB- to compute from an input word-graph the best utterance together with its meaning .
Dialogue <tag name="TECHNIQUE" value="start"/>context<tag name="TECHNIQUE" value="end"/> is taken into account by dividing up the OVIS corpus into context-dependent subcorpora .
Each system question triggers a subcorpus by which the user answer is analyzed and interpreted .
Our experiments indicate that the <tag name="TECHNIQUE" value="start"/>context-sensitive DOP<tag name="TECHNIQUE" value="end"/> model obtains better accuracy than the original model , allowing for fast and robust processing of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>spoken input<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .


##N06-2018
<tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>MMR-Based Active Machine Learning<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> For <tag name="DOMAIN" value="start"/>Bio Named Entity Recognition<tag name="DOMAIN" value="end"/> .
This paper presents a new <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>active learning<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> paradigm which considers not only the uncertainty of the classifier but also the diversity of the corpus .
The two measures for uncertainty and diversity were combined using the <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>MMR<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Maximal Marginal Relevance<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> -RRB- method to give the sampling scores in our <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>active learning<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> strategy .
We incorporated <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>MMR-based active machinelearning<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> idea into the <tag name="DOMAIN" value="start"/>biomedical namedentity recognition<tag name="DOMAIN" value="end"/> system .
Our experimental results indicated that our strategies for <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>active-learning<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> based sample selection could significantly reduce the human effort .


##P09-3007
Accurate Learning for Chinese  <tag name="DOMAIN" value="start"/>Function Tags<tag name="DOMAIN" value="end"/> from Minimal Features .
<tag name="TECHNIQUE" value="start"/>Data-driven<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>function tag<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> assignment has been studied for English using Penn Treebank data .
In this paper , we address the question of whether such method can be applied to other languages and Treebank resources .
In addition to simply extend previous method from English to Chinese , we also proposed an effective way to recognize <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>function tags<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> directly from <tag name="TECHNIQUE" value="start"/>lexical information<tag name="TECHNIQUE" value="end"/> , which is easily scalable for languages that lack sufficient parsing resources or have inherent linguistic challenges for parsing .
We investigated a <tag name="TECHNIQUE" value="start"/>supervised sequence learning<tag name="TECHNIQUE" value="end"/> method to automatically recognize <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>function tags<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , which achieves an F-score of 0.938 on gold-standard POS -LRB- Part-ofSpeech -RRB- tagged Chinese text -- a statistically significant improvement over existing Chinese function label assignment systems .
Results show that a small number of linguistically motivated <tag name="TECHNIQUE" value="start"/>lexical features<tag name="TECHNIQUE" value="end"/> are sufficient to achieve comparable performance to systems using sophisticated parse trees .


##P97-1021
A <tag name="TECHNIQUE" value="start"/>DOP<tag name="TECHNIQUE" value="end"/> Model For <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Semantic Interpretation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
In data-oriented language processing , an annotated language corpus is used as a stochastic grammar .
The most probable analysis of a new sentence is constructed by combining fragments from the corpus in the most probable way .
This approach has been successfully used for syntactic analysis , using corpora with syntactic annotations such as the Penn Tree-bank .
If a corpus with semantically annotated sentences is used , the same approach can also generate the most probable <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>semantic interpretation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> of an input sentence .
The present paper explains this <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>semantic interpretation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> method .
A <tag name="TECHNIQUE" value="start"/>data-oriented<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>semantic interpretation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> algorithm was tested on two semantically annotated corpora : the English ATIS corpus and the Dutch OVIS corpus .
Experiments show an increase in semantic accuracy if larger corpus-fragments are taken into consideration .


##W06-1207
Classifying Particle Semantics In <tag name="DOMAIN" value="start"/>English Verb-Particle Constructions<tag name="DOMAIN" value="end"/> .
Previous computational work on learning the semantic properties of <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>verb-particle constructions<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> -LRB- <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>VPCs<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> -RRB- has focused on their compositionality , and has left unaddressed the issue of which meaning of the component words is being used in a given <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>VPC<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We develop a <tag name="TECHNIQUE" value="start"/>feature space<tag name="TECHNIQUE" value="end"/> for use in classification of the sense contributed by the particle in a <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>VPC<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , and test this on <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>VPCs<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> using the particle up .
The features that capture <tag name="TECHNIQUE" value="start"/>linguistic properties<tag name="TECHNIQUE" value="end"/> of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>VPCs<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> that are relevant to the semantics of the particle outperform linguistically uninformed word co-occurrence features in our experiments on unseen test <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>VPCs<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .


##C04-1075
A High-Performance <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Coreference Resolution<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> System Using A <tag name="TECHNIQUE" value="start"/>Constraint-Based Multi-Agent<tag name="TECHNIQUE" value="end"/> Strategy .
This paper presents a <tag name="TECHNIQUE" value="start"/>constraint-based multiagent<tag name="TECHNIQUE" value="end"/> strategy to <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>coreference resolution<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> of general noun phrases in unrestricted English text .
For a given anaphor and all the preceding referring expressions as the antecedent candidates , a common <tag name="TECHNIQUE" value="start"/>constraint agent<tag name="TECHNIQUE" value="end"/> is first presented to filter out invalid antecedent candidates using various kinds of general knowledge .
Then , according to the type of the anaphor , a special <tag name="TECHNIQUE" value="start"/>constraint agent<tag name="TECHNIQUE" value="end"/> is proposed to filter out more invalid antecedent candidates using constraints which are derived from various kinds of special knowledge .
Finally , a simple <tag name="TECHNIQUE" value="start"/>preference agent<tag name="TECHNIQUE" value="end"/> is used to choose an antecedent for the anaphor form the remaining antecedent candidates , based on the proximity principle .
One interesting observation is that the most recent antecedent of an anaphor in the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>coreferential chain<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> is sometimes indirectly linked to the anaphor via some other antecedents in the chain .
In this case , we find that the most recent antecedent always contains little information to directly determine the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>coreference<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> relationship with the anaphor .
Therefore , for a given anaphor , the corresponding special constraint agent can always safely filter out these less informative antecedent candidates .
In this way , rather than finding the most recent antecedent for an anaphor , our system tries to find the most direct and informative antecedent .
Evaluation shows that our system achieves Precision \/ Recall \/ F-measures of 84.7 % \/ 65.8 % \/ 73.9 and 82.8 % \/ 55.7 % \/ 66.5 on MUC6 and MUC-7 English <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>coreference<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> tasks respectively .
This means that our system achieves significantly better precision rates by about 8 percent over the best-reported systems while keeping recall rates .


##P05-2011
Towards An Optimal Lexicalization In A Natural-Sounding Portable <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Natural Language Generator<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> For <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Dialog<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Systems .
In contrast to the latest progress in speech recognition , the state-of-the-art in <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>natural language generation for spoken language dialog<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> systems is lagging behind .
The core dialog managers are now more sophisticated ; and natural-sounding and flexible output is expected , but not achieved with current simple techniques such as template-based systems .
Portability of systems across subject domains and languages is another increasingly important requirement in dialog systems .
This paper presents an outline of <tag name="FOCUS" value="start"/>LEGEND<tag name="FOCUS" value="end"/> , a system that is both portable and generates natural-sounding output .
This goal is achieved through the novel use of existing <tag name="TECHNIQUE" value="start"/>lexical resources<tag name="TECHNIQUE" value="end"/> such as <tag name="TECHNIQUE" value="start"/>FrameNet<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>WordNet<tag name="TECHNIQUE" value="end"/> .


##W09-2402
Making Sense of <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Word Sense Variation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
We present a pilot study of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>word-sense<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> annotation using multiple annotators , relatively polysemous words , and a heterogenous corpus .
Annotators selected senses for words in context , using an annotation interface that presented <tag name="TECHNIQUE" value="start"/>WordNet<tag name="TECHNIQUE" value="end"/> senses .
Interannotator agreement -LRB- IA -RRB- results show that annotators agree well or not , depending primarily on the individual words and their general usage properties .
Our focus is on identifying systematic differences across words and annotators that can account for IA variation .
We identify three lexical use factors : semantic specificity of the context , sense concreteness , and similarity of senses .
We discuss systematic differences in sense selection across annotators , and present the use of <tag name="TECHNIQUE" value="start"/>association rules<tag name="TECHNIQUE" value="end"/> to mine the data for systematic differences across annotators .


##P09-2047
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Query Segmentation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Based on <tag name="TECHNIQUE" value="start"/>Eigenspace Similarity<tag name="TECHNIQUE" value="end"/> .
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Query segmentation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> is essential to query processing .
It aims to tokenize query words into several semantic segments and help the search engine to improve the precision of retrieval .
In this paper , we present a novel <tag name="TECHNIQUE" value="start"/>unsupervised learning<tag name="TECHNIQUE" value="end"/> approach to <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>query segmentation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> based on <tag name="TECHNIQUE" value="start"/>principal eigenspace similarity<tag name="TECHNIQUE" value="end"/> of <tag name="TECHNIQUE" value="start"/>queryword-frequency matrix<tag name="TECHNIQUE" value="end"/> derived from <tag name="TECHNIQUE" value="start"/>web statistics<tag name="TECHNIQUE" value="end"/> .
Experimental results show that our approach could achieve superior performance of 35.8 % and 17.7 % in Fmeasure over the two baselines respectively , i.e. MI -LRB- Mutual Information -RRB- approach and EM optimization approach .


##P98-1035
Exploiting <tag name="TECHNIQUE" value="start"/>Syntactic Structure<tag name="TECHNIQUE" value="end"/> for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Language Modeling<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
The paper presents a <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>language model<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> that develops <tag name="TECHNIQUE" value="start"/>syntactic structure<tag name="TECHNIQUE" value="end"/> and uses it to extract meaningful information from the word history , thus enabling the use of <tag name="TECHNIQUE" value="start"/>long distance dependencies<tag name="TECHNIQUE" value="end"/> .
The model assigns probability to every joint sequence of words-binary-parse-structure with headword annotation and operates in a left-to-right manner - therefore usable for <tag name="DOMAIN" value="start"/>automatic speech recognition<tag name="DOMAIN" value="end"/> .
The model , its <tag name="TECHNIQUE" value="start"/>probabilistic<tag name="TECHNIQUE" value="end"/> parameterization , and a set of experiments meant to evaluate its predictive power are presented ; an improvement over standard <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>trigram modeling<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> is achieved .


##C90-3072
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Spelling-Checking<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> For Highly Inflective Languages .
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Spelling-checkers<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> have become an integral part of most text processing software .
From different reasons among which the speed of processing prevails they are usually based on dictionaries of word forms instead of words .
This approach is sufficient for languages with little inflection such as English , but fails for highly inflective languages such as Czech , Russian , Slovak or other Slavonic languages .
We have developed a special method for describing inflection for the purpose of building <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>spelling-checkers<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> for such languages .
The speed of the resulting program lies somewhere in the middle of the scale of existing spelling-checkers for English and the main dictionary fits into the standard 360K floppy , whereas the number of recognized word forms exceeds 6 million -LRB- for Czech -RRB- .
Further , a special method has been developed for easy <tag name="TECHNIQUE" value="start"/>word classification<tag name="TECHNIQUE" value="end"/> .


##P92-1015
<tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Prosodic<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> Aids To <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Syntactic And Semantic<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Analysis Of Spoken English .
<tag name="TECHNIQUE" value="start"/>Prosody<tag name="TECHNIQUE" value="end"/> can be useful in resolving certain lexical and structural ambiguities in spoken English .
In this paper we present some results of employing two types of <tag name="TECHNIQUE" value="start"/>prosodic<tag name="TECHNIQUE" value="end"/> information , namely <tag name="TECHNIQUE" value="start"/>pitch and pause<tag name="TECHNIQUE" value="end"/> , to assist <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>syntactic and semantic<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> analysis during <tag name="DOMAIN" value="start"/>parsing<tag name="DOMAIN" value="end"/> .


##W99-0904
<tag name="TECHNIQUE" value="start"/>Unsupervised Learning<tag name="TECHNIQUE" value="end"/> Of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Derivational Morphology<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> From <tag name="TECHNIQUE" value="start"/>Inflectional Lexicons<tag name="TECHNIQUE" value="end"/> .
We present in this paper an <tag name="TECHNIQUE" value="start"/>unsupervised<tag name="TECHNIQUE" value="end"/> method to learn <tag name="DOMAIN" value="start"/>suffixes and suffixation operations<tag name="DOMAIN" value="end"/> from an <tag name="TECHNIQUE" value="start"/>inflectional lexicon<tag name="TECHNIQUE" value="end"/> of a language .
The elements acquired with our method are used to build <tag name="DOMAIN" value="start"/>stemming<tag name="DOMAIN" value="end"/> procedures and can assist lexicographers in the development of new lexical resources .


##W01-0908
Using The <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Distribution Of Performance<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> For <tag name="DOMAIN" value="start"/>Studying Statistical NLP Systems<tag name="DOMAIN" value="end"/> And Corpora .
Statistical NLP systems are frequently evaluated and compared on the basis of their performances on a single split of training and test data .
Results obtained using a single split are , however , subject to sampling noise .
In this paper we argue in favor of reporting a <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>distribution of performance<tag name="TECHNIQUE" value="end"/><tag name="FOCUS" value="end"/> gures , obtained by <tag name="TECHNIQUE" value="start"/>resampling<tag name="TECHNIQUE" value="end"/> the training data , rather than a single number .
The additional information from distributions can be used to make statistically quanti ed statements about di erences across parameter settings , systems , and corpora .


##W03-0907
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Story Understanding<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Through <tag name="TECHNIQUE" value="start"/>Multi-Representation <tag name="TECHNIQUE" value="end"/> Model Construction .
We present an implemented model of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>story understanding<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> and apply it to the understanding of a children 's story .
We argue that understanding a story consists of building <tag name="TECHNIQUE" value="start"/>multirepresentation<tag name="TECHNIQUE" value="end"/> models of the story and that story models are efficiently constructed using a <tag name="TECHNIQUE" value="start"/>satisfiability solver<tag name="TECHNIQUE" value="end"/> .
We present a computer program that contains <tag name="TECHNIQUE" value="start"/>multiple representations<tag name="TECHNIQUE" value="end"/> of commonsense knowledge , takes a narrative as input , transforms the narrative and representations of commonsense knowledge into a <tag name="TECHNIQUE" value="start"/>satisfiability<tag name="TECHNIQUE" value="end"/> problem , runs a satisfiability solver , and produces models of the story as output .
The narrative , models , and representations are expressed in the language of Shanahan 's event calculus .


##P07-1083
<tag name="TECHNIQUE" value="start"/>Alignment-Based Discriminative<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>String Similarity<tag name="DOMAIN" value="end"/> .
A character-based measure of similarity is an important component of many natural language processing systems , including approaches to transliteration , coreference , word alignment , spelling correction , and the identi cation of cognates in related vocabularies .
We propose an <tag name="TECHNIQUE" value="start"/>alignment-based discriminative<tag name="TECHNIQUE" value="end"/> framework for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>string similarity<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We gather features from substring pairs consistent with a <tag name="TECHNIQUE" value="start"/>character-based alignment<tag name="TECHNIQUE" value="end"/> of the two strings .
This approach achieves exceptional performance ; on nine separate cognate identi cation experiments using six language pairs , we more than double the precision of traditional orthographic measures like Longest Common Subsequence Ratio and Dice 's Coef cient .
We also show strong improvements over other recent discriminative and heuristic similarity functions .


##W09-1113
Learning Where to Look : Modeling <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Eye Movements in Reading<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We propose a novel <tag name="TECHNIQUE" value="start"/>machine learning<tag name="TECHNIQUE" value="end"/> task that consists in learning to predict which words in a text are  <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>fixated by a reader<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
In a first pilot experiment , we show that it is possible to outperform a majority baseline using a <tag name="TECHNIQUE" value="start"/>transitionbased<tag name="TECHNIQUE" value="end"/> model with a <tag name="TECHNIQUE" value="start"/>logistic regression classifier<tag name="TECHNIQUE" value="end"/> and a very limited set of features .
We also show that the model is capable of capturing frequency effects on <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>eye movements<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> observed in human readers .


##P04-3025
Incorporating <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Topic Information<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> Into <tag name="DOMAIN" value="start"/>Semantic Analysis<tag name="DOMAIN" value="end"/> Models .
This paper reports experiments in <tag name="DOMAIN" value="start"/>classifying texts<tag name="DOMAIN" value="end"/> based upon their favorability towards the subject of the text using a feature set enriched with <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>topic<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> information on a small dataset of music reviews hand-annotated for topic .
The results of these experiments suggest ways in which incorporating <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>topic<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> information into such models may yield improvement over models which do not use topic information .


##D08-1026
Incorporating <tag name="TECHNIQUE" value="start"/>Temporal<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>Semantic<tag name="TECHNIQUE" value="end"/> Information with Eye Gaze for Automatic <tag name="DOMAIN" value="start"/>Word Acquisition in Multimodal Conversational<tag name="DOMAIN" value="end"/> Systems .
One major bottleneck in <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>conversational<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> systems is their incapability in interpreting unexpected user language inputs such as out-ofvocabulary words .
To overcome this problem , <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>conversational<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> systems must be able to learn new words automatically during human machine conversation .
Motivated by psycholinguistic findings on eye gaze and human language processing , we are developing techniques to incorporate human eye gaze for automatic <tag name="DOMAIN" value="start"/>word acquisition in multimodal conversational<tag name="DOMAIN" value="end"/> systems .
This paper investigates the use of <tag name="TECHNIQUE" value="start"/>temporal alignment<tag name="TECHNIQUE" value="end"/> between speech and <tag name="TECHNIQUE" value="start"/>eye gaze<tag name="TECHNIQUE" value="end"/> and the use of <tag name="TECHNIQUE" value="start"/>domain knowledge<tag name="TECHNIQUE" value="end"/> in word acquisition .
Our experiment results indicate that <tag name="TECHNIQUE" value="start"/>eye gaze<tag name="TECHNIQUE" value="end"/> provides a potential channel for automatically acquiring new words .
The use of extra <tag name="TECHNIQUE" value="start"/>temporal and domain knowledge<tag name="TECHNIQUE" value="end"/> can significantly improve acquisition performance .


##W04-2314
<tag name="TECHNIQUE" value="start"/>Bootstrapping<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Spoken Dialog<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Systems With Data Reuse .
Building natural language spoken dialog systems requires large amounts of human transcribed and labeled speech utterances to reach useful operational service performances .
Furthermore , the design of such complex systems consists of several manual steps .
The User Experience -LRB- UE -RRB- expert analyzes and de nes by hand the system core functionalities : the system semantic scope -LRB- call-types -RRB- and the dialog manager strategy which will drive the human-machine interaction .
This approach is extensive and error prone since it involves several non-trivial design decisions that can only be evaluated after the actual system deployment .
Moreover , scalability is compromised by time , costs and the high level of UE know-how needed to reach a consistent design .
We propose a novel approach for <tag name="TECHNIQUE" value="start"/>bootstrapping<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>spoken dialog<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> systems based on reuse of existing transcribed and labeled data , common reusable dialog <tag name="TECHNIQUE" value="start"/>templates and patterns<tag name="TECHNIQUE" value="end"/> , generic language and understanding models , and a consistent design process .
We demonstrate that our approach reduces design and development time while providing an effective system without any application speci c data .


##W97-0213
A Perspective On <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Word Sense Disambiguation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Methods And Their Evaluation .
In this position paper , we make several observations about the state of the art in automatic <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>word sense disambiguation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
Motivated by these observations , we offer several specific proposals to the community regarding improved evaluation criteria , common training and testing resources , and the definition of sense inventories .


##P06-1009
<tag name="TECHNIQUE" value="start"/>Discriminative<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Word Alignment<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> With <tag name="TECHNIQUE" value="start"/>Conditional Random Fields<tag name="TECHNIQUE" value="end"/> .
In this paper we present a novel approach for inducing <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>word alignments<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> from sentence aligned data .
We use a <tag name="TECHNIQUE" value="start"/>Conditional Random Field<tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/>CRF<tag name="TECHNIQUE" value="end"/> -RRB- , a <tag name="TECHNIQUE" value="start"/>discriminative<tag name="TECHNIQUE" value="end"/> model , which is estimated on a small supervised training set .
The <tag name="TECHNIQUE" value="start"/>CRF<tag name="TECHNIQUE" value="end"/> is conditioned on both the source and target texts , and thus allows for the use of arbitrary and overlapping features over these data .
Moreover , the <tag name="TECHNIQUE" value="start"/>CRF<tag name="TECHNIQUE" value="end"/> has efficient training and decoding processes which both find globally optimal solutions .
We apply this <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>alignment<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> model to both French-English and Romanian-English language pairs .
We show how a large number of highly predictive features can be easily incorporated into the <tag name="TECHNIQUE" value="start"/>CRF<tag name="TECHNIQUE" value="end"/> , and demonstratethatevenwithonlyafewhundred word-aligned training sentences , our model improves over the current state-ofthe-art with <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>alignment<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> error rates of 5.29 and 25.8 for the two tasks respectively .


##W00-1214
<tag name="TECHNIQUE" value="start"/>Machine Learning<tag name="TECHNIQUE" value="end"/> Methods For Chinese <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Web Page Categorization<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
This paper reports our evaluation of <tag name="TECHNIQUE" value="start"/>k Nearest Neighbor<tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/>kNN<tag name="TECHNIQUE" value="end"/> -RRB- , <tag name="TECHNIQUE" value="start"/>Support Vector Machines<tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/>SVM<tag name="TECHNIQUE" value="end"/> -RRB- , and <tag name="TECHNIQUE" value="start"/>Adaptive Resonance Associative Map<tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/>ARAM<tag name="TECHNIQUE" value="end"/> -RRB- on Chinese <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>web page classification<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
Benchmark experiments based on a Chinese web corpus showed that their predictive performance were roughly comparable although <tag name="TECHNIQUE" value="start"/>ARAM<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>kNN<tag name="TECHNIQUE" value="end"/> slightly outperformed <tag name="TECHNIQUE" value="start"/>SVM<tag name="TECHNIQUE" value="end"/> in small categories .
In addition , inserting <tag name="TECHNIQUE" value="start"/>rules<tag name="TECHNIQUE" value="end"/> into <tag name="TECHNIQUE" value="start"/>ARAM<tag name="TECHNIQUE" value="end"/> helped to improve performance , especially for small welldefined categories .


##I08-4025
Training a <tag name="TECHNIQUE" value="start"/>Perceptron with Global and Local Features<tag name="TECHNIQUE" value="end"/> for Chinese <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Word Segmentation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
This paper proposes the use of <tag name="TECHNIQUE" value="start"/>global features<tag name="TECHNIQUE" value="end"/> for Chinese <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>word segmentation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
These <tag name="TECHNIQUE" value="start"/>global features<tag name="TECHNIQUE" value="end"/> are combined with <tag name="TECHNIQUE" value="start"/>local features<tag name="TECHNIQUE" value="end"/> using the <tag name="TECHNIQUE" value="start"/>averaged perceptron<tag name="TECHNIQUE" value="end"/> algorithm over N-best candidate word segmentations .
The N-best candidates are produced using a <tag name="TECHNIQUE" value="start"/>conditional random field<tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/>CRF<tag name="TECHNIQUE" value="end"/> -RRB- <tag name="TECHNIQUE" value="start"/>character-based tagger<tag name="TECHNIQUE" value="end"/> for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>word segmentation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
Our experiments show that by adding <tag name="TECHNIQUE" value="start"/>global features<tag name="TECHNIQUE" value="end"/> , performance is significantly improved compared to the character-based CRF tagger .
Performance is also improved compared to using only <tag name="TECHNIQUE" value="start"/>local features<tag name="TECHNIQUE" value="end"/> .
Our system obtains an F-score of 0.9355 on the CityU corpus , 0.9263 on the CKIP corpus , 0.9512 on the SXU corpus , 0.9296 on the NCC corpus and 0.9501 on the CTB corpus .
All results are for the closed track in the fourth SIGHAN Chinese <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Word Segmentation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Bakeoff .


##N07-4014
The <tag name="TECHNIQUE" value="start"/>Hidden Information State<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Dialogue Manager<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> : A Real-World <tag name="TECHNIQUE" value="start"/>POMDP-Based<tag name="TECHNIQUE" value="end"/> System .
The <tag name="TECHNIQUE" value="start"/>Hidden Information State -LRB- HIS -RRB-<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Dialogue<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> System is the first trainable and scalable implementation of a <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>spoken dialog<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> system based on the <tag name="TECHNIQUE" value="start"/>PartiallyObservable Markov-Decision-Process<tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/>POMDP<tag name="TECHNIQUE" value="end"/> -RRB- model of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>dialogue<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
The system responds to n-best output from the speech recogniser , maintains multiple concurrent dialogue state hypotheses , and provides a visual display showing how competing hypotheses are ranked .
The demo is a prototype application for the Tourist Information Domain and achieved a task completion rate of over 90 % in a recent user study .


##P09-1069
Learning a <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Compositional Semantic Parser<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> using an Existing <tag name="TECHNIQUE" value="start"/>Syntactic Parser<tag name="TECHNIQUE" value="end"/> .
We present a new approach to learning a <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>semantic parser<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> -LRB- a system that maps natural language sentences into logical form -RRB- .
Unlikepreviousmethods , itexploitsanexisting <tag name="TECHNIQUE" value="start"/>syntactic parser<tag name="TECHNIQUE" value="end"/> to produce disambiguated <tag name="TECHNIQUE" value="start"/>parse trees<tag name="TECHNIQUE" value="end"/> that drive the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>compositional semantic<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> interpretation .
The resulting system produces improved results on standard corpora on natural language interfaces for <tag name="DOMAIN" value="start"/>database querying<tag name="DOMAIN" value="end"/> and simulated <tag name="DOMAIN" value="start"/>robot control<tag name="DOMAIN" value="end"/> .


##N09-1028
Using a <tag name="TECHNIQUE" value="start"/>Dependency Parser<tag name="TECHNIQUE" value="end"/> to Improve <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>SMT<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> for Subject-Object-Verb Languages .
We introduce a novel <tag name="TECHNIQUE" value="start"/>precedence reordering<tag name="TECHNIQUE" value="end"/> approach based on a <tag name="TECHNIQUE" value="start"/>dependency parser<tag name="TECHNIQUE" value="end"/> to <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>statistical<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>machine translation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> systems .
Similar to other preprocessing reordering approaches , our method can efficiently incorporate <tag name="TECHNIQUE" value="start"/>linguistic knowledge<tag name="TECHNIQUE" value="end"/> into <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>SMT<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> systems without increasing the complexity of decoding .
For a set of five subject-object-verb -LRB- SOV -RRB- order languages , we show significant improvements in BLEU scores when <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>translating<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> from English , compared to other reordering approaches , in state-of-the-art phrase-based SMT systems .


##J89-2001
A <tag name="TECHNIQUE" value="start"/>Pragmatics-Based<tag name="TECHNIQUE" value="end"/> Approach To <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Ellipsis Resolution<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Intersentential elliptical utterances<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> occur frequently during <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>information-seeking dialogs<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> in task domains .
This paper presents a <tag name="TECHNIQUE" value="start"/>pragmatics-based<tag name="TECHNIQUE" value="end"/> framework for interpreting such utterances .
Discourse expectations and focusing heuristics are used to facilitate recognition of an information-seeker 's intent in uttering an elliptical fragment .
The <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>ellipsis<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> is comprehended by identifying both the aspect of the information-seeker 's task-related plan highlighted by the fragment and the conversational discourse goal fulfilled by the utterance .
The contribution of this approach is its consideration of <tag name="TECHNIQUE" value="start"/>pragmatic<tag name="TECHNIQUE" value="end"/> information , including <tag name="TECHNIQUE" value="start"/>discourse content<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>conversational goals<tag name="TECHNIQUE" value="end"/> , rather than just the precise representation of the preceding utterance .


##C88-1021
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Anaphora Resolution<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> : A <tag name="TECHNIQUE" value="start"/>Multi-Strategy<tag name="TECHNIQUE" value="end"/> Approach .
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Anaphora resolution<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> has proven to be a very difficult problem ; it requires the integrated application of syntactic , semantic , and pragmatic knowledge .
This paper examines the hypothesis that instead of attempting to construct a monolithic method for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>resolving anaphora<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , the combination of <tag name="TECHNIQUE" value="start"/>multiple strategies<tag name="TECHNIQUE" value="end"/> , each exploiting a different knowledge source , proves more effective , theoretically and computationally .
Cognitive plausibility is established in that human judgements of the optimal anaphoric referent accord with those of the <tag name="TECHNIQUE" value="start"/>strategy-based<tag name="TECHNIQUE" value="end"/> method , and human inability to determine a unique referent corresponds to the cases where different strategies offer conflicting candidates for the anaphoric referent .


##N09-2044
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Classifying Factored Genres<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> with <tag name="TECHNIQUE" value="start"/>Part-of-Speech Histograms<tag name="TECHNIQUE" value="end"/> .
This work addresses the problem of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>genre classification<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> of text and speech transcripts , with the goal of handling genres not seen in training .
Two frameworks employing different statistics on <tag name="TECHNIQUE" value="start"/>word\/POS histograms<tag name="TECHNIQUE" value="end"/> with a <tag name="TECHNIQUE" value="start"/>PCA transform<tag name="TECHNIQUE" value="end"/> are examined : a single model for each genre and a factored representation of genre .
The impact of the two frameworks on the classification of training-matched and new <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>genres<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> is discussed .
Results show that the factored models allow for a finer-grained representation of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>genre<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> and can more accurately characterize <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>genres<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> not seen in training .


##W08-0616
Using Natural Language Processing to <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Classify Suicide Notes<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We hypothesize that <tag name="TECHNIQUE" value="start"/>machine-learning<tag name="TECHNIQUE" value="end"/> algorithms -LRB- <tag name="TECHNIQUE" value="start"/>MLA<tag name="TECHNIQUE" value="end"/> -RRB- can classify completer and simulated <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>suicide notes<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> as well as mental health professionals -LRB- MHP -RRB- .
Five MHPs classified 66 simulated or completer notes ; <tag name="TECHNIQUE" value="start"/>MLAs<tag name="TECHNIQUE" value="end"/> were used for the same task .
Results : MHPs were accurate 71 % of the time ; using the <tag name="TECHNIQUE" value="start"/>sequential minimization optimization<tag name="TECHNIQUE" value="end"/> algorithm -LRB- <tag name="TECHNIQUE" value="start"/>SMO<tag name="TECHNIQUE" value="end"/> -RRB- <tag name="TECHNIQUE" value="start"/>MLAs<tag name="TECHNIQUE" value="end"/> were accurate 78 % of the time .
There was no significant difference between the MLA and MPH classifiers .
This is an important first step in developing an <tag name="TECHNIQUE" value="start"/>evidence<tag name="TECHNIQUE" value="end"/> based <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>suicide predictor<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> for emergency department use .


##W02-0309
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Biomedical Text Retrieval<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> In Languages With A Complex Morphology .
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Document retrieval<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> in languages with a rich and complex morphology -- particularly in terms of derivation and -LRB- single-word -RRB- composition -- suffers from serious performance degradation with the stemming-only query-term-to-text-word matching paradigm .
We propose an alternative approach in which <tag name="DOMAIN" value="start"/>morphologically<tag name="DOMAIN" value="end"/> complex word forms are segmented into relevant subwords -LRB- such as <tag name="TECHNIQUE" value="start"/>stems<tag name="TECHNIQUE" value="end"/> , <tag name="TECHNIQUE" value="start"/>named entities<tag name="TECHNIQUE" value="end"/> , <tag name="TECHNIQUE" value="start"/>acronyms<tag name="TECHNIQUE" value="end"/> -RRB- , and <tag name="TECHNIQUE" value="start"/>subwords<tag name="TECHNIQUE" value="end"/> constitute the basic unit for indexing and <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>retrieval<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We evaluate our approach on a large biomedical document collection .


##W05-1302
Adaptive <tag name="TECHNIQUE" value="start"/>String Similarity<tag name="TECHNIQUE" value="end"/> Metrics For <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Biomedical Reference Resolution<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
In this paper we present the evaluation of a set of <tag name="TECHNIQUE" value="start"/>string similarity<tag name="TECHNIQUE" value="end"/> metrics used to <tag name="DOMAIN" value="start"/>resolve the mapping from strings to concepts<tag name="DOMAIN" value="end"/> in the UMLS MetaThesaurus .
<tag name="TECHNIQUE" value="start"/>String similarity<tag name="TECHNIQUE" value="end"/> is conceived as a single component in a full <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Reference Resolution<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> System that would resolve such a mapping .
Given this qualification , we obtain positive results achieving 73.6 F-measure -LRB- 76.1 precision and 71.4 recall -RRB- for the task of assigning the correct UMLS concept to a given string .
Our results demonstrate that <tag name="TECHNIQUE" value="start"/>adaptive string similarity<tag name="TECHNIQUE" value="end"/> methods based on <tag name="TECHNIQUE" value="start"/>Conditional Random Fields<tag name="TECHNIQUE" value="end"/> outperform standard metrics in this domain .


##H05-1058
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Part-Of-Speech Tagging<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> Using <tag name="TECHNIQUE" value="start"/>Virtual Evidence And Negative Training<tag name="TECHNIQUE" value="end"/> .
We present a <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>part-of-speech tagger<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> which introduces two new concepts : <tag name="TECHNIQUE" value="start"/>virtual evidence<tag name="TECHNIQUE" value="end"/> in the form of an observed child node , and <tag name="TECHNIQUE" value="start"/>negative training<tag name="TECHNIQUE" value="end"/> data to learn the conditional probabilities for the observed child .
Associated with each word is a exible feature-set which can include binary ags , neighboring words , etc. .
The conditional probability of Tag given Word + Features is implemented using a <tag name="TECHNIQUE" value="start"/>factored language-model with back-off<tag name="TECHNIQUE" value="end"/> to avoid data sparsity problems .
This model remains within the framework of <tag name="TECHNIQUE" value="start"/>Dynamic Bayesian Networks<tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/>DBNs<tag name="TECHNIQUE" value="end"/> -RRB- and is conditionally-structured , but resolves the label bias problem inherent in the conditional Markov  model -LRB- CMM -RRB- .


##W04-3013
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Context Sensing<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Using <tag name="TECHNIQUE" value="start"/>Speech And Common Sense<tag name="TECHNIQUE" value="end"/> .
We present a method of inferring aspects of a person 's <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>context<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> by capturing <tag name="TECHNIQUE" value="start"/>conversation topics<tag name="TECHNIQUE" value="end"/> and using <tag name="TECHNIQUE" value="start"/>prior knowledge<tag name="TECHNIQUE" value="end"/> of human behavior .
This paper claims that topic-spotting performance can be improved by using a large database of <tag name="TECHNIQUE" value="start"/>common sense<tag name="TECHNIQUE" value="end"/> knowledge .
We describe two systems we built to infer <tag name="DOMAIN" value="start"/>context from noisy transcriptions of spoken conversations<tag name="DOMAIN" value="end"/> using <tag name="TECHNIQUE" value="start"/>common sense<tag name="TECHNIQUE" value="end"/> , and detail some preliminary results .
The GISTER system uses <tag name="TECHNIQUE" value="start"/>OMCSNet<tag name="TECHNIQUE" value="end"/> , a <tag name="TECHNIQUE" value="start"/>commonsense semantic network<tag name="TECHNIQUE" value="end"/> , to infer the most likely topics under discussion in a conversation stream .
The <tag name="FOCUS" value="start"/>OVERHEAR<tag name="FOCUS" value="end"/> system is built on top of GISTER , and distinguishes between aspects of the conversation that refer to past , present , and future events by using <tag name="TECHNIQUE" value="start"/>LifeNet<tag name="TECHNIQUE" value="end"/> , a <tag name="TECHNIQUE" value="start"/>probabilistic graphical<tag name="TECHNIQUE" value="end"/> model of human behavior , to help infer the events that occurred in each of those three time periods .
We conclude by discussing some of the future directions we may take this work .


##P04-3003
Constructing <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Transliteration Lexicons<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> From <tag name="TECHNIQUE" value="start"/>Web<tag name="TECHNIQUE" value="end"/> Corpora .
This paper proposes a novel approach to automating the construction of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>transliterated-term lexicons<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
A simple <tag name="TECHNIQUE" value="start"/>syllable alignment<tag name="TECHNIQUE" value="end"/> algorithm is used to construct <tag name="TECHNIQUE" value="start"/>confusion matrices<tag name="TECHNIQUE" value="end"/> for cross-language syllable-phoneme conversion .
Each row in the <tag name="TECHNIQUE" value="start"/>confusion matrix<tag name="TECHNIQUE" value="end"/> consists of a set of syllables in the source language that are -LRB- correctly or erroneously -RRB- matched <tag name="TECHNIQUE" value="start"/>phonetically<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>statistically<tag name="TECHNIQUE" value="end"/> to a syllable in the target language .
Two conversions using <tag name="TECHNIQUE" value="start"/>phoneme-to-phoneme<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>text-to-phoneme syllabification<tag name="TECHNIQUE" value="end"/> algorithms are automatically deduced from a training corpus of paired terms and are used to calculate the degree of similarity between phonemes for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>transliterated-term<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> extraction .
In a large-scale experiment using this automated learning process for conversions , more than 200,000 <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>transliterated-term<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> pairs were successfully extracted by analyzing query results from Internet search engines .
Experimental results indicate the proposed approach shows promise in <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>transliterated-term<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> extraction .


##N07-2039
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Reversible Sound-to-Letter\/Letter-to-Sound<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Modeling Based on <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Syllable Structure<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> .
This paper describes a new <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>grapheme-tophoneme<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> framework , based on a combination of formal <tag name="TECHNIQUE" value="start"/>linguistic<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>statistical<tag name="TECHNIQUE" value="end"/> methods .
A <tag name="TECHNIQUE" value="start"/>context-free grammar<tag name="TECHNIQUE" value="end"/> is used to <tag name="TECHNIQUE" value="start"/>parse<tag name="TECHNIQUE" value="end"/> words into their underlying <tag name="TECHNIQUE" value="start"/>syllable structure<tag name="TECHNIQUE" value="end"/> , and a set of subword `` spellneme '' units encoding both phonemic and graphemic information can be automatically derived from the parsed words .
A <tag name="TECHNIQUE" value="start"/>statistical a1 - gram<tag name="TECHNIQUE" value="end"/> model can then be trained on a large lexicon of words represented in terms of these linguistically motivated subword units .
The framework has potential applications in modeling unknown words and in linking spoken spellings with spoken pronunciations for fully automatic new-word acquisition via dialogue interaction .
Results are reported on <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>sound-to-letter<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> experiments for the nouns in the Phonebook corpus .


##W98-1117
A <tag name="TECHNIQUE" value="start"/>Maximum-Entropy<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Partial Parser<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> For Unrestricted Text .
This paper describes a <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>partial parser<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> that assigns syntactic structures to sequences of partof-speech tags .
The program uses the <tag name="TECHNIQUE" value="start"/>maximum entropy<tag name="TECHNIQUE" value="end"/> parameter estimation method , which Mlows a flexible combination of different knowledge sources : the <tag name="TECHNIQUE" value="start"/>hierarchical structure<tag name="TECHNIQUE" value="end"/> , <tag name="TECHNIQUE" value="start"/>parts of speech<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>phrasal categories<tag name="TECHNIQUE" value="end"/> .
In effect , the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>parser<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> goes beyond simple bracketing and recognizes even fairly complex structures .
We give accuracy figures for different applications of the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>parser<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .


##C92-4199
Recognizing <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Unregistered Names<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> For Mandarin <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Word Identification<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Word Identification<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> has been an important and active issue in Chinese Natural Language Processing .
In this paper , a new mechanism , based on the concept of <tag name="TECHNIQUE" value="start"/>sublanguage<tag name="TECHNIQUE" value="end"/> , is proposed for identifying <tag name="DOMAIN" value="start"/>unknown words<tag name="DOMAIN" value="end"/> , especially <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>personal names<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> , in Chinese newspapers .
The proposed mechanism includes title .
<tag name="TECHNIQUE" value="start"/>driven name recognition<tag name="TECHNIQUE" value="end"/> , <tag name="TECHNIQUE" value="start"/>adaptive dynamic word formation<tag name="TECHNIQUE" value="end"/> , identification of Z-character and 3-character Chinese names without title .
We will show the e ~ : perimental results for two corpora and compare them with the results by the NTIIU 's statistic-based system , the only system that we know has attacked the same problem .
The ezperimental results have shown significant improvements over the WI systems without the name identification capability .


##P96-1020
<tag name="TECHNIQUE" value="start"/> Pattern-Based Context-Free Grammars<tag name="TECHNIQUE" value="end"/> For <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Machine Translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
This paper proposes the use of <tag name="TECHNIQUE" value="start"/>`` patternbased '' context-free grammars<tag name="TECHNIQUE" value="end"/> as a basis for building <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>machine translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> -LRB- <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>MT<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> -RRB- systems , which are now being adopted as personal tools by a broad range of users in the cyberspace society .
We discuss major requirements for such tools , including easy customization for diverse domains , the efficiency of the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> algorithm , and scalability -LRB- incremental improvement in <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> quality through user interaction -RRB- , and describe how our approach meets these requirements .


##N09-3005
Using <tag name="TECHNIQUE" value="start"/>Language Modeling<tag name="TECHNIQUE" value="end"/> to <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Select Useful Annotation Data<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
An <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>annotation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> project typically has an abundant supply of unlabeled data that can be drawn from some corpus , but because the labeling process is expensive , it is helpful to pre-screen the pool of the candidate instances based on some criterion of future usefulness .
In many cases , that criterion is to improve the presence of the rare classes in the data to be annotated .
We propose a novel method for solving this problem and show that it compares favorably to a random sampling baseline and a clustering algorithm .


##P07-3016
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Clustering Hungarian Verbs<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> on the Basis of <tag name="TECHNIQUE" value="start"/>Complementation Patterns<tag name="TECHNIQUE" value="end"/> .
Our paper reports an attempt to apply an <tag name="TECHNIQUE" value="start"/>unsupervised clustering<tag name="TECHNIQUE" value="end"/> algorithm to a Hungarian treebank in order to obtain <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>semantic verb classes<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
Starting from the hypothesis that semantic metapredicates underlie verbs ' syntactic realization , we investigate how one can obtain <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>semantically motivated verb classes<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> by automatic means .
The 150 most frequent Hungarian verbs were clustered on the basis of their <tag name="TECHNIQUE" value="start"/>complementation patterns<tag name="TECHNIQUE" value="end"/> , yielding a set of basic classes and hints about the features that determine<tag name="DOMAIN" value="start"/> <tag name="FOCUS" value="start"/>verbal subcategorization<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
The resulting classes serve as a basis for the subsequent analysis of their alternation behavior .


##I05-5003
Using <tag name="TECHNIQUE" value="start"/>Machine Translation Evaluation<tag name="TECHNIQUE" value="end"/> Techniques to Determine <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Sentence-level Semantic Equivalence<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
The task of <tag name="TECHNIQUE" value="start"/>machine translation -LRB- MT -RRB- evaluation<tag name="TECHNIQUE" value="end"/> is closely related to the task of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>sentence-level semantic equivalence classification<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
This paper investigates the utility of applying standard <tag name="TECHNIQUE" value="start"/>MT evaluation<tag name="TECHNIQUE" value="end"/> methods -LRB- <tag name="TECHNIQUE" value="start"/>BLEU<tag name="TECHNIQUE" value="end"/> , <tag name="TECHNIQUE" value="start"/>NIST<tag name="TECHNIQUE" value="end"/> , <tag name="TECHNIQUE" value="start"/>WER<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>PER<tag name="TECHNIQUE" value="end"/> -RRB- to building classifiers to predict <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>semantic equivalence and entailment<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We also introduce a novel classification method based on <tag name="TECHNIQUE" value="start"/>PER<tag name="TECHNIQUE" value="end"/> which leverages <tag name="TECHNIQUE" value="start"/>part of speech<tag name="TECHNIQUE" value="end"/> information of the words contributing to the word matches and non-matches in the sentence .
Our results show that <tag name="TECHNIQUE" value="start"/>MT evaluation<tag name="TECHNIQUE" value="end"/> techniques are able to produce useful features for <tag name="DOMAIN" value="start"/>paraphrase classification<tag name="DOMAIN" value="end"/> and to a lesser extent <tag name="DOMAIN" value="start"/>entailment<tag name="DOMAIN" value="end"/> .
Our technique gives a substantial improvement in <tag name="DOMAIN" value="start"/>paraphrase classification<tag name="DOMAIN" value="end"/> accuracy over all of the other models used in the experiments .


##C04-1022
Automatic Learning Of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Language Model Structure<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
<tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Statistical<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>language modeling<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> remains a challenging task , in particular for morphologically rich languages .
Recently , new approaches based on <tag name="TECHNIQUE" value="start"/>factored<tag name="TECHNIQUE" value="end"/> language models have been developed to address this problem .
These models provide principled ways of including additional conditioning variables other than the preceding words , such as <tag name="TECHNIQUE" value="start"/>morphological<tag name="TECHNIQUE" value="end"/> or <tag name="TECHNIQUE" value="start"/>syntactic features<tag name="TECHNIQUE" value="end"/> .
However , the number of possible choices for model parameters creates a large space of models that can not be searched exhaustively .
This paper presents an entirely <tag name="TECHNIQUE" value="start"/>data-driven<tag name="TECHNIQUE" value="end"/> model selection procedure based on <tag name="TECHNIQUE" value="start"/>genetic search<tag name="TECHNIQUE" value="end"/> , which is shown to outperform both knowledge-based and random selection procedures on two di erent <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>language modeling<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> tasks -LRB- Arabic and Turkish -RRB- .


##N09-2016
Learning <tag name="TECHNIQUE" value="start"/>Bayesian Networks<tag name="TECHNIQUE" value="end"/> for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Semantic Frame Composition in a Spoken Dialog<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> System .
A <tag name="TECHNIQUE" value="start"/>stochastic<tag name="TECHNIQUE" value="end"/> approach based on <tag name="TECHNIQUE" value="start"/>Dynamic Bayesian Networks<tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/>DBNs<tag name="TECHNIQUE" value="end"/> -RRB- is introduced for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>spoken language understanding<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
<tag name="TECHNIQUE" value="start"/>DBN-based<tag name="TECHNIQUE" value="end"/> models allow to infer and then to compose <tag name="DOMAIN" value="start"/>semantic frame-based tree structures from speech transcriptions<tag name="DOMAIN" value="end"/> .
Experimental results on the French MEDIA dialog corpus show the appropriateness of the technique which both lead to good tree identification results and can provide the dialog system with n-best lists of scored hypotheses .


##D08-1008
<tag name="TECHNIQUE" value="start"/>Dependency-based<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Semantic Role Labeling<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> of PropBank .
We present a PropBank <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>semantic role labeling<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> system for English that is integrated with a <tag name="TECHNIQUE" value="start"/>dependency parser<tag name="TECHNIQUE" value="end"/> .
To tackle the problem of joint syntactic -- semantic analysis , the system relies on a syntactic and a semantic subcomponent .
The syntactic model is a <tag name="TECHNIQUE" value="start"/>projective parser<tag name="TECHNIQUE" value="end"/> using <tag name="TECHNIQUE" value="start"/>pseudo-projective transformations<tag name="TECHNIQUE" value="end"/> , and the semantic model uses <tag name="TECHNIQUE" value="start"/>global inference<tag name="TECHNIQUE" value="end"/> mechanisms on top of a pipeline of classifiers .
The complete syntactic -- semantic output is selected from a candidate pool generated by the subsystems .
We evaluate the system on the CoNLL2005 test sets using <tag name="TECHNIQUE" value="start"/>segment-based<tag name="TECHNIQUE" value="end"/> and <tag name="TECHNIQUE" value="start"/>dependency-based<tag name="TECHNIQUE" value="end"/> metrics .
Using the <tag name="TECHNIQUE" value="start"/>segment-based<tag name="TECHNIQUE" value="end"/> CoNLL-2005 metric , our system achieves a near state-of-the-art F1 figure of 77.97 on the WSJ+B rown test set , or 78.84 if punctuation is treated consistently .
Using a <tag name="TECHNIQUE" value="start"/>dependency-based<tag name="TECHNIQUE" value="end"/> metric , the F1 figure of our system is 84.29 on the test set from CoNLL-2008 .
Our system is the first <tag name="TECHNIQUE" value="start"/>dependency-based<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>semantic role labeler<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> for PropBank that rivals constituent-based systems in terms of performance .


##J98-4003
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Machine Transliteration<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
It is challenging to translate names and technical terms across languages with different alphabets and sound inventories .
These items are commonly transliterated , i.e. , replaced with approximate phonetic equivalents .
For example , `` computer '' in English comes out as `` konpyuutaa '' in Japanese .
Translating such items from Japanese back to English is even more challenging , and of practical interest , as transliterated items make up the bulk of text phrases not found in bilingual dictionaries .
We describe and evaluate a method for performing<tag name="DOMAIN" value="start"/> <tag name="FOCUS" value="start"/>backwards transliterations<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> by machine .
This method uses a <tag name="TECHNIQUE" value="start"/>generative<tag name="TECHNIQUE" value="end"/> model , incorporating several distinct stages in the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>transliteration<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> process .


##W09-0439
Stabilizing <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Minimum Error Rate Training<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> .
The most commonly used method for training feature weights in <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/> statistical<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/>machine translation -LRB- SMT -RRB-<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> systems is Och 's <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>minimum error rate training<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>MERT<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> -RRB- procedure .
Awell-knownproblemwithOch 's procedure is that it tends to be sensitive to small changes in the system , particularly when the number of features is large .
In this paper , we quantify the stability of Och 's procedure by supplying different random seeds to a core component of the procedure -LRB- Powell 's algorithm -RRB- .
We show that for systems with many features , there is extensive variation in outcomes , both on the development data and on the test data .
Weanalyzethecausesofthisvariationand proposemodificationstotheMERTprocedure that improve stability while helping performance on test data .


##I05-1001
A New Method for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Sentiment Classification<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> in Text Retrieval .
Traditional text categorization is usually a topic-based task , but a subtle demand on information retrieval is to distinguish between positive and negative view on text topic .
In this paper , a new method is explored to solve this problem .
Firstly , a batch of <tag name="TECHNIQUE" value="start"/>Concerned Concepts<tag name="TECHNIQUE" value="end"/> in the researched domain is predefined .
Secondly , the special knowledge representing the positive or negative <tag name="TECHNIQUE" value="start"/> context<tag name="TECHNIQUE" value="end"/> of these concepts within sentences is built up .
At last , an evaluating function based on the knowledge is defined for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>sentiment classification<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> of free text .
We introduce some <tag name="TECHNIQUE" value="start"/>linguistic knowledge<tag name="TECHNIQUE" value="end"/> in these procedures to make our method effective .
As a result , the new method proves better compared with <tag name="TECHNIQUE" value="start"/>SVM<tag name="TECHNIQUE" value="end"/> when experimenting on Chinese texts about a certain topic .


##W06-3811
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Synonym Extraction<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Using A <tag name="TECHNIQUE" value="start"/>Semantic Distance On A Dictionary<tag name="TECHNIQUE" value="end"/> .
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Synonyms extraction<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> is a difficult task to achieve and evaluate .
Some studies have tried to exploit general dictionaries for that purpose , seeing them as graphs where words are related by the definition they appear in , in a complex network of an arguably semantic nature .
The advantage of using a general <tag name="TECHNIQUE" value="start"/>dictionary<tag name="TECHNIQUE" value="end"/> lies in the coverage , and the availability of such resources , in general and also in specialised domains .
We present here a method exploiting such a <tag name="TECHNIQUE" value="start"/>graph structure<tag name="TECHNIQUE" value="end"/> to compute a distance between words .
This distance is used to isolate candidate <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>synonyms<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> for a given word .
We present an evaluation of the relevance of the candidates on a sample of the lexicon .


##C94-1027
<tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Part-Of-Speech Tagging<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> With <tag name="TECHNIQUE" value="start"/>Neural Networks<tag name="TECHNIQUE" value="end"/> .
Text corpora which are tagged with <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>part-of-speech<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> information are useful in many areas of linguistic research .
In this paper , a new <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>part-of-speech tagging<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> method hased on <tag name="TECHNIQUE" value="start"/>neural networks<tag name="TECHNIQUE" value="end"/> -LRB- <tag name="FOCUS" value="start"/>Net-Tagger<tag name="FOCUS" value="end"/> -RRB- is presented and its performance is compared to that of a llMM-tagger -LRB- Cutting et al. , 1992 -RRB- and a trigrambased tagger -LRB- Kempe , 1993 -RRB- .
It is shown that the <tag name="FOCUS" value="start"/>Net-Tagger<tag name="FOCUS" value="end"/> performs as well as the trigram-based tagger and better than the iIMM-tagger .


##P05-1046
<tag name="TECHNIQUE" value="start"/> Unsupervised Learning <tag name="TECHNIQUE" value="end"/> Of <tag name="DOMAIN" value="start"/> Field Segmentation<tag name="DOMAIN" value="end"/> Models For <tag name="DOMAIN" value="start"/>Information Extraction<tag name="DOMAIN" value="end"/> .
The applicability of many current <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>information extraction<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> techniques is severely limited by the need for supervised training data .
We demonstrate that for certain <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>field structured extraction<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> tasks , such as classified advertisements and bibliographic citations , small amounts of prior knowledge can be used to learn effective models in a primarily unsupervised fashion .
Although hidden Markov models -LRB- HMMs -RRB- provide a suitable generative model for field structured text , general unsupervised HMM learning fails to learn useful structure in either of our domains .
However , one can dramatically improve the quality of the learned structure by exploiting simple prior knowledge of the desired solutions .
In both domains , we found that <tag name="TECHNIQUE" value="start"/>unsupervised<tag name="TECHNIQUE" value="end"/> methods can attain accuracies with 400 unlabeled examples comparable to those attained by supervised methods on 50 labeled examples , and that <tag name="TECHNIQUE" value="start"/>semi-supervised<tag name="TECHNIQUE" value="end"/> methods can make good use of small amounts of labeled data .


##W03-1014
Learning <tag name="TECHNIQUE" value="start"/>Extraction Patterns<tag name="TECHNIQUE" value="end"/> For <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Subjective Expressions<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
This paper presents a <tag name="TECHNIQUE" value="start"/>bootstrapping<tag name="TECHNIQUE" value="end"/> process that learns linguistically rich <tag name="TECHNIQUE" value="start"/>extraction patterns<tag name="TECHNIQUE" value="end"/> for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>subjective -LRB- opinionated -RRB- expressions<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
High-precision classifiers label unannotated data to automatically create a large training set , which is then given to an <tag name="TECHNIQUE" value="start"/>extraction pattern learning<tag name="TECHNIQUE" value="end"/> algorithm .
The <tag name="TECHNIQUE" value="start"/>learned patterns<tag name="TECHNIQUE" value="end"/> are then used to identify more <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>subjective sentences<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
The <tag name="TECHNIQUE" value="start"/>bootstrapping<tag name="TECHNIQUE" value="end"/> process learns many subjective <tag name="TECHNIQUE" value="start"/>patterns<tag name="TECHNIQUE" value="end"/> and increases recall while maintaining high precision .


##P95-1038
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Evaluation Of Semantic Clusters<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
<tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>Semantic clusters<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> of a domain form an important feature that can be useful for performing syntactic and semantic disambiguation .
Several attempts have been made to extract the semantic clusters of a domain by probabilistic or taxonomic techniques .
However , not much progress has been made in evaluating the obtained semantic clusters .
This paper focuses on an <tag name="FOCUS" value="start"/><tag name="DOMAIN" value="start"/>evaluation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> mechanism that can be used to <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>evaluate semantic clusters<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> produced by a system against those provided by human experts .


##P98-1017
An Efficient <tag name="TECHNIQUE" value="start"/>Kernel<tag name="TECHNIQUE" value="end"/> for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Multilingual Generation in Speech-to-Speech Dialogue Translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
We present core aspects of a fully implemented <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>generation component in a multilingual speechto-speech dialogue translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> system .
Its design was particularly influenced by the necessity of real-time processing and usability for multiple languages and domains .
We developed a general <tag name="TECHNIQUE" value="start"/>kernel<tag name="TECHNIQUE" value="end"/> system comprising a <tag name="TECHNIQUE" value="start"/>microplanning<tag name="TECHNIQUE" value="end"/> and a <tag name="TECHNIQUE" value="start"/>syntactic realizer<tag name="TECHNIQUE" value="end"/> module .
Tile microplanner performs <tag name="TECHNIQUE" value="start"/>lexical and syntactic choice<tag name="TECHNIQUE" value="end"/> , based on <tag name="TECHNIQUE" value="start"/>constraint-satisfaction<tag name="TECHNIQUE" value="end"/> techniques .
The <tag name="TECHNIQUE" value="start"/>syntactic realizer<tag name="TECHNIQUE" value="end"/> processes <tag name="TECHNIQUE" value="start"/>HPSG grammars<tag name="TECHNIQUE" value="end"/> reflecting the latest developments of the underlying linguistic theory , utilizing their pre-processing into the TAG formalism .
The declarative nature of the knowledge bases , i.e. , the <tag name="TECHNIQUE" value="start"/>microplanning constraints<tag name="TECHNIQUE" value="end"/> and the <tag name="TECHNIQUE" value="start"/>HPSG grammars<tag name="TECHNIQUE" value="end"/> allowed an easy adaption to new domains and languages .
The successful integration of our component into the translation system Verbmobil proved the fulfillment of the specific real-time constraints .


##P08-1084
<tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Unsupervised Multilingual Learning<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> for <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Morphological Segmentation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
For centuries , the deep connection between languages has brought about major discoveries about human communication .
In this paper we investigate how this powerful source of information can be exploited for <tag name="FOCUS" value="start"/>unsupervised language learning<tag name="FOCUS" value="end"/> .
In particular , we study the task of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>morphological segmentation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> of multiple languages .
We present a <tag name="TECHNIQUE" value="start"/>nonparametric Bayesian<tag name="TECHNIQUE" value="end"/> model that jointly induces <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>morpheme segmentations<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> of each language under consideration and at the same time identifies cross-lingual morpheme patterns , or abstract morphemes .
We apply our modeltothreeSemiticlanguages : Arabic , Hebrew , Aramaic , as well as to English .
Our results demonstrate that learning <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>morphological<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> models in tandem reduces error by up to 24 % relative to monolingual models .
Furthermore , we provide evidence that our joint model achieves better performance when applied to languages from the same family .


##P09-1067
<tag name="TECHNIQUE" value="start"/>Variational Decoding<tag name="TECHNIQUE" value="end"/> for <tag name="FOCUS" value="start"/><tag name="TECHNIQUE" value="start"/>Statistical <tag name="TECHNIQUE" value="end"/><tag name="DOMAIN" value="start"/>Machine Translation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
<tag name="TECHNIQUE" value="start"/>Statistical<tag name="TECHNIQUE" value="end"/> models in <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>machine translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> exhibit spurious ambiguity .
That is , the probability of an output string is split among many distinct derivations -LRB- e.g. , trees or segmentations -RRB- .
In principle , the goodness of a string is measured by the total probability of its many derivations .
However , finding the best string -LRB- e.g. , during decoding -RRB- is then computationally intractable .
Therefore , most systems use a simple Viterbi approximation that measures the goodness of a string using only its most probable derivation .
Instead , we develop a <tag name="TECHNIQUE" value="start"/>variational approximation<tag name="TECHNIQUE" value="end"/> , which considers all the derivations but still allows tractable decoding .
Our particular <tag name="TECHNIQUE" value="start"/>variational distributions<tag name="TECHNIQUE" value="end"/> are parameterized as <tag name="TECHNIQUE" value="start"/>n-gram<tag name="TECHNIQUE" value="end"/> models .
We also analytically show that interpolating thesen-gram models for different n is similar to minimumrisk decoding for BLEU -LRB- Tromble et al. , 2008 -RRB- .
Experiments show that our approach improves the state of the art .


##W04-1607
<tag name="TECHNIQUE" value="start"/>Finite-State<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Morphological<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Analysis Of Persian .
This paper describes a two-level <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>morphological<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> analyzer for Persian using a system based on the <tag name="TECHNIQUE" value="start"/>Xerox finite state<tag name="TECHNIQUE" value="end"/> tools .
Persian language presents certain challenges to computational analysis : There is a complex verbal conjugation paradigm which includes long-distance morphological dependencies ; phonological alternations apply at morpheme boundaries ; word and noun phrase boundaries are difficult to define since morphemes may be detached from their stems and distinct words can appear without an intervening space .
In this work , we develop these problems and provide solutions in a <tag name="TECHNIQUE" value="start"/>finitestate<tag name="TECHNIQUE" value="end"/> <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>morphology<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> system .


##C96-2159
<tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Decision Tree Learning<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> Algorithm With <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Structured Attributes<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> : Application To <tag name="DOMAIN" value="start"/>Verbal Case Frame Acquisition<tag name="DOMAIN" value="end"/> .
The <tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Decision Tree Learning<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> Algorithms -LRB- <tag name="TECHNIQUE" value="start"/>DTLAs<tag name="TECHNIQUE" value="end"/> -RRB- are getting keen attention from the natural language processing research comlnunity , and there have been a series of attempts to apply them to <tag name="DOMAIN" value="start"/>verbal case frame acquisition<tag name="DOMAIN" value="end"/> .
However , a <tag name="TECHNIQUE" value="start"/>DTLA<tag name="TECHNIQUE" value="end"/> can not handle structured attributes like nouns , which are classified under a thesaurus .
In this paper , we present a new <tag name="TECHNIQUE" value="start"/>DTLA<tag name="TECHNIQUE" value="end"/> that can rationally handle the structured attributes .
In the process of tree generation , the algorithm generalizes each attribute optimally using a given <tag name="TECHNIQUE" value="start"/>thesaurus<tag name="TECHNIQUE" value="end"/> .
We apply this algorithm to a bilingual corpus and show that it successfiflly learned a <tag name="TECHNIQUE" value="start"/>generalized decision tree<tag name="TECHNIQUE" value="end"/> for <tag name="DOMAIN" value="start"/>classifying the verb<tag name="DOMAIN" value="end"/> `` take '' and that the tree was smaller with more prediction power on the open data than the tree learned by the conventional DTLA .


##C04-1030
<tag name="TECHNIQUE" value="start"/><tag name="FOCUS" value="start"/>Reordering Constraints<tag name="FOCUS" value="end"/><tag name="TECHNIQUE" value="end"/> For <tag name="FOCUS" value="start"/>Phrase-Based Statistical <tag name="DOMAIN" value="start"/>Machine Translation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> .
In <tag name="FOCUS" value="start"/>statistical  <tag name="DOMAIN" value="start"/>machine translation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> , the generation of a translation hypothesis is computationally expensive .
If arbitrary reorderings are permitted , the search problem is NP-hard .
On the other hand , if we restrict the possible reorderings in an appropriate way , we obtain a polynomial-time search algorithm .
We investigate different <tag name="TECHNIQUE" value="start"/>reordering constraints<tag name="TECHNIQUE" value="end"/> for <tag name="FOCUS" value="start"/>phrase-based statistical <tag name="DOMAIN" value="start"/>machine translation<tag name="DOMAIN" value="end"/><tag name="FOCUS" value="end"/> , namely the <tag name="TECHNIQUE" value="start"/>IBM constraints<tag name="TECHNIQUE" value="end"/> and the <tag name="TECHNIQUE" value="start"/>ITG constraints<tag name="TECHNIQUE" value="end"/> .
We present efficient <tag name="TECHNIQUE" value="start"/>dynamic programming<tag name="TECHNIQUE" value="end"/> algorithms for both constraints .
We evaluate the constraints with respect to <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> quality on two Japanese -- English tasks .
We show that the <tag name="TECHNIQUE" value="start"/>reordering constraints<tag name="TECHNIQUE" value="end"/> improve <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>translation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> quality compared to an unconstrained search that permits arbitrary <tag name="TECHNIQUE" value="start"/>phrase reorderings<tag name="TECHNIQUE" value="end"/> .
The <tag name="TECHNIQUE" value="start"/>ITG constraints<tag name="TECHNIQUE" value="end"/> preform best on both tasks and yield statistically significant improvements compared to the unconstrained search .


##J96-3003
Efficient <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Multilingual Phoneme-To-Grapheme Conversion<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Based On <tag name="TECHNIQUE" value="start"/>HMM<tag name="TECHNIQUE" value="end"/> .
Grapheme-to-phoneme conversion -LRB- GTPC -RRB- has been achieved in most European languagesby dictionary look-up or using rules .
The application of these methods , however , in the reverse process , -LRB- i.e. , in <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>phoneme-to-grapheme conversion<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> -LRB- <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>PTGC<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> -RRB- -RRB- creates serious problems , especially in inflectionally rich languages .
In this paper the <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>PTGC<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> problem is approached from a completely different point of view .
Instead of rules or a dictionary , the statistics of language connecting pronunciation to spelling are exploited .
The novelty lies in modeling the natural language <tag name="TECHNIQUE" value="start"/>intraword features<tag name="TECHNIQUE" value="end"/> using the theory of <tag name="TECHNIQUE" value="start"/>hidden Markov models<tag name="TECHNIQUE" value="end"/> -LRB- <tag name="TECHNIQUE" value="start"/>HMM<tag name="TECHNIQUE" value="end"/> -RRB- and performing the conversion using the <tag name="TECHNIQUE" value="start"/>Viterbi<tag name="TECHNIQUE" value="end"/> algorithm .
The <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>PTGC<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> system has been established and tested on various multilingual corpora .
Initially , the first-order <tag name="TECHNIQUE" value="start"/>HMM<tag name="TECHNIQUE" value="end"/> and the common <tag name="TECHNIQUE" value="start"/>Viterbi<tag name="TECHNIQUE" value="end"/> algorithm were used to obtain a single transcription for each word .
Afterwards , the second-order <tag name="TECHNIQUE" value="start"/>HMM<tag name="TECHNIQUE" value="end"/> and the <tag name="TECHNIQUE" value="start"/>N-best<tag name="TECHNIQUE" value="end"/> algorithm adapted to <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>PTGC<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> were implemented to provide one or more transcriptions for each word input -LRB- homophones -RRB- .
This system gave an average score of more than 99 % correctly transcribed words -LRB- overall success in the first four candidates -RRB- for most of the seven languages it was tested on -LRB- Dutch , English , French , German , Greek , Italian , and Spanish -RRB- .
The system can be adapted to almost any language with little effort and can be implemented in hardware to serve in real-time speech recognition systems .


##W06-2207
A <tag name="TECHNIQUE" value="start"/>Hybrid<tag name="TECHNIQUE" value="end"/> Approach For The Acquisition Of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Information Extraction Patterns<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> .
In this paper we present a <tag name="TECHNIQUE" value="start"/>hybrid<tag name="TECHNIQUE" value="end"/> approach for the acquisition of <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>syntacticosemantic patterns<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> from raw text .
Our approach <tag name="TECHNIQUE" value="start"/>co-trains a decision list learner<tag name="TECHNIQUE" value="end"/> whose feature space covers the set of all <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>syntactico-semantic patterns<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> with an <tag name="TECHNIQUE" value="start"/>Expectation Maximization clustering<tag name="TECHNIQUE" value="end"/> algorithm that uses the text words as attributes .
We show that the combination of the two methods always outperforms the <tag name="TECHNIQUE" value="start"/>decision list learner<tag name="TECHNIQUE" value="end"/> alone .
Furthermore , using a <tag name="TECHNIQUE" value="start"/>modular architecture<tag name="TECHNIQUE" value="end"/> we investigate several algorithms for <tag name="DOMAIN" value="start"/>pattern ranking<tag name="DOMAIN" value="end"/> , the most important component of the <tag name="TECHNIQUE" value="start"/>decision list learner<tag name="TECHNIQUE" value="end"/> .


##I05-1066
Automatic <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>Slide Generation<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> Based on <tag name="TECHNIQUE" value="start"/>Discourse Structure Analysis<tag name="TECHNIQUE" value="end"/> .
In this paper , we describe a method of automatically <tag name="DOMAIN" value="start"/><tag name="FOCUS" value="start"/>generating summary slides<tag name="FOCUS" value="end"/><tag name="DOMAIN" value="end"/> from a text .
The slides are generated by itemizing <tag name="TECHNIQUE" value="start"/>topic\/non-topic<tag name="TECHNIQUE" value="end"/> parts that are extracted from the text based on <tag name="TECHNIQUE" value="start"/>syntactic\/case<tag name="TECHNIQUE" value="end"/> analysis .
The indentations of the items are controlled according to the <tag name="TECHNIQUE" value="start"/>discourse structure<tag name="TECHNIQUE" value="end"/> , which is detected by <tag name="TECHNIQUE" value="start"/>cue phrases<tag name="TECHNIQUE" value="end"/> , identi cation of <tag name="TECHNIQUE" value="start"/>word chain<tag name="TECHNIQUE" value="end"/> and similarity between two sentences .
Our experiments demonstrates generated slides are far easier to read in comparison with original texts .

