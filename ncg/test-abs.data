We	O
present	O
a	O
new	O
approach	O
for	O
pretraining	B-RESEARCH_PROBLEM
a	I-RESEARCH_PROBLEM
bi-directional	I-RESEARCH_PROBLEM
transformer	I-RESEARCH_PROBLEM
model	E-RESEARCH_PROBLEM
that	O
provides	O
significant	O
performance	O
gains	O
across	O
a	O
variety	O
of	O
language	O
understanding	O
problems	O
.	O
Our	O
model	O
solves	O
a	O
cloze	O
-	O
style	O
word	O
reconstruction	O
task	O
,	O
where	O
each	O
word	O
is	O
ablated	O
and	O
must	O
be	O
predicted	O
given	O
the	O
rest	O
of	O
the	O
text	O
.	O
Experiments	O
demonstrate	O
large	O
performance	O
gains	O
on	O
GLUE	O
and	O
new	O
state	O
of	O
the	O
art	O
results	O
on	O
NER	O
as	O
well	O
as	O
constituency	O
parsing	O
benchmarks	O
,	O
consistent	O
with	O
the	O
concurrently	O
introduced	O
BERT	O
model	O
.	O
We	O
also	O
present	O
a	O
detailed	O
analysis	O
of	O
a	O
number	O
of	O
factors	O
that	O
contribute	O
to	O
effective	O
pretraining	O
,	O
including	O
data	O
domain	O
and	O
size	O
,	O
model	O
capacity	O
,	O
and	O
variations	O
on	O
the	O
cloze	O
objective	O
.	O

Syntactic	B-RESEARCH_PROBLEM
constituency	I-RESEARCH_PROBLEM
parsing	E-RESEARCH_PROBLEM
is	O
a	O
fundamental	O
problem	O
in	O
natural	O
language	O
processing	O
and	O
has	O
been	O
the	O
subject	O
of	O
intensive	O
research	O
and	O
engineering	O
for	O
decades	O
.	O
As	O
a	O
result	O
,	O
the	O
most	O
accurate	O
parsers	O
are	O
domain	O
specific	O
,	O
complex	O
,	O
and	O
inefficient	O
.	O
In	O
this	O
paper	O
we	O
show	O
that	O
the	O
domain	O
agnostic	O
attention	O
-	O
enhanced	O
sequence	O
-	O
to	O
-	O
sequence	O
model	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
the	O
most	O
widely	O
used	O
syntactic	O
constituency	O
parsing	O
dataset	O
,	O
when	O
trained	O
on	O
a	O
large	O
synthetic	O
corpus	O
that	O
was	O
annotated	O
using	O
existing	O
parsers	O
.	O
It	O
also	O
matches	O
the	O
performance	O
of	O
standard	O
parsers	O
when	O
trained	O
only	O
on	O
a	O
small	O
human	O
-	O
annotated	O
dataset	O
,	O
which	O
shows	O
that	O
this	O
model	O
is	O
highly	O
data	O
-	O
efficient	O
,	O
in	O
contrast	O
to	O
sequence	O
-	O
to	O
-	O
sequence	O
models	O
without	O
the	O
attention	O
mechanism	O
.	O
Our	O
parser	O
is	O
also	O
fast	O
,	O
processing	O
over	O
a	O
hundred	O
sentences	O
per	O
second	O
with	O
an	O
unoptimized	O
CPU	O
implementation	O
.	O
*	O
Equal	O
contribution	O
1	O
arXiv:1412.7449v3	O
[	O
cs.	O
CL	O
]	O
9	O
Jun	O
2015	O

Recent	O
work	O
has	O
proposed	O
several	O
generative	O
neural	O
models	O
for	O
constituency	B-RESEARCH_PROBLEM
parsing	E-RESEARCH_PROBLEM
that	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
.	O
Since	O
direct	O
search	O
in	O
these	O
generative	O
models	O
is	O
difficult	O
,	O
they	O
have	O
primarily	O
been	O
used	O
to	O
rescore	O
candidate	O
outputs	O
from	O
base	O
parsers	O
in	O
which	O
decoding	O
is	O
more	O
straightforward	O
.	O
We	O
first	O
present	O
an	O
algorithm	O
for	O
direct	O
search	O
in	O
these	O
generative	O
models	O
.	O
We	O
then	O
demonstrate	O
that	O
the	O
rescoring	O
results	O
are	O
at	O
least	O
partly	O
due	O
to	O
implicit	O
model	O
combination	O
rather	O
than	O
reranking	O
effects	O
.	O
Finally	O
,	O
we	O
show	O
that	O
explicit	O
model	O
combination	O
can	O
improve	O
performance	O
even	O
further	O
,	O
resulting	O
in	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
numbers	O
on	O
the	O
PTB	O
of	O
94.25	O
F1	O
when	O
training	O
only	O
on	O
gold	O
data	O
and	O
94.66	O
F1	O
when	O
using	O
external	O
data	O
.	O

Both	O
bottom	O
-	O
up	O
and	O
top	O
-	O
down	O
strategies	O
have	O
been	O
used	O
for	O
neural	B-RESEARCH_PROBLEM
transition	I-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
based	I-RESEARCH_PROBLEM
constituent	I-RESEARCH_PROBLEM
parsing	E-RESEARCH_PROBLEM
.	O
The	O
parsing	O
strategies	O
differ	O
in	O
terms	O
of	O
the	O
order	O
in	O
which	O
they	O
recognize	O
productions	O
in	O
the	O
derivation	O
tree	O
,	O
where	O
bottom	O
-	O
up	O
strategies	O
and	O
top	O
-	O
down	O
strategies	O
take	O
post-order	O
and	O
pre-order	O
traversal	O
over	O
trees	O
,	O
respectively	O
.	O
Bottom	O
-	O
up	O
parsers	O
benefit	O
from	O
rich	O
features	O
from	O
readily	O
built	O
partial	O
parses	O
,	O
but	O
lack	O
lookahead	O
guidance	O
in	O
the	O
parsing	O
process	O
;	O
top	O
-	O
down	O
parsers	O
benefit	O
from	O
non-local	O
guidance	O
for	O
local	O
decisions	O
,	O
but	O
rely	O
on	O
a	O
strong	O
encoder	O
over	O
the	O
input	O
to	O
predict	O
a	O
constituent	O
hierarchy	O
before	O
its	O
construction	O
.	O
To	O
mitigate	O
both	O
issues	O
,	O
we	O
propose	O
a	O
novel	O
parsing	O
system	O
based	O
on	O
in	O
-	O
order	O
traversal	O
over	O
syntactic	O
trees	O
,	O
designing	O
a	O
set	O
of	O
transition	O
actions	O
to	O
find	O
a	O
compromise	O
between	O
bottom	O
-	O
up	O
constituent	O
information	O
and	O
top	O
-	O
down	O
lookahead	O
information	O
.	O
Based	O
on	O
stack	O
-	O
LSTM	O
,	O
our	O
psycholinguistically	O
motivated	O
constituent	O
parsing	O
system	O
achieves	O
91.8	O
F	O
1	O
on	O
the	O
WSJ	O
benchmark	O
.	O
Furthermore	O
,	O
the	O
system	O
achieves	O
93.6	O
F	O
1	O
with	O
supervised	O
reranking	O
and	O
94.2	O
F	O
1	O
with	O
semi-supervised	O
reranking	O
,	O
which	O
are	O
the	O
best	O
results	O
on	O
the	O
WSJ	O
benchmark	O
.	O

We	O
recast	O
syntactic	B-RESEARCH_PROBLEM
parsing	E-RESEARCH_PROBLEM
as	O
a	O
language	O
modeling	O
problem	O
and	O
use	O
recent	O
advances	O
in	O
neural	O
network	O
language	O
modeling	O
to	O
achieve	O
a	O
new	O
state	O
of	O
the	O
art	O
for	O
constituency	O
Penn	O
Treebank	O
parsing	O
-	O
93.8	O
F	O
1	O
on	O
section	O
23	O
,	O
using	O
2	O
-	O
21	O
as	O
training	O
,	O
24	O
as	O
development	O
,	O
plus	O
tri-training	O
.	O
When	O
trees	O
are	O
converted	O
to	O
Stanford	O
dependencies	O
,	O
UAS	O
and	O
LAS	O
are	O
95.9	O
%	O
and	O
94.1	O
%	O
.	O

Coreference	B-RESEARCH_PROBLEM
resolution	E-RESEARCH_PROBLEM
systems	O
are	O
typically	O
trained	O
with	O
heuristic	O
loss	O
functions	O
that	O
require	O
careful	O
tuning	O
.	O
In	O
this	O
paper	O
we	O
instead	O
apply	O
reinforcement	O
learning	O
to	O
directly	O
optimize	O
a	O
neural	O
mention	O
-	O
ranking	O
model	O
for	O
coreference	O
evaluation	O
metrics	O
.	O
We	O
experiment	O
with	O
two	O
approaches	O
:	O
the	O
REINFORCE	O
policy	O
gradient	O
algorithm	O
and	O
a	O
rewardrescaled	O
max	O
-	O
margin	O
objective	O
.	O
We	O
find	O
the	O
latter	O
to	O
be	O
more	O
effective	O
,	O
resulting	O
in	O
significant	O
improvements	O
over	O
the	O
current	O
state	O
-	O
of	O
the	O
-	O
art	O
on	O
the	O
English	O
and	O
Chinese	O
portions	O
of	O
the	O
CoNLL	O
2012	O
Shared	O
Task	O
.	O
1	O
Code	O
and	O
trained	O
models	O
are	O
available	O
at	O
https	O

In	O
this	O
work	O
,	O
we	O
present	O
a	O
word	O
embedding	O
model	O
that	O
learns	O
cross	O
-	O
sentence	O
dependency	O
for	O
improving	O
end	O
-	O
to	O
-	O
end	O
co-reference	O
resolution	O
(	O
E2E	B-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
CR	E-RESEARCH_PROBLEM
)	O
.	O
While	O
the	O
traditional	O
E2E	B-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
CR	E-RESEARCH_PROBLEM
model	O
generates	O
word	O
representations	O
by	O
running	O
long	O
short	O
-	O
term	O
memory	O
(	O
LSTM	O
)	O
recurrent	O
neural	O
networks	O
on	O
each	O
sentence	O
of	O
an	O
input	O
article	O
or	O
conversation	O
separately	O
,	O
we	O
propose	O
linear	O
sentence	O
linking	O
and	O
attentional	O
sentence	O
linking	O
models	O
to	O
learn	O
crosssentence	O
dependency	O
.	O
Both	O
sentence	O
linking	O
strategies	O
enable	O
the	O
LSTMs	O
to	O
make	O
use	O
of	O
valuable	O
information	O
from	O
context	O
sentences	O
while	O
calculating	O
the	O
representation	O
of	O
the	O
current	O
input	O
word	O
.	O
With	O
this	O
approach	O
,	O
the	O
LSTMs	O
learn	O
word	O
embeddings	O
considering	O
knowledge	O
not	O
only	O
from	O
the	O
current	O
sentence	O
but	O
also	O
from	O
the	O
entire	O
input	O
document	O
.	O
Experiments	O
show	O
that	O
learning	O
cross	O
-	O
sentence	O
dependency	O
enriches	O
information	O
contained	O
by	O
the	O
word	O
representations	O
,	O
and	O
improves	O
the	O
performance	O
of	O
the	O
co-reference	O
resolution	O
model	O
compared	O
with	O
our	O
baseline	O
.	O

We	O
introduce	O
the	O
first	O
end	B-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
to	I-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
end	I-RESEARCH_PROBLEM
coreference	I-RESEARCH_PROBLEM
resolution	E-RESEARCH_PROBLEM
model	O
and	O
show	O
that	O
it	O
significantly	O
outperforms	O
all	O
previous	O
work	O
without	O
using	O
a	O
syntactic	O
parser	O
or	O
handengineered	O
mention	O
detector	O
.	O
The	O
key	O
idea	O
is	O
to	O
directly	O
consider	O
all	O
spans	O
in	O
a	O
document	O
as	O
potential	O
mentions	O
and	O
learn	O
distributions	O
over	O
possible	O
antecedents	O
for	O
each	O
.	O
The	O
model	O
computes	O
span	O
embeddings	O
that	O
combine	O
context	O
-	O
dependent	O
boundary	O
representations	O
with	O
a	O
headfinding	O
attention	O
mechanism	O
.	O
It	O
is	O
trained	O
to	O
maximize	O
the	O
marginal	O
likelihood	O
of	O
gold	O
antecedent	O
spans	O
from	O
coreference	O
clusters	O
and	O
is	O
factored	O
to	O
enable	O
aggressive	O
pruning	O
of	O
potential	O
mentions	O
.	O
Experiments	O
demonstrate	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
,	O
with	O
again	O
of	O
1.5	O
F1	O
on	O
the	O
OntoNotes	O
benchmark	O
and	O
by	O
3.1	O
F1	O
using	O
a	O
5	O
-	O
model	O
ensemble	O
,	O
despite	O
the	O
fact	O
that	O
this	O
is	O
the	O
first	O
approach	O
to	O
be	O
successfully	O
trained	O
with	O
no	O
external	O
resources	O
.	O

Transcribing	B-RESEARCH_PROBLEM
structured	I-RESEARCH_PROBLEM
data	I-RESEARCH_PROBLEM
into	I-RESEARCH_PROBLEM
natural	I-RESEARCH_PROBLEM
language	I-RESEARCH_PROBLEM
descriptions	E-RESEARCH_PROBLEM
has	O
emerged	O
as	O
a	O
challenging	O
task	O
,	O
referred	O
to	O
as	O
"	O
data	B-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
to	I-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
text	E-RESEARCH_PROBLEM
"	O
.	O
These	O
structures	O
generally	O
regroup	O
multiple	O
elements	O
,	O
as	O
well	O
as	O
their	O
attributes	O
.	O
Most	O
attempts	O
rely	O
on	O
translation	O
encoder	O
-	O
decoder	O
methods	O
which	O
linearize	O
elements	O
into	O
a	O
sequence	O
.	O
This	O
however	O
loses	O
most	O
of	O
the	O
structure	O
contained	O
in	O
the	O
data	O
.	O
In	O
this	O
work	O
,	O
we	O
propose	O
to	O
overpass	O
this	O
limitation	O
with	O
a	O
hierarchical	O
model	O
that	O
encodes	O
the	O
data	O
-	O
structure	O
at	O
the	O
element	O
-	O
level	O
and	O
the	O
structure	O
level	O
.	O
Evaluations	O
on	O
RotoWire	O
show	O
the	O
effectiveness	O
of	O
our	O
model	O
w.r.t.	O
qualitative	O
and	O
quantitative	O
metrics	O
.	O

Natural	B-RESEARCH_PROBLEM
language	I-RESEARCH_PROBLEM
generation	E-RESEARCH_PROBLEM
lies	O
at	O
the	O
core	O
of	O
generative	O
dialogue	O
systems	O
and	O
conversational	O
agents	O
.	O
We	O
describe	O
an	O
ensemble	O
neural	O
language	O
generator	O
,	O
and	O
present	O
several	O
novel	O
methods	O
for	O
data	O
representation	O
and	O
augmentation	O
that	O
yield	O
improved	O
results	O
in	O
our	O
model	O
.	O
We	O
test	O
the	O
model	O
on	O
three	O
datasets	O
in	O
the	O
restaurant	O
,	O
TV	O
and	O
laptop	O
domains	O
,	O
and	O
report	O
both	O
objective	O
and	O
subjective	O
evaluations	O
of	O
our	O
best	O
model	O
.	O
Using	O
a	O
range	O
of	O
automatic	O
metrics	O
,	O
as	O
well	O
as	O
human	O
evaluators	O
,	O
we	O
show	O
that	O
our	O
approach	O
achieves	O
better	O
results	O
than	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
on	O
the	O
same	O
datasets	O
.	O

Most	O
previous	O
work	O
on	O
neural	B-RESEARCH_PROBLEM
text	I-RESEARCH_PROBLEM
generation	I-RESEARCH_PROBLEM
from	I-RESEARCH_PROBLEM
graph	I-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
structured	I-RESEARCH_PROBLEM
data	E-RESEARCH_PROBLEM
relies	O
on	O
standard	O
sequence	O
-	O
to	O
-	O
sequence	O
methods	O
.	O
These	O
approaches	O
linearise	O
the	O
input	O
graph	O
to	O
be	O
fed	O
to	O
a	O
recurrent	O
neural	O
network	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
an	O
alternative	O
encoder	O
based	O
on	O
graph	O
convolutional	O
networks	O
that	O
directly	O
exploits	O
the	O
input	O
structure	O
.	O
We	O
report	O
results	O
on	O
two	O
graphto	O
-	O
sequence	O
datasets	O
that	O
empirically	O
show	O
the	O
benefits	O
of	O
explicitly	O
encoding	O
the	O
input	O
graph	O
structure	O
.	O
1	O

*	O
Data	B-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
to	I-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
text	I-RESEARCH_PROBLEM
generation	E-RESEARCH_PROBLEM
can	O
be	O
conceptually	O
divided	O
into	O
two	O
parts	O
:	O
ordering	O
and	O
structuring	O
the	O
information	O
(	O
planning	O
)	O
,	O
and	O
generating	O
fluent	O
language	O
describing	O
the	O
information	O
(	O
realization	O
)	O
.	O
Modern	O
neural	O
generation	O
systems	O
conflate	O
these	O
two	O
steps	O
into	O
a	O
single	O
end	O
-	O
to	O
-	O
end	O
differentiable	O
system	O
.	O
We	O
propose	O
to	O
split	O
the	O
generation	O
process	O
into	O
a	O
symbolic	O
text	O
-	O
planning	O
stage	O
that	O
is	O
faithful	O
to	O
the	O
input	O
,	O
followed	O
by	O
a	O
neural	O
generation	O
stage	O
that	O
focuses	O
only	O
on	O
realization	O
.	O
For	O
training	O
a	O
plan	O
-	O
to	O
-	O
text	O
generator	O
,	O
we	O
present	O
a	O
method	O
for	O
matching	O
reference	O
texts	O
to	O
their	O
corresponding	O
text	O
plans	O
.	O
For	O
inference	O
time	O
,	O
we	O
describe	O
a	O
method	O
for	O
selecting	O
high	O
-	O
quality	O
text	O
plans	O
for	O
new	O
inputs	O
.	O
We	O
implement	O
and	O
evaluate	O
our	O
approach	O
on	O
the	O
WebNLG	O
benchmark	O
.	O

In	O
the	O
last	O
few	O
years	O
,	O
many	O
different	O
methods	O
have	O
been	O
focusing	O
on	O
using	O
deep	O
recurrent	O
neural	O
networks	O
for	O
natural	B-RESEARCH_PROBLEM
language	I-RESEARCH_PROBLEM
generation	E-RESEARCH_PROBLEM
.	O
The	O
most	O
widely	O
used	O
sequence	O
-	O
to	O
-	O
sequence	O
neural	O
methods	O
are	O
word	O
-	O
based	O
:	O
as	O
such	O
,	O
they	O
need	O
a	O
pre-processing	O
step	O
called	O
delexicalization	O
(	O
conversely	O
,	O
relexicalization	O
)	O
to	O
deal	O
with	O
uncommon	O
or	O
unknown	O
words	O
.	O
These	O
forms	O
of	O
processing	O
,	O
however	O
,	O
give	O
rise	O
to	O
models	O
that	O
depend	O
on	O
the	O
vocabulary	O
used	O
and	O
are	O
not	O
completely	O
neural	O
.	O
In	O
this	O
work	O
,	O
we	O
present	O
an	O
end	O
-	O
to	O
-	O
end	O
sequence	O
-	O
to	O
-	O
sequence	O
model	O
with	O
attention	O
mechanism	O
which	O
reads	O
and	O
generates	O
at	O
a	O
character	O
level	O
,	O
no	O
longer	O
requiring	O
delexicalization	O
,	O
tokenization	O
,	O
nor	O
even	O
lowercasing	O
.	O
Moreover	O
,	O
since	O
characters	O
constitute	O
the	O
common	O
"	O
building	O
blocks	O
"	O
of	O
every	O
text	O
,	O
it	O
also	O
allows	O
a	O
more	O
general	O
approach	O
to	O
text	B-RESEARCH_PROBLEM
generation	E-RESEARCH_PROBLEM
,	O
enabling	O
the	O
possibility	O
to	O
exploit	O
transfer	O
learning	O
for	O
training	O
.	O
These	O
skills	O
are	O
obtained	O
thanks	O
to	O
two	O
major	O
features	O
:	O
(	O
i	O
)	O
the	O
possibility	O
to	O
alternate	O
between	O
the	O
standard	O
generation	O
mechanism	O
and	O
a	O
copy	O
one	O
,	O
which	O
allows	O
to	O
directly	O
copy	O
input	O
facts	O
to	O
produce	O
outputs	O
,	O
and	O
(	O
ii	O
)	O
the	O
use	O
of	O
an	O
original	O
training	O
pipeline	O
that	O
further	O
improves	O
the	O
quality	O
of	O
the	O
generated	O
texts	O
.	O
We	O
also	O
introduce	O
a	O
new	O
dataset	O
called	O
E2E	O
+	O
,	O
designed	O
to	O
highlight	O
the	O
copying	O
capabilities	O
of	O
character	O
-	O
based	O
models	O
,	O
that	O
is	O
a	O
modified	O
version	O
of	O
the	O
well	O
-	O
known	O
E2E	O
dataset	O
used	O
in	O
the	O
E2E	O
Challenge	O
.	O

Background	O
:	O
Given	O
the	O
importance	O
of	O
relation	O
or	O
event	O
extraction	O
from	O
biomedical	O
research	O
publications	O
to	O
support	O
knowledge	O
capture	O
and	O
synthesis	O
,	O
and	O
the	O
strong	O
dependency	O
of	O
approaches	O
to	O
this	O
information	O
extraction	O
task	O
on	O
syntactic	O
information	O
,	O
it	O
is	O
valuable	O
to	O
understand	O
which	O
approaches	O
to	O
syntactic	O
processing	O
of	O
biomedical	O
text	O
have	O
the	O
highest	O
performance	O
.	O
We	O
perform	O
an	O
empirical	O
study	O
comparing	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
traditional	O
feature	O
-	O
based	O
and	O
neural	O
network	O
-	O
based	O
models	O
for	O
two	O
core	O
natural	O
language	O
processing	O
tasks	O
of	O
part	O
-	O
of	O
-	O
speech	O
(	O
POS	O
)	O
tagging	O
and	O
dependency	B-RESEARCH_PROBLEM
parsing	E-RESEARCH_PROBLEM
on	O
two	O
benchmark	O
biomedical	O
corpora	O
,	O
GENIA	O
and	O
CRAFT	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
there	O
is	O
no	O
recent	O
work	O
making	O
such	O
comparisons	O
in	O
the	O
biomedical	O
context	O
;	O
specifically	O
no	O
detailed	O
analysis	O
of	O
neural	O
models	O
on	O
this	O
data	O
is	O
available	O
.	O
Experimental	O
results	O
show	O
that	O
in	O
general	O
,	O
the	O
neural	O
models	O
outperform	O
the	O
feature	O
-	O
based	O
models	O
on	O
two	O
benchmark	O
biomedical	O
corpora	O
GENIA	O
and	O
CRAFT	O
.	O
We	O
also	O
perform	O
a	O
task	O
-	O
oriented	O
evaluation	O
to	O
investigate	O
the	O
influences	O
of	O
these	O
models	O
in	O
a	O
downstream	O
application	O
on	O
biomedical	O
event	O
extraction	O
,	O
and	O
show	O
that	O
better	O
intrinsic	O
parsing	O
performance	O
does	O
not	O
always	O
imply	O
better	O
extrinsic	O
event	O
extraction	O
performance	O
.	O
We	O
have	O
presented	O
a	O
detailed	O
empirical	O
study	O
comparing	O
traditional	O
feature	O
-	O
based	O
and	O
neural	O
network	O
-	O
based	O
models	O
for	O
POS	B-RESEARCH_PROBLEM
tagging	E-RESEARCH_PROBLEM
and	O
dependency	B-RESEARCH_PROBLEM
parsing	E-RESEARCH_PROBLEM
in	O
the	O
biomedical	O
context	O
,	O
and	O
also	O
investigated	O
the	O
influence	O
of	O
parser	O
selection	O
for	O
a	O
biomedical	O
event	O
extraction	O
downstream	O
task	O
.	O

We	O
present	O
structured	O
perceptron	O
training	O
for	O
neural	B-RESEARCH_PROBLEM
network	I-RESEARCH_PROBLEM
transition	I-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
based	I-RESEARCH_PROBLEM
dependency	I-RESEARCH_PROBLEM
parsing	E-RESEARCH_PROBLEM
.	O
We	O
learn	O
the	O
neural	O
network	O
representation	O
using	O
a	O
gold	O
corpus	O
augmented	O
by	O
a	O
large	O
number	O
of	O
automatically	O
parsed	O
sentences	O
.	O
Given	O
this	O
fixed	O
network	O
representation	O
,	O
we	O
learn	O
a	O
final	O
layer	O
using	O
the	O
structured	O
perceptron	O
with	O
beam	O
-	O
search	O
decoding	O
.	O
On	O
the	O
Penn	O
Treebank	O
,	O
our	O
parser	O
reaches	O
94.	O
26	O
%	O
unlabeled	O
and	O
92.41	O
%	O
labeled	O
attachment	O
accuracy	O
,	O
which	O
to	O
our	O
knowledge	O
is	O
the	O
best	O
accuracy	O
on	O
Stanford	O
Dependencies	O
to	O
date	O
.	O
We	O
also	O
provide	O
indepth	O
ablative	O
analysis	O
to	O
determine	O
which	O
aspects	O
of	O
our	O
model	O
provide	O
the	O
largest	O
gains	O
in	O
accuracy	O
.	O

We	O
adapt	O
the	O
greedy	O
stack	O
LSTM	O
dependency	O
parser	O
of	O
Dyer	O
et	O
al.	O
(	O
2015	O
)	O
to	O
support	O
a	O
training	O
-	O
with	O
-	O
exploration	O
procedure	O
using	O
dynamic	O
oracles	O
(	O
Goldberg	O
and	O
Nivre	O
,	O
2013	O
)	O
instead	O
of	O
assuming	O
an	O
error	O
-	O
free	O
action	O
history	O
.	O
This	O
form	O
of	O
training	O
,	O
which	O
accounts	O
for	O
model	O
predictions	O
at	O
training	O
time	O
,	O
improves	O
parsing	S-RESEARCH_PROBLEM
accuracies	O
.	O
We	O
discuss	O
some	O
modifications	O
needed	O
in	O
order	O
to	O
get	O
training	O
with	O
exploration	O
to	O
work	O
well	O
for	O
a	O
probabilistic	O
neural	O
network	O
dependency	O
parser	O
.	O

We	O
introduce	O
a	O
globally	B-RESEARCH_PROBLEM
normalized	I-RESEARCH_PROBLEM
transition	I-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
based	I-RESEARCH_PROBLEM
neural	I-RESEARCH_PROBLEM
network	E-RESEARCH_PROBLEM
model	O
that	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
part	O
-	O
ofspeech	O
tagging	O
,	O
dependency	O
parsing	O
and	O
sentence	O
compression	O
results	O
.	O
Our	O
model	O
is	O
a	O
simple	O
feed	O
-	O
forward	O
neural	O
network	O
that	O
operates	O
on	O
a	O
task	O
-	O
specific	O
transition	O
system	O
,	O
yet	O
achieves	O
comparable	O
or	O
better	O
accuracies	O
than	O
recurrent	O
models	O
.	O
We	O
discuss	O
the	O
importance	O
of	O
global	O
as	O
opposed	O
to	O
local	O
normalization	O
:	O
a	O
key	O
insight	O
is	O
that	O
the	O
label	O
bias	O
problem	O
implies	O
that	O
globally	O
normalized	O
models	O
can	O
be	O
strictly	O
more	O
expressive	O
than	O
locally	O
normalized	O
models	O
.	O

The	O
scarcity	O
of	O
labeled	O
training	O
data	O
often	O
prohibits	O
the	O
internationalization	O
of	O
NLP	O
models	O
to	O
multiple	O
languages	O
.	O
Recent	O
developments	O
in	O
cross	O
-	O
lingual	O
understanding	O
(	O
XLU	S-RESEARCH_PROBLEM
)	O
has	O
made	O
progress	O
in	O
this	O
area	O
,	O
trying	O
to	O
bridge	O
the	O
language	O
barrier	O
using	O
language	O
universal	O
representations	O
.	O
However	O
,	O
even	O
if	O
the	O
language	O
problem	O
was	O
resolved	O
,	O
models	O
trained	O
in	O
one	O
language	O
would	O
not	O
transfer	O
to	O
another	O
language	O
perfectly	O
due	O
to	O
the	O
natural	O
domain	O
drift	O
across	O
languages	O
and	O
cultures	O
.	O
We	O
consider	O
the	O
setting	O
of	O
semi-supervised	O
cross	O
-	O
lingual	O
understanding	O
,	O
where	O
labeled	O
data	O
is	O
available	O
in	O
a	O
source	O
language	O
(	O
English	O
)	O
,	O
but	O
only	O
unlabeled	O
data	O
is	O
available	O
in	O
the	O
target	O
language	O
.	O
We	O
combine	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
cross	O
-	O
lingual	O
methods	O
with	O
recently	O
proposed	O
methods	O
for	O
weakly	O
supervised	O
learning	O
such	O
as	O
unsupervised	O
pre-training	O
and	O
unsupervised	O
data	O
augmentation	O
to	O
simultaneously	O
close	O
both	O
the	O
language	O
gap	O
and	O
the	O
domain	O
gap	O
in	O
XLU	S-RESEARCH_PROBLEM
.	O
We	O
show	O
that	O
addressing	O
the	O
domain	O
gap	O
is	O
crucial	O
.	O
We	O
improve	O
over	O
strong	O
baselines	O
and	O
achieve	O
a	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
for	O
cross	O
-	O
lingual	O
document	O
classification	O
.	O

One	O
-	O
hot	O
CNN	O
(	O
convolutional	O
neural	O
network	O
)	O
has	O
been	O
shown	O
to	O
be	O
effective	O
for	O
text	B-RESEARCH_PROBLEM
categorization	E-RESEARCH_PROBLEM
(	O
Johnson	O
&	O
Zhang	O
,	O
2015a	O
;	O
b	O
)	O
.	O
We	O
view	O
it	O
as	O
a	O
special	O
case	O
of	O
a	O
general	O
framework	O
which	O
jointly	O
trains	O
a	O
linear	O
model	O
with	O
a	O
non-linear	O
feature	O
generator	O
consisting	O
of	O
'	O
text	O
region	O
embedding	O
+	O
pooling	O
'	O
.	O
Under	O
this	O
framework	O
,	O
we	O
explore	O
a	O
more	O
sophisticated	O
region	O
embedding	O
method	O
using	O
Long	O
Short	O
-	O
Term	O
Memory	O
(	O
LSTM	O
)	O
.	O
LSTM	O
can	O
embed	O
text	O
regions	O
of	O
variable	O
(	O
and	O
possibly	O
large	O
)	O
sizes	O
,	O
whereas	O
the	O
region	O
size	O
needs	O
to	O
be	O
fixed	O
in	O
a	O
CNN	O
.	O
We	O
seek	O
effective	O
and	O
efficient	O
use	O
of	O
LSTM	O
for	O
this	O
purpose	O
in	O
the	O
supervised	O
and	O
semi-supervised	O
settings	O
.	O
The	O
best	O
results	O
were	O
obtained	O
by	O
combining	O
region	O
embeddings	O
in	O
the	O
form	O
of	O
LSTM	O
and	O
convolution	O
layers	O
trained	O
on	O
unlabeled	O
data	O
.	O
The	O
results	O
indicate	O
that	O
on	O
this	O
task	O
,	O
embeddings	O
of	O
text	O
regions	O
,	O
which	O
can	O
convey	O
complex	O
concepts	O
,	O
are	O
more	O
useful	O
than	O
embeddings	O
of	O
single	O
words	O
in	O
isolation	O
.	O

Increasingly	O
large	O
document	O
collections	O
require	O
improved	O
information	O
processing	O
methods	O
for	O
searching	O
,	O
retrieving	O
,	O
and	O
organizing	O
text	O
.	O
Central	O
to	O
these	O
information	O
processing	O
methods	O
is	O
document	B-RESEARCH_PROBLEM
classification	E-RESEARCH_PROBLEM
,	O
which	O
has	O
become	O
an	O
important	O
application	O
for	O
supervised	O
learning	O
.	O
Recently	O
the	O
performance	O
of	O
traditional	O
supervised	O
classifiers	O
has	O
degraded	O
as	O
the	O
number	O
of	O
documents	O
has	O
increased	O
.	O
This	O
is	O
because	O
along	O
with	O
growth	O
in	O
the	O
number	O
of	O
documents	O
has	O
come	O
an	O
increase	O
in	O
the	O
number	O
of	O
categories	O
.	O
This	O
paper	O
approaches	O
this	O
problem	O
differently	O
from	O
current	O
document	B-RESEARCH_PROBLEM
classification	E-RESEARCH_PROBLEM
methods	O
that	O
view	O
the	O
problem	O
as	O
multi-class	O
classification	O
.	O
Instead	O
we	O
perform	O
hierarchical	B-RESEARCH_PROBLEM
classification	E-RESEARCH_PROBLEM
using	O
an	O
approach	O
we	O
call	O
Hierarchical	O
Deep	O
Learning	O
for	O
Text	O
classification	O
(	O
HDLTex	O
)	O
.	O
HDLTex	O
employs	O
stacks	O
of	O
deep	O
learning	O
architectures	O
to	O
provide	O
specialized	O
understanding	O
at	O
each	O
level	O
of	O
the	O
document	O
hierarchy	O
.	O

Cross	B-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
lingual	I-RESEARCH_PROBLEM
document	I-RESEARCH_PROBLEM
classification	E-RESEARCH_PROBLEM
aims	O
at	O
training	O
a	O
document	O
classifier	O
on	O
resources	O
in	O
one	O
language	O
and	O
transferring	O
it	O
to	O
a	O
different	O
language	O
without	O
any	O
additional	O
resources	O
.	O
Several	O
approaches	O
have	O
been	O
proposed	O
in	O
the	O
literature	O
and	O
the	O
current	O
best	O
practice	O
is	O
to	O
evaluate	O
them	O
on	O
a	O
subset	O
of	O
the	O
Reuters	O
Corpus	O
Volume	O
2	O
.	O
However	O
,	O
this	O
subset	O
covers	O
only	O
few	O
languages	O
(	O
English	O
,	O
German	O
,	O
French	O
and	O
Spanish	O
)	O
and	O
almost	O
all	O
published	O
works	O
focus	O
on	O
the	O
the	O
transfer	O
between	O
English	O
and	O
German	O
.	O
In	O
addition	O
,	O
we	O
have	O
observed	O
that	O
the	O
class	O
prior	O
distributions	O
differ	O
significantly	O
between	O
the	O
languages	O
.	O
We	O
argue	O
that	O
this	O
complicates	O
the	O
evaluation	O
of	O
the	O
multilinguality	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
new	O
subset	O
of	O
the	O
Reuters	O
corpus	O
with	O
balanced	O
class	O
priors	O
for	O
eight	O
languages	O
.	O

Deep	O
contextualized	O
embeddings	O
trained	O
using	O
unsupervised	O
language	O
modeling	O
(	O
e.g.	O
,	O
ELMo	O
and	O
BERT	O
)	O
are	O
successful	O
in	O
a	O
wide	O
range	O
of	O
NLP	O
tasks	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
new	O
contextualized	O
embedding	O
model	O
of	O
words	O
and	O
entities	O
for	O
named	O
entity	O
disambiguation	O
(	O
NED	S-RESEARCH_PROBLEM
)	O
.	O
Our	O
model	O
is	O
based	O
on	O
the	O
bidirectional	O
transformer	O
encoder	O
and	O
produces	O
contextualized	O
embeddings	O
for	O
words	O
and	O
entities	O
in	O
the	O
input	O
text	O
.	O
The	O
embeddings	O
are	O
trained	O
using	O
a	O
new	O
masked	O
entity	O
prediction	O
task	O
that	O
aims	O
to	O
train	O
the	O
model	O
by	O
predicting	O
randomly	O
masked	O
entities	O
in	O
entityannotated	O
texts	O
.	O
We	O
trained	O
the	O
model	O
using	O
entity	O
-	O
annotated	O
texts	O
obtained	O
from	O
Wikipedia	O
.	O
We	O
evaluated	O
our	O
model	O
by	O
addressing	O
NED	S-RESEARCH_PROBLEM
using	O
a	O
simple	O
NED	S-RESEARCH_PROBLEM
model	O
based	O
on	O
the	O
trained	O
contextualized	O
embeddings	O
.	O
As	O
a	O
result	O
,	O
we	O
achieved	O
stateof	O
-	O
the	O
-	O
art	O
or	O
competitive	O
results	O
on	O
several	O
standard	O
NED	S-RESEARCH_PROBLEM
datasets	O
.	O

We	O
introduce	O
a	O
new	O
type	O
of	O
deep	B-RESEARCH_PROBLEM
contextualized	I-RESEARCH_PROBLEM
word	I-RESEARCH_PROBLEM
representation	E-RESEARCH_PROBLEM
that	O
models	O
both	O
(	O
1	O
)	O
complex	O
characteristics	O
of	O
word	O
use	O
(	O
e.g.	O
,	O
syntax	O
and	O
semantics	O
)	O
,	O
and	O
(	O
2	O
)	O
how	O
these	O
uses	O
vary	O
across	O
linguistic	O
contexts	O
(	O
i.e.	O
,	O
to	O
model	O
polysemy	O
)	O
.	O
Our	O
word	O
vectors	O
are	O
learned	O
functions	O
of	O
the	O
internal	O
states	O
of	O
a	O
deep	O
bidirectional	O
language	O
model	O
(	O
biLM	O
)	O
,	O
which	O
is	O
pretrained	O
on	O
a	O
large	O
text	O
corpus	O
.	O
We	O
show	O
that	O
these	O
representations	O
can	O
be	O
easily	O
added	O
to	O
existing	O
models	O
and	O
significantly	O
improve	O
the	O
state	O
of	O
the	O
art	O
across	O
six	O
challenging	O
NLP	O
problems	O
,	O
including	O
question	O
answering	O
,	O
textual	O
entailment	O
and	O
sentiment	O
analysis	O
.	O
We	O
also	O
present	O
an	O
analysis	O
showing	O
that	O
exposing	O
the	O
deep	O
internals	O
of	O
the	O
pre-trained	O
network	O
is	O
crucial	O
,	O
allowing	O
downstream	O
models	O
to	O
mix	O
different	O
types	O
of	O
semi-supervision	O
signals	O
.	O

In	O
this	O
article	O
,	O
we	O
tackle	O
the	O
issue	O
of	O
the	O
limited	O
quantity	O
of	O
manually	O
sense	O
annotated	O
corpora	O
for	O
the	O
task	O
of	O
word	O
sense	O
disambiguation	O
,	O
by	O
exploiting	O
the	O
semantic	O
relationships	O
between	O
senses	O
such	O
as	O
synonymy	O
,	O
hypernymy	O
and	O
hyponymy	O
,	O
in	O
order	O
to	O
compress	O
the	O
sense	O
vocabulary	O
of	O
Princeton	O
WordNet	O
,	O
and	O
thus	O
reduce	O
the	O
number	O
of	O
different	O
sense	O
tags	O
that	O
must	O
be	O
observed	O
to	O
disambiguate	O
all	O
words	O
of	O
the	O
lexical	O
database	O
.	O
We	O
propose	O
two	O
different	O
methods	O
that	O
greatly	O
reduce	O
the	O
size	O
of	O
neural	B-RESEARCH_PROBLEM
WSD	E-RESEARCH_PROBLEM
models	O
,	O
with	O
the	O
benefit	O
of	O
improving	O
their	O
coverage	O
without	O
additional	O
training	O
data	O
,	O
and	O
without	O
impacting	O
their	O
precision	O
.	O
In	O
addition	O
to	O
our	O
methods	O
,	O
we	O
present	O
a	O
WSD	S-RESEARCH_PROBLEM
system	O
which	O
relies	O
on	O
pre-trained	O
BERT	O
word	O
vectors	O
in	O
order	O
to	O
achieve	O
results	O
that	O
significantly	O
outperforms	O
the	O
state	O
of	O
the	O
art	O
on	O
all	O
WSD	S-RESEARCH_PROBLEM
evaluation	O
tasks	O
.	O

Word	O
Sense	O
Disambiguation	O
(	O
WSD	S-RESEARCH_PROBLEM
)	O
aims	O
to	O
identify	O
the	O
correct	O
meaning	O
of	O
polysemous	O
words	O
in	O
the	O
particular	O
context	O
.	O
Lexical	O
resources	O
like	O
WordNet	O
which	O
are	O
proved	O
to	O
be	O
of	O
great	O
help	O
for	O
WSD	S-RESEARCH_PROBLEM
in	O
the	O
knowledge	O
-	O
based	O
methods	O
.	O
However	O
,	O
previous	O
neural	O
networks	O
for	O
WSD	S-RESEARCH_PROBLEM
always	O
rely	O
on	O
massive	O
labeled	O
data	O
(	O
context	O
)	O
,	O
ignoring	O
lexical	O
resources	O
like	O
glosses	O
(	O
sense	O
definitions	O
)	O
.	O
In	O
this	O
paper	O
,	O
we	O
integrate	O
the	O
context	O
and	O
glosses	O
of	O
the	O
target	O
word	O
into	O
a	O
unified	O
framework	O
in	O
order	O
to	O
make	O
full	O
use	O
of	O
both	O
labeled	O
data	O
and	O
lexical	O
knowledge	O
.	O
Therefore	O
,	O
we	O
propose	O
GAS	O
:	O
a	O
gloss	O
-	O
augmented	O
WSD	S-RESEARCH_PROBLEM
neural	O
network	O
which	O
jointly	O
encodes	O
the	O
context	O
and	O
glosses	O
of	O
the	O
target	O
word	O
.	O
GAS	O
models	O
the	O
semantic	O
relationship	O
between	O
the	O
context	O
and	O
the	O
gloss	O
in	O
an	O
improved	O
memory	O
network	O
framework	O
,	O
which	O
breaks	O
the	O
barriers	O
of	O
the	O
previous	O
supervised	O
methods	O
and	O
knowledge	O
-	O
based	O
methods	O
.	O
We	O
further	O
extend	O
the	O
original	O
gloss	O
of	O
word	O
sense	O
via	O
its	O
semantic	O
relations	O
in	O
WordNet	O
to	O
enrich	O
the	O
gloss	O
information	O
.	O

Disambiguation	S-RESEARCH_PROBLEM
is	O
an	O
open	O
problem	O
in	O
Natural	O
Language	O
Processing	O
which	O
is	O
particularly	O
challenging	O
and	O
useful	O
in	O
the	O
unsupervised	O
setting	O
where	O
all	O
the	O
words	O
in	O
any	O
given	O
text	O
need	O
to	O
be	O
disambiguated	O
without	O
using	O
any	O
labeled	O
data	O
.	O
Typically	O
WSD	S-RESEARCH_PROBLEM
systems	O
use	O
the	O
sentence	O
or	O
a	O
small	O
window	O
of	O
words	O
around	O
the	O
target	O
word	O
as	O
the	O
context	O
for	O
disambiguation	O
because	O
their	O
computational	O
complexity	O
scales	O
exponentially	O
with	O
the	O
size	O
of	O
the	O
context	O
.	O
In	O
this	O
paper	O
,	O
we	O
leverage	O
the	O
formalism	O
of	O
topic	O
model	O
to	O
design	O
a	O
WSD	S-RESEARCH_PROBLEM
system	O
that	O
scales	O
linearly	O
with	O
the	O
number	O
of	O
words	O
in	O
the	O
context	O
.	O
As	O
a	O
result	O
,	O
our	O
system	O
is	O
able	O
to	O
utilize	O
the	O
whole	O
document	O
as	O
the	O
context	O
for	O
a	O
word	O
to	O
be	O
disambiguated	O
.	O
The	O
proposed	O
method	O
is	O
a	O
variant	O
of	O
Latent	O
Dirichlet	O
Allocation	O
in	O
which	O
the	O
topic	O
proportions	O
for	O
a	O
document	O
are	O
replaced	O
by	O
synset	O
proportions	O
.	O
We	O
further	O
utilize	O
the	O
information	O
in	O
the	O
WordNet	O
by	O
assigning	O
a	O
non-uniform	O
prior	O
to	O
synset	O
distribution	O
over	O
words	O
and	O
a	O
logistic	O
-	O
normal	O
prior	O
for	O
document	O
distribution	O
over	O
synsets	O
.	O
We	O
evaluate	O
the	O
proposed	O
method	O
on	O
Senseval	O
-	O
2	O
,	O
Senseval	O
-	O
3	O
,	O
SemEval	O
-	O
2007	O
,	O
SemEval-2013	O
and	O
SemEval	O
-	O
2015	O
English	O
All	O
-	O
Word	O
WSD	S-RESEARCH_PROBLEM
datasets	O
and	O
show	O
that	O
it	O
outperforms	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
unsupervised	O
knowledge	O
-	O
based	O
WSD	S-RESEARCH_PROBLEM
system	O
by	O
a	O
significant	O
margin	O
.	O

Due	O
to	O
recent	O
technical	O
and	O
scientific	O
advances	O
,	O
we	O
have	O
a	O
wealth	O
of	O
information	O
hidden	O
in	O
unstructured	O
text	O
data	O
such	O
as	O
offline	O
/	O
online	O
narratives	O
,	O
research	O
articles	O
,	O
and	O
clinical	O
reports	O
.	O
To	O
mine	O
these	O
data	O
properly	O
,	O
attributable	O
to	O
their	O
innate	O
ambiguity	O
,	O
a	O
Word	B-RESEARCH_PROBLEM
Sense	I-RESEARCH_PROBLEM
Disambiguation	E-RESEARCH_PROBLEM
(	O
WSD	S-RESEARCH_PROBLEM
)	O
algorithm	O
can	O
avoid	O
numbers	O
of	O
difficulties	O
in	O
Natural	O
Language	O
Processing	O
(	O
NLP	O
)	O
pipeline	O
.	O
However	O
,	O
considering	O
a	O
large	O
number	O
of	O
ambiguous	O
words	O
in	O
one	O
language	O
or	O
technical	O
domain	O
,	O
we	O
may	O
encounter	O
limiting	O
constraints	O
for	O
proper	O
deployment	O
of	O
existing	O
WSD	S-RESEARCH_PROBLEM
models	O
.	O
This	O
paper	O
attempts	O
to	O
address	O
the	O
problem	O
of	O
oneclassifier	O
-	O
per	O
-	O
one	O
-	O
word	O
WSD	S-RESEARCH_PROBLEM
algorithms	O
by	O
proposing	O
a	O
single	O
Bidirectional	O
Long	O
Short	O
-	O
Term	O
Memory	O
(	O
BLSTM	O
)	O
network	O
which	O
by	O
considering	O
senses	O
and	O
context	O
sequences	O
works	O
on	O
all	O
ambiguous	O
words	O
collectively	O
.	O
Evaluated	O
on	O
SensEval	O
-	O
3	O
benchmark	O
,	O
we	O
show	O
the	O
result	O
of	O
our	O
model	O
is	O
comparable	O
with	O
top	O
-	O
performing	O
WSD	S-RESEARCH_PROBLEM
algorithms	O
.	O
We	O
also	O
discuss	O
how	O
applying	O
additional	O
modifications	O
alleviates	O
the	O
model	O
fault	O
and	O
the	O
need	O
for	O
more	O
training	O
data	O
.	O

Word	B-RESEARCH_PROBLEM
Sense	I-RESEARCH_PROBLEM
Disambiguation	E-RESEARCH_PROBLEM
models	O
exist	O
in	O
many	O
flavors	O
.	O
Even	O
though	O
supervised	O
ones	O
tend	O
to	O
perform	O
best	O
in	O
terms	O
of	O
accuracy	O
,	O
they	O
often	O
lose	O
ground	O
to	O
more	O
flexible	O
knowledge	O
-	O
based	O
solutions	O
,	O
which	O
do	O
not	O
require	O
training	O
by	O
a	O
word	O
expert	O
for	O
every	O
disambiguation	O
target	O
.	O
To	O
bridge	O
this	O
gap	O
we	O
adopt	O
a	O
different	O
perspective	O
and	O
rely	O
on	O
sequence	O
learning	O
to	O
frame	O
the	O
disambiguation	O
problem	O
:	O
we	O
propose	O
and	O
study	O
in	O
depth	O
a	O
series	O
of	O
end	O
-	O
to	O
-	O
end	O
neural	O
architectures	O
directly	O
tailored	O
to	O
the	O
task	O
,	O
from	O
bidirectional	O
Long	O
Short	O
-	O
Term	O
Memory	O
to	O
encoder	O
-	O
decoder	O
models	O
.	O
Our	O
extensive	O
evaluation	O
over	O
standard	O
benchmarks	O
and	O
in	O
multiple	O
languages	O
shows	O
that	O
sequence	O
learning	O
enables	O
more	O
versatile	O
all	O
-	O
words	O
models	O
that	O
consistently	O
lead	O
to	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
,	O
even	O
against	O
word	O
experts	O
with	O
engineered	O
features	O
.	O

Contextualized	O
word	O
embeddings	O
(	O
CWE	O
)	O
such	O
as	O
provided	O
by	O
ELMo	O
(	O
Peters	O
et	O
al.	O
,	O
2018	O
)	O
,	O
Flair	O
NLP	O
(	O
Akbik	O
et	O
al.	O
,	O
2018	O
)	O
,	O
or	O
BERT	O
(	O
Devlin	O
et	O
al.	O
,	O
2019	O
area	O
major	O
recent	O
innovation	O
in	O
NLP	O
.	O
CWEs	O
provide	O
semantic	O
vector	O
representations	O
of	O
words	O
depending	O
on	O
their	O
respective	O
context	O
.	O
Their	O
advantage	O
over	O
static	O
word	O
embeddings	O
has	O
been	O
shown	O
fora	O
number	O
of	O
tasks	O
,	O
such	O
as	O
text	O
classification	O
,	O
sequence	O
tagging	O
,	O
or	O
machine	O
translation	O
.	O
Since	O
vectors	O
of	O
the	O
same	O
word	O
type	O
can	O
vary	O
depending	O
on	O
the	O
respective	O
context	O
,	O
they	O
implicitly	O
provide	O
a	O
model	O
for	O
word	O
sense	O
disambiguation	O
(	O
WSD	S-RESEARCH_PROBLEM
)	O
.	O
We	O
introduce	O
a	O
simple	O
but	O
effective	O
approach	O
to	O
WSD	S-RESEARCH_PROBLEM
using	O
a	O
nearest	O
neighbor	O
classification	O
on	O
CWEs	O
.	O
We	O
compare	O
the	O
performance	O
of	O
different	O
CWE	O
models	O
for	O
the	O
task	O
and	O
can	O
report	O
improvements	O
above	O
the	O
current	O
state	O
of	O
the	O
art	O
for	O
two	O
standard	O
WSD	S-RESEARCH_PROBLEM
benchmark	O
datasets	O
.	O
We	O
further	O
show	O
that	O
the	O
pre-trained	O
BERT	O
model	O
is	O
able	O
to	O
place	O
polysemic	O
words	O
into	O
distinct	O
'	O
sense	O
'	O
regions	O
of	O
the	O
embedding	O
space	O
,	O
while	O
ELMo	O
and	O
Flair	O
NLP	O
do	O
not	O
seem	O
to	O
possess	O
this	O
ability	O
.	O

In	O
Word	O
Sense	O
Disambiguation	O
(	O
WSD	S-RESEARCH_PROBLEM
)	O
,	O
the	O
predominant	O
approach	O
generally	O
involves	O
a	O
supervised	O
system	O
trained	O
on	O
sense	O
annotated	O
corpora	O
.	O
The	O
limited	O
quantity	O
of	O
such	O
corpora	O
however	O
restricts	O
the	O
coverage	O
and	O
the	O
performance	O
of	O
these	O
systems	O
.	O
In	O
this	O
article	O
,	O
we	O
propose	O
anew	O
method	O
that	O
solves	O
these	O
issues	O
by	O
taking	O
advantage	O
of	O
the	O
knowledge	O
present	O
in	O
WordNet	O
,	O
and	O
especially	O
the	O
hypernymy	O
and	O
hyponymy	O
relationships	O
between	O
synsets	O
,	O
in	O
order	O
to	O
reduce	O
the	O
number	O
of	O
different	O
sense	O
tags	O
that	O
are	O
necessary	O
to	O
disambiguate	O
all	O
words	O
of	O
the	O
lexical	O
database	O
.	O
Our	O
method	O
leads	O
to	O
state	O
of	O
the	O
art	O
results	O
on	O
most	O
WSD	S-RESEARCH_PROBLEM
evaluation	O
tasks	O
,	O
while	O
improving	O
the	O
coverage	O
of	O
supervised	O
systems	O
,	O
reducing	O
the	O
training	O
time	O
and	O
the	O
size	O
of	O
the	O
models	O
,	O
without	O
additional	O
training	O
data	O
.	O
In	O
addition	O
,	O
we	O
exhibit	O
results	O
that	O
significantly	O
outperform	O
the	O
state	O
of	O
the	O
art	O
when	O
our	O
method	O
is	O
combined	O
with	O
an	O
ensembling	O
technique	O
and	O
the	O
addition	O
of	O
the	O
WordNet	O
Gloss	O
Tagged	O
as	O
training	O
corpus	O
.	O

Word	O
Sense	O
Disambiguation	O
(	O
WSD	S-RESEARCH_PROBLEM
)	O
aims	O
to	O
identify	O
the	O
correct	O
meaning	O
of	O
polysemous	O
words	O
in	O
the	O
particular	O
context	O
.	O
Lexical	O
resources	O
like	O
WordNet	O
which	O
are	O
proved	O
to	O
be	O
of	O
great	O
help	O
for	O
WSD	S-RESEARCH_PROBLEM
in	O
the	O
knowledge	O
-	O
based	O
methods	O
.	O
However	O
,	O
previous	O
neural	O
networks	O
for	O
WSD	S-RESEARCH_PROBLEM
always	O
rely	O
on	O
massive	O
labeled	O
data	O
(	O
context	O
)	O
,	O
ignoring	O
lexical	O
resources	O
like	O
glosses	O
(	O
sense	O
definitions	O
)	O
.	O
In	O
this	O
paper	O
,	O
we	O
integrate	O
the	O
context	O
and	O
glosses	O
of	O
the	O
target	O
word	O
into	O
a	O
unified	O
framework	O
in	O
order	O
to	O
make	O
full	O
use	O
of	O
both	O
labeled	O
data	O
and	O
lexical	O
knowledge	O
.	O
Therefore	O
,	O
we	O
propose	O
GAS	O
:	O
a	O
gloss	O
-	O
augmented	O
WSD	S-RESEARCH_PROBLEM
neural	O
network	O
which	O
jointly	O
encodes	O
the	O
context	O
and	O
glosses	O
of	O
the	O
target	O
word	O
.	O
GAS	O
models	O
the	O
semantic	O
relationship	O
between	O
the	O
context	O
and	O
the	O
gloss	O
in	O
an	O
improved	O
memory	O
network	O
framework	O
,	O
which	O
breaks	O
the	O
barriers	O
of	O
the	O
previous	O
supervised	O
methods	O
and	O
knowledge	O
-	O
based	O
methods	O
.	O
We	O
further	O
extend	O
the	O
original	O
gloss	O
of	O
word	O
sense	O
via	O
its	O
semantic	O
relations	O
in	O
WordNet	O
to	O
enrich	O
the	O
gloss	O
information	O
.	O

Determining	O
the	O
intended	O
sense	O
of	O
words	O
in	O
text	O
-	O
word	O
sense	O
disambiguation	O
(	O
WSD	S-RESEARCH_PROBLEM
)	O
-	O
is	O
a	O
longstanding	O
problem	O
in	O
natural	O
language	O
processing	O
.	O
Recently	O
,	O
researchers	O
have	O
shown	O
promising	O
results	O
using	O
word	O
vectors	O
extracted	O
from	O
a	O
neural	O
network	O
language	O
model	O
as	O
features	O
in	O
WSD	S-RESEARCH_PROBLEM
algorithms	O
.	O
However	O
,	O
a	O
simple	O
average	O
or	O
concatenation	O
of	O
word	O
vectors	O
for	O
each	O
word	O
in	O
a	O
text	O
loses	O
the	O
sequential	O
and	O
syntactic	O
information	O
of	O
the	O
text	O
.	O
In	O
this	O
paper	O
,	O
we	O
study	O
WSD	S-RESEARCH_PROBLEM
with	O
a	O
sequence	O
learning	O
neural	O
net	O
,	O
LSTM	O
,	O
to	O
better	O
capture	O
the	O
sequential	O
and	O
syntactic	O
patterns	O
of	O
the	O
text	O
.	O
To	O
alleviate	O
the	O
lack	O
of	O
training	O
data	O
in	O
all	O
-	O
words	O
WSD	S-RESEARCH_PROBLEM
,	O
we	O
employ	O
the	O
same	O
LSTM	O
in	O
a	O
semi-supervised	O
label	O
propagation	O
classifier	O
.	O
We	O
demonstrate	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
,	O
especially	O
on	O
verbs	O
.	O
This	O
work	O
is	O
licensed	O
under	O
a	O
Creative	O
Commons	O
Attribution	O
4.0	O
International	O
License	O
.	O

Work	O
done	O
as	O
a	O
Google	O
AI	O
Resident	O
Language	O
modeling	O
tasks	O
,	O
in	O
which	O
words	O
,	O
or	O
word	O
-	O
pieces	O
,	O
are	O
predicted	O
on	O
the	O
basis	O
of	O
a	O
local	O
context	O
,	O
have	O
been	O
very	O
effective	O
for	O
learning	B-RESEARCH_PROBLEM
word	I-RESEARCH_PROBLEM
embeddings	E-RESEARCH_PROBLEM
and	O
context	B-RESEARCH_PROBLEM
dependent	I-RESEARCH_PROBLEM
representations	I-RESEARCH_PROBLEM
of	I-RESEARCH_PROBLEM
phrases	E-RESEARCH_PROBLEM
.	O
Motivated	O
by	O
the	O
observation	O
that	O
efforts	O
to	O
code	O
world	O
knowledge	O
into	O
machine	O
readable	O
knowledge	O
bases	O
or	O
human	O
readable	O
encyclopedias	O
tend	O
to	O
be	O
entity	O
-	O
centric	O
,	O
we	O
investigate	O
the	O
use	O
of	O
a	O
fill	O
-	O
in	O
-	O
the	O
-	O
blank	O
task	O
to	O
learn	O
context	O
independent	O
representations	O
of	O
entities	O
from	O
the	O
text	O
contexts	O
in	O
which	O
those	O
entities	O
were	O
mentioned	O
.	O
We	O
show	O
that	O
large	O
scale	O
training	O
of	O
neural	O
models	O
allows	O
us	O
to	O
learn	O
high	O
quality	O
entity	O
representations	O
,	O
and	O
we	O
demonstrate	O
successful	O
results	O
on	O
four	O
domains	O
:	O
(	O
1	O
)	O
existing	O
entity	O
-	O
level	O
typing	O
benchmarks	O
,	O
including	O
a	O
64	O
%	O
error	O
reduction	O
over	O
previous	O
work	O
on	O
TypeNet	O
;	O
(	O
2	O
)	O
a	O
novel	O
few	O
-	O
shot	O
category	O
reconstruction	O
task	O
;	O
(	O
3	O
)	O
existing	O
entity	O
linking	O
benchmarks	O
,	O
where	O
we	O
match	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
on	O
CoNLL	O
-	O
Aida	O
without	O
linking	O
-	O
specific	O
features	O
and	O
obtain	O
a	O
score	O
of	O
89.8	O
%	O
on	O
TAC	O
-	O
KBP	O
2010	O
without	O
using	O
any	O
alias	O
table	O
,	O
external	O
knowledge	O
base	O
or	O
in	O
domain	O
training	O
data	O
and	O
(	O
4	O
)	O
answering	O
trivia	O
questions	O
,	O
which	O
uniquely	O
identify	O
entities	O
.	O
Our	O
global	O
entity	O
representations	O
encode	O
fine	O
-	O
grained	O
type	O
categories	O
,	O
such	O
as	O
Scottish	O
footballers	O
,	O
and	O
can	O
answer	O
trivia	O
questions	O
such	O
as	O
Who	O
was	O
the	O
last	O
inmate	O
of	O
Spandau	O
jail	O
in	O
Berlin	O
?	O
INTRODUCTION	O
A	O
long	O
term	O
goal	O
of	O
artificial	O
intelligence	O
has	O
been	O
the	O
development	O
and	O
population	O
of	O
an	O
entitycentric	O
representation	O
of	O
human	O
knowledge	O
.	O

Named	B-RESEARCH_PROBLEM
Entity	I-RESEARCH_PROBLEM
Disambiguation	E-RESEARCH_PROBLEM
(	O
NED	S-RESEARCH_PROBLEM
)	O
refers	O
to	O
the	O
task	O
of	O
resolving	O
multiple	O
named	O
entity	O
mentions	O
in	O
a	O
document	O
to	O
their	O
correct	O
references	O
in	O
a	O
knowledge	O
base	O
(	O
KB	O
)	O
(	O
e.g.	O
,	O
Wikipedia	O
)	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
novel	O
embedding	O
method	O
specifically	O
designed	O
for	O
NED	S-RESEARCH_PROBLEM
.	O
The	O
proposed	O
method	O
jointly	O
maps	O
words	O
and	O
entities	O
into	O
the	O
same	O
continuous	O
vector	O
space	O
.	O
We	O
extend	O
the	O
skip	O
-	O
gram	O
model	O
by	O
using	O
two	O
models	O
.	O
The	O
KB	O
graph	O
model	O
learns	O
the	O
relatedness	O
of	O
entities	O
using	O
the	O
link	O
structure	O
of	O
the	O
KB	O
,	O
whereas	O
the	O
anchor	O
context	O
model	O
aims	O
to	O
align	O
vectors	O
such	O
that	O
similar	O
words	O
and	O
entities	O
occur	O
close	O
to	O
one	O
another	O
in	O
the	O
vector	O
space	O
by	O
leveraging	O
KB	O
anchors	O
and	O
their	O
context	O
words	O
.	O
By	O
combining	O
contexts	O
based	O
on	O
the	O
proposed	O
embedding	O
with	O
standard	O
NED	S-RESEARCH_PROBLEM
features	O
,	O
we	O
achieved	O
state	O
-	O
of	O
-	O
theart	O
accuracy	O
of	O
93.1	O
%	O
on	O
the	O
standard	O
CoNLL	O
dataset	O
and	O
85.2	O
%	O
on	O
the	O
TAC	O
2010	O
dataset	O
.	O

3D	O
Morphable	O
Models	O
(	O
3	O
DMMs	O
)	O
are	O
powerful	O
statistical	O
models	O
of	O
3D	O
facial	O
shape	O
and	O
texture	O
,	O
and	O
among	O
the	O
stateof	O
-	O
the	O
-	O
art	O
methods	O
for	O
reconstructing	B-RESEARCH_PROBLEM
facial	I-RESEARCH_PROBLEM
shape	I-RESEARCH_PROBLEM
from	I-RESEARCH_PROBLEM
single	I-RESEARCH_PROBLEM
images	E-RESEARCH_PROBLEM
.	O
With	O
the	O
advent	O
of	O
new	O
3D	O
sensors	O
,	O
many	O
3	O
D	O
facial	O
datasets	O
have	O
been	O
collected	O
containing	O
both	O
neutral	O
as	O
well	O
as	O
expressive	O
faces	O
.	O
However	O
,	O
all	O
datasets	O
are	O
captured	O
under	O
controlled	O
conditions	O
.	O
Thus	O
,	O
even	O
though	O
powerful	O
3	O
D	O
facial	O
shape	O
models	O
can	O
be	O
learnt	O
from	O
such	O
data	O
,	O
it	O
is	O
difficult	O
to	O
build	O
statistical	O
texture	O
models	O
that	O
are	O
sufficient	O
to	O
reconstruct	O
faces	O
captured	O
in	O
unconstrained	O
conditions	O
(	O
"	O
in	O
-	O
the	O
-	O
wild	O
"	O
)	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
the	O
first	O
,	O
to	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
"	O
in	O
-	O
the	O
-	O
wild	O
"	O
3	O
DMM	O
by	O
combining	O
a	O
powerful	O
statistical	O
model	O
of	O
facial	O
shape	O
,	O
which	O
describes	O
both	O
identity	O
and	O
expression	O
,	O
with	O
an	O
"	O
in	O
-	O
the	O
-wild	O
"	O
texture	O
model	O
.	O
We	O
show	O
that	O
the	O
employment	O
of	O
such	O
an	O
"	O
in	O
-	O
thewild	O
"	O
texture	O
model	O
greatly	O
simplifies	O
the	O
fitting	O
procedure	O
,	O
because	O
there	O
is	O
no	O
need	O
to	O
optimize	O
with	O
regards	O
to	O
the	O
illumination	O
parameters	O
.	O
Furthermore	O
,	O
we	O
propose	O
anew	O
fast	O
algorithm	O
for	O
fitting	O
the	O
3	O
DMM	O
in	O
arbitrary	O
images	O
.	O

Figure	O
1	O
:	O
A	O
few	O
results	O
from	O
our	O
VRN	O
-	O
Guided	O
method	O
,	O
on	O
a	O
full	O
range	O
of	O
pose	O
,	O
including	O
large	O
expressions	O
.	O
Abstract	O
3	B-RESEARCH_PROBLEM
D	I-RESEARCH_PROBLEM
face	I-RESEARCH_PROBLEM
reconstruction	E-RESEARCH_PROBLEM
is	O
a	O
fundamental	O
Computer	O
Vision	O
problem	O
of	O
extraordinary	O
difficulty	O
.	O
Current	O
systems	O
often	O
assume	O
the	O
availability	O
of	O
multiple	O
facial	O
images	O
(	O
sometimes	O
from	O
the	O
same	O
subject	O
)	O
as	O
input	O
,	O
and	O
must	O
address	O
a	O
number	O
of	O
methodological	O
challenges	O
such	O
as	O
establishing	O
dense	O
correspondences	O
across	O
large	O
facial	O
poses	O
,	O
expressions	O
,	O
and	O
non-uniform	O
illumination	O
.	O
In	O
general	O
these	O
methods	O
require	O
complex	O
and	O
inefficient	O
pipelines	O
for	O
model	O
building	O
and	O
fitting	O
.	O
In	O
this	O
work	O
,	O
we	O
propose	O
to	O
address	O
many	O
of	O
these	O
limitations	O
by	O
training	O
a	O
Convolutional	O
Neural	O
Network	O
(	O
CNN	O
)	O
on	O
an	O
appropriate	O
dataset	O
consisting	O
of	O
2D	O
images	O
and	O
3D	O
facial	O
models	O
or	O
scans	O
.	O
Our	O
CNN	O
works	O
with	O
just	O
a	O
single	O
2	O
D	O
facial	O
image	O
,	O
does	O
not	O
require	O
accurate	O
alignment	O
nor	O
establishes	O
dense	O
correspondence	O
between	O
images	O
,	O
works	O
for	O
arbitrary	O
facial	O
poses	O
and	O
expressions	O
,	O
and	O
can	O
be	O
used	O
to	O
reconstruct	O
the	O
whole	O
3	O
D	O
facial	O
geometry	O
(	O
including	O
the	O
non-visible	O
parts	O
of	O
the	O
face	O
)	O
bypassing	O
the	O
construction	O
(	O
during	O
training	O
)	O
and	O
fitting	O
(	O
during	O
testing	O
)	O
of	O
a	O
3D	O
Morphable	O
Model	O
.	O
We	O
achieve	O
this	O
via	O
a	O
simple	O
CNN	O
architecture	O
that	O
performs	O
direct	O
regression	O
of	O
a	O
volumetric	O
representation	O
of	O
the	O
3D	O
facial	O
geometry	O
from	O
a	O
single	O
2D	O
image	O
.	O

Face	B-RESEARCH_PROBLEM
alignment	E-RESEARCH_PROBLEM
is	O
a	O
classic	O
problem	O
in	O
the	O
computer	O
vision	O
field	O
.	O
Previous	O
works	O
mostly	O
focus	O
on	O
sparse	O
alignment	O
with	O
a	O
limited	O
number	O
of	O
facial	O
landmark	O
points	O
,	O
i.e.	O
,	O
facial	O
landmark	O
detection	O
.	O
In	O
this	O
paper	O
,	O
for	O
the	O
first	O
time	O
,	O
we	O
aim	O
at	O
providing	O
a	O
very	O
dense	O
3D	O
alignment	O
for	O
largepose	O
face	O
images	O
.	O
To	O
achieve	O
this	O
,	O
we	O
train	O
a	O
CNN	O
to	O
estimate	O
the	O
3D	O
face	O
shape	O
,	O
which	O
not	O
only	O
aligns	O
limited	O
facial	O
landmarks	O
but	O
also	O
fits	O
face	O
contours	O
and	O
SIFT	O
feature	O
points	O
.	O
Moreover	O
,	O
we	O
also	O
address	O
the	O
bottleneck	O
of	O
training	O
CNN	O
with	O
multiple	O
datasets	O
,	O
due	O
to	O
different	O
landmark	O
markups	O
on	O
different	O
datasets	O
,	O
such	O
as	O
5	O
,	O
34	O
,	O
68	O
.	O
Experimental	O
results	O
show	O
our	O
method	O
not	O
only	O
provides	O
highquality	O
,	O
dense	O
3	O
D	O
face	O
fitting	O
but	O
also	O
outperforms	O
the	O
stateof	O
-	O
the	O
-	O
art	O
facial	O
landmark	O
detection	O
methods	O
on	O
the	O
challenging	O
datasets	O
.	O
Our	O
model	O
can	O
run	O
at	O
real	O
time	O
during	O
testing	O
and	O
it	O
's	O
available	O
at	O

Face	B-RESEARCH_PROBLEM
alignment	E-RESEARCH_PROBLEM
,	O
which	O
fits	O
a	O
face	O
model	O
to	O
an	O
image	O
and	O
extracts	O
the	O
semantic	O
meanings	O
of	O
facial	O
pixels	O
,	O
has	O
been	O
an	O
important	O
topic	O
in	O
CV	O
community	O
.	O
However	O
,	O
most	O
algorithms	O
are	O
designed	O
for	O
faces	O
in	O
small	O
to	O
medium	O
poses	O
(	O
below	O
45	O
)	O
,	O
lacking	O
the	O
ability	O
to	O
align	O
faces	O
in	O
large	O
poses	O
up	O
to	O
90	O
.	O
The	O
challenges	O
are	O
three	O
-	O
fold	O
:	O
Firstly	O
,	O
the	O
commonly	O
used	O
landmark	O
-	O
based	O
face	O
model	O
assumes	O
that	O
all	O
the	O
landmarks	O
are	O
visible	O
and	O
is	O
therefore	O
not	O
suitable	O
for	O
profile	O
views	O
.	O
Secondly	O
,	O
the	O
face	O
appearance	O
varies	O
more	O
dramatically	O
across	O
large	O
poses	O
,	O
ranging	O
from	O
frontal	O
view	O
to	O
profile	O
view	O
.	O
Thirdly	O
,	O
labelling	O
landmarks	O
in	O
large	O
poses	O
is	O
extremely	O
challenging	O
since	O
the	O
invisible	O
landmarks	O
have	O
to	O
be	O
guessed	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
solution	O
to	O
the	O
three	O
problems	O
in	O
an	O
new	O
alignment	O
framework	O
,	O
called	O
3D	O
Dense	O
Face	O
Alignment	O
(	O
3DDFA	O
)	O
,	O
in	O
which	O
a	O
dense	O
3	O
D	O
face	O
model	O
is	O
fitted	O
to	O
the	O
image	O
via	O
convolutional	O
neutral	O
network	O
(	O
CNN	O
)	O
.	O

Style	O
Translation	O
Facial	B-RESEARCH_PROBLEM
landmark	I-RESEARCH_PROBLEM
detection	E-RESEARCH_PROBLEM
,	O
or	O
face	B-RESEARCH_PROBLEM
alignment	E-RESEARCH_PROBLEM
,	O
is	O
a	O
fundamental	O
task	O
that	O
has	O
been	O
extensively	O
studied	O
.	O
In	O
this	O
paper	O
,	O
we	O
investigate	O
a	O
new	O
perspective	O
of	O
facial	O
landmark	O
detection	O
and	O
demonstrate	O
it	O
leads	O
to	O
further	O
notable	O
improvement	O
.	O
Given	O
that	O
any	O
face	O
images	O
can	O
be	O
factored	O
into	O
space	O
of	O
style	O
that	O
captures	O
lighting	O
,	O
texture	O
and	O
image	O
environment	O
,	O
and	O
a	O
style	O
-	O
invariant	O
structure	O
space	O
,	O
our	O
key	O
idea	O
is	O
to	O
leverage	O
disentangled	O
style	O
and	O
shape	O
space	O
of	O
each	O
individual	O
to	O
augment	O
existing	O
structures	O
via	O
style	O
translation	O
.	O
With	O
these	O
augmented	O
synthetic	O
samples	O
,	O
our	O
semi-supervised	O
model	O
surprisingly	O
outperforms	O
the	O
fully	O
-	O
supervised	O
one	O
by	O
a	O
large	O
margin	O
.	O
Extensive	O
experiments	O
verify	O
the	O
effectiveness	O
of	O
our	O
idea	O
with	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
WFLW	O
[	O
69	O
]	O
,	O
300W	O
[	O
56	O
]	O
,	O
COFW	O
[	O
7	O
]	O
,	O
and	O
AFLW	O
[	O
36	O
]	O
datasets	O
.	O
Our	O
proposed	O
structure	O
is	O
general	O
and	O
could	O
be	O
assembled	O
into	O
any	O
face	B-RESEARCH_PROBLEM
alignment	E-RESEARCH_PROBLEM
frameworks	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
Deep	O
Alignment	O
Network	O
(	O
DAN	O
)	O
,	O
a	O
robust	O
face	B-RESEARCH_PROBLEM
alignment	E-RESEARCH_PROBLEM
method	O
based	O
on	O
a	O
deep	O
neural	O
network	O
architecture	O
.	O
DAN	O
consists	O
of	O
multiple	O
stages	O
,	O
where	O
each	O
stage	O
improves	O
the	O
locations	O
of	O
the	O
facial	O
landmarks	O
estimated	O
by	O
the	O
previous	O
stage	O
.	O
Our	O
method	O
uses	O
entire	O
face	O
images	O
at	O
all	O
stages	O
,	O
contrary	O
to	O
the	O
recently	O
proposed	O
face	O
alignment	O
methods	O
that	O
rely	O
on	O
local	O
patches	O
.	O
This	O
is	O
possible	O
thanks	O
to	O
the	O
use	O
of	O
landmark	O
heatmaps	O
which	O
provide	O
visual	O
information	O
about	O
landmark	O
locations	O
estimated	O
at	O
the	O
previous	O
stages	O
of	O
the	O
algorithm	O
.	O
The	O
use	O
of	O
entire	O
face	O
images	O
rather	O
than	O
patches	O
allows	O
DAN	O
to	O
handle	O
face	O
images	O
with	O
large	O
variation	O
in	O
head	O
pose	O
and	O
difficult	O
initializations	O
.	O
An	O
extensive	O
evaluation	O
on	O
two	O
publicly	O
available	O
datasets	O
shows	O
that	O
DAN	O
reduces	O
the	O
state	O
-	O
of	O
-	O
theart	O
failure	O
rate	O
by	O
up	O
to	O
70	O
%	O
.	O
Our	O
method	O
has	O
also	O
been	O
submitted	O
for	O
evaluation	O
as	O
part	O
of	O
the	O
Menpo	O
challenge	O
.	O

Face	B-RESEARCH_PROBLEM
Alignment	E-RESEARCH_PROBLEM
is	O
an	O
active	O
computer	O
vision	O
domain	O
,	O
that	O
consists	O
in	O
localizing	O
a	O
number	O
of	O
facial	O
landmarks	O
that	O
vary	O
across	O
datasets	O
.	O
State	O
-	O
of	O
-	O
the	O
-	O
art	O
face	O
alignment	O
methods	O
either	O
consist	O
in	O
end	O
-	O
to	O
-	O
end	O
regression	O
,	O
or	O
in	O
refining	O
the	O
shape	O
in	O
a	O
cascaded	O
manner	O
,	O
starting	O
from	O
an	O
initial	O
guess	O
.	O
In	O
this	O
paper	O
,	O
we	O
introduce	O
DeCaFA	O
,	O
an	O
end	O
-	O
to	O
-	O
end	O
deep	O
convolutional	O
cascade	O
architecture	O
for	O
face	O
alignment	O
.	O
DeCaFA	O
uses	O
fully	O
-	O
convolutional	O
stages	O
to	O
keep	O
full	O
spatial	O
resolution	O
throughout	O
the	O
cascade	O
.	O
Between	O
each	O
cascade	O
stage	O
,	O
DeCaFA	O
uses	O
multiple	O
chained	O
transfer	O
layers	O
with	O
spatial	O
softmax	O
to	O
produce	O
landmark	O
-	O
wise	O
attention	O
maps	O
for	O
each	O
of	O
several	O
landmark	O
alignment	O
tasks	O
.	O
Weighted	O
intermediate	O
supervision	O
,	O
as	O
well	O
as	O
efficient	O
feature	O
fusion	O
between	O
the	O
stages	O
allow	O
to	O
learn	O
to	O
progressively	O
refine	O
the	O
attention	O
maps	O
in	O
an	O
end	O
-	O
to	O
-	O
end	O
manner	O
.	O
We	O
show	O
experimentally	O
that	O
DeCaFA	O
significantly	O
outperforms	O
existing	O
approaches	O
on	O
300W	O
,	O
CelebA	O
and	O
WFLW	O
databases	O
.	O

:	O
Dense	O
face	O
alignment	O
(	O
odd	O
rows	O
)	O
and	O
3D	O
face	O
reconstruction	O
(	O
even	O
rows	O
)	O
results	O
from	O
our	O
proposed	O
method	O
.	O
For	O
alignment	O
,	O
only	O
68	O
key	O
points	O
are	O
plotted	O
for	O
clear	O
display	O
;	O
for	O
3D	O
reconstruction	O
,	O
reconstructed	O
shapes	O
are	O
rendered	O
with	O
headlight	O
for	O
better	O
view	O
.	O
Our	O
method	O
offers	O
strong	O
robustness	O
and	O
good	O
performance	O
even	O
in	O
presence	O
of	O
large	O
poses	O
(	O
the	O
3th	O
,	O
4th	O
and	O
5th	O
columns	O
)	O
and	O
occlusions	O
(	O
the	O
6th	O
,	O
7th	O
and	O
8th	O
columns	O
)	O
.	O
Best	O
viewed	O
in	O
color	O
.	O
3	B-RESEARCH_PROBLEM
D	I-RESEARCH_PROBLEM
face	I-RESEARCH_PROBLEM
reconstruction	E-RESEARCH_PROBLEM
from	O
a	O
single	O
2D	O
image	O
is	O
a	O
challenging	O
problem	O
with	O
broad	O
applications	O
.	O
Recent	O
methods	O
typically	O
aim	O
to	O
learn	O
a	O
CNN	O
-	O
based	O
3	O
D	O
face	O
model	O
that	O
regresses	O
coefficients	O
of	O
3D	O
Morphable	O
Model	O
(	O
3	O
DMM	O
)	O
from	O
2D	O
images	O
to	O
render	O
3	O
D	O
face	O
reconstruction	O
or	O
dense	O
face	O
alignment	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
novel	O
face	B-RESEARCH_PROBLEM
detection	E-RESEARCH_PROBLEM
network	O
with	O
three	O
novel	O
contributions	O
that	O
address	O
three	O
key	O
aspects	O
of	O
face	B-RESEARCH_PROBLEM
detection	E-RESEARCH_PROBLEM
,	O
including	O
better	O
feature	O
learning	O
,	O
progressive	O
loss	O
design	O
and	O
anchor	O
assign	O
based	O
data	O
augmentation	O
,	O
respectively	O
.	O
First	O
,	O
we	O
propose	O
a	O
Feature	O
Enhance	O
Module	O
(	O
FEM	O
)	O
for	O
enhancing	O
the	O
original	O
feature	O
maps	O
to	O
extend	O
the	O
single	O
shot	O
detector	O
to	O
dual	O
shot	O
detector	O
.	O
Second	O
,	O
we	O
adopt	O
Progressive	O
Anchor	O
Loss	O
(	O
PAL	O
)	O
computed	O
by	O
two	O
different	O
sets	O
of	O
anchors	O
to	O
effectively	O
facilitate	O
the	O
features	O
.	O
Third	O
,	O
we	O
use	O
an	O
Improved	O
Anchor	O
Matching	O
(	O
IAM	O
)	O
by	O
integrating	O
novel	O
anchor	O
assign	O
strategy	O
into	O
data	O
aug	O
-	O

A	O
unified	O
deep	O
neural	O
network	O
,	O
denoted	O
the	O
multi	O
-scale	O
CNN	O
(	O
MS	O
-	O
CNN	O
)	O
,	O
is	O
proposed	O
for	O
fast	B-RESEARCH_PROBLEM
multi-scale	I-RESEARCH_PROBLEM
object	I-RESEARCH_PROBLEM
detection	E-RESEARCH_PROBLEM
.	O
The	O
MS	O
-	O
CNN	O
consists	O
of	O
a	O
proposal	O
sub-network	O
and	O
a	O
detection	O
sub-network	O
.	O
In	O
the	O
proposal	O
sub-network	O
,	O
detection	O
is	O
performed	O
at	O
multiple	O
output	O
layers	O
,	O
so	O
that	O
receptive	O
fields	O
match	O
objects	O
of	O
different	O
scales	O
.	O
These	O
complementary	O
scale	O
-	O
specific	O
detectors	O
are	O
combined	O
to	O
produce	O
a	O
strong	O
multi-scale	O
object	O
detector	O
.	O
The	O
unified	O
network	O
is	O
learned	O
end	O
-	O
to	O
-	O
end	O
,	O
by	O
optimizing	O
a	O
multi	O
-	O
task	O
loss	O
.	O
Feature	O
upsampling	O
by	O
deconvolution	O
is	O
also	O
explored	O
,	O
as	O
an	O
alternative	O
to	O
input	O
upsampling	O
,	O
to	O
reduce	O
the	O
memory	O
and	O
computation	O
costs	O
.	O
State	O
-	O
of	O
-	O
the	O
-	O
art	O
object	B-RESEARCH_PROBLEM
detection	E-RESEARCH_PROBLEM
performance	O
,	O
at	O
up	O
to	O
15	O
fps	O
,	O
is	O
reported	O
on	O
datasets	O
,	O
such	O
as	O
KITTI	O
and	O
Caltech	O
,	O
containing	O
a	O
substantial	O
number	O
of	O
small	O
objects	O
.	O

Region	O
proposal	O
mechanisms	O
are	O
essential	O
for	O
existing	O
deep	O
learning	O
approaches	O
to	O
object	B-RESEARCH_PROBLEM
detection	E-RESEARCH_PROBLEM
in	O
images	O
.	O
Although	O
they	O
can	O
generally	O
achieve	O
a	O
good	O
detection	O
performance	O
under	O
normal	O
circumstances	O
,	O
their	O
recall	O
in	O
a	O
scene	O
with	O
extreme	O
cases	O
is	O
unacceptably	O
low	O
.	O
This	O
is	O
mainly	O
because	O
bounding	O
box	O
annotations	O
contain	O
much	O
environment	O
noise	O
information	O
,	O
and	O
non-maximum	O
suppression	O
(	O
NMS	O
)	O
is	O
required	O
to	O
select	O
target	O
boxes	O
.	O
Therefore	O
,	O
in	O
this	O
paper	O
,	O
we	O
propose	O
the	O
first	O
anchorfree	O
and	O
NMS	O
-	O
free	O
object	B-RESEARCH_PROBLEM
detection	E-RESEARCH_PROBLEM
model	O
,	O
called	O
weakly	O
supervised	O
multimodal	O
annotation	O
segmentation	O
(	O
WSMA	O
-	O
Seg	O
)	O
,	O
which	O
utilizes	O
segmentation	O
models	O
to	O
achieve	O
an	O
accurate	O
and	O
robust	O
object	O
detection	O
without	O
NMS	O
.	O
In	O
WSMA	O
-	O
Seg	O
,	O
multimodal	O
annotations	O
are	O
proposed	O
to	O
achieve	O
an	O
instance	O
-	O
aware	O
segmentation	O
using	O
weakly	O
supervised	O
bounding	O
boxes	O
;	O
we	O
also	O
develop	O
a	O
run-data	O
-	O
based	O
following	O
algorithm	O
to	O
trace	O
contours	O
of	O
objects	O
.	O
In	O
addition	O
,	O
we	O
propose	O
a	O
multi-scale	O
pooling	O
segmentation	O
(	O
MSP	O
-	O
Seg	O
)	O
as	O
the	O
underlying	O
segmentation	O
model	O
of	O
WSMA	O
-	O
Seg	O
to	O
achieve	O
a	O
more	O
accurate	O
segmentation	O
and	O
to	O
enhance	O
the	O
detection	O
accuracy	O
of	O
WSMA	O
-	O
Seg.	O
Experimental	O
results	O
on	O
multiple	O
datasets	O
show	O
that	O
the	O
proposed	O
WSMA	O
-	O
Seg	O
approach	O
outperforms	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
detectors	O
.	O

Though	O
tremendous	O
strides	O
have	O
been	O
made	O
in	O
uncontrolled	O
face	O
detection	O
,	O
accurate	O
and	O
efficient	O
face	B-RESEARCH_PROBLEM
localisation	E-RESEARCH_PROBLEM
in	O
the	O
wild	O
remains	O
an	O
open	O
challenge	O
.	O
This	O
paper	O
presents	O
a	O
robust	O
single	O
-	O
stage	O
face	O
detector	O
,	O
named	O
RetinaFace	O
,	O
which	O
performs	O
pixel	O
-	O
wise	O
face	B-RESEARCH_PROBLEM
localisation	E-RESEARCH_PROBLEM
on	O
various	O
scales	O
of	O
faces	O
by	O
taking	O
advantages	O
of	O
joint	O
extra-supervised	O
and	O
self	O
-	O
supervised	O
multi-task	O
learning	O
.	O
Specifically	O
,	O
We	O
make	O
contributions	O
in	O
the	O
following	O
five	O
aspects	O
:	O
(	O
1	O
)	O
We	O
manually	O
annotate	O
five	O
facial	O
landmarks	O
on	O
the	O
WIDER	O
FACE	O
dataset	O
and	O
observe	O
significant	O
improvement	O
in	O
hard	O
face	O
detection	O
with	O
the	O
assistance	O
of	O
this	O
extra	O
supervision	O
signal	O
.	O
(	O
2	O
)	O
We	O
further	O
add	O
a	O
selfsupervised	O
mesh	O
decoder	O
branch	O
for	O
predicting	O
a	O
pixel	O
-	O
wise	O
3D	O
shape	O
face	O
information	O
in	O
parallel	O
with	O
the	O
existing	O
supervised	O
branches	O
.	O
(	O
3	O
)	O
On	O
the	O
WIDER	O
FACE	O
hard	O
test	O
set	O
,	O
RetinaFace	O
outperforms	O
the	O
state	O
of	O
the	O
art	O
average	O
precision	O
(	O
AP	O
)	O
by	O
1.1	O
%	O
(	O
achieving	O
AP	O
equal	O
to	O
91.4	O
%	O
)	O
.	O

Although	O
tremendous	O
strides	O
have	O
been	O
made	O
in	O
face	B-RESEARCH_PROBLEM
detection	E-RESEARCH_PROBLEM
,	O
one	O
of	O
the	O
remaining	O
open	O
challenges	O
is	O
to	O
achieve	O
real	O
-	O
time	O
speed	O
on	O
the	O
CPU	O
as	O
well	O
as	O
maintain	O
high	O
performance	O
,	O
since	O
effective	O
models	O
for	O
face	B-RESEARCH_PROBLEM
detection	E-RESEARCH_PROBLEM
tend	O
to	O
be	O
computationally	O
prohibitive	O
.	O
To	O
address	O
this	O
challenge	O
,	O
we	O
propose	O
a	O
novel	O
face	O
detector	O
,	O
named	O
FaceBoxes	O
,	O
with	O
superior	O
performance	O
on	O
both	O
speed	O
and	O
accuracy	O
.	O
Specifically	O
,	O
our	O
method	O
has	O
a	O
lightweight	O
yet	O
powerful	O
network	O
structure	O
that	O
consists	O
of	O
the	O
Rapidly	O
Digested	O
Convolutional	O
Layers	O
(	O
RDCL	O
)	O
and	O
the	O
Multiple	O
Scale	O
Convolutional	O
Layers	O
(	O
MSCL	O
)	O
.	O
The	O
RDCL	O
is	O
designed	O
to	O
enable	O
Face	O
-	O
Boxes	O
to	O
achieve	O
real	O
-	O
time	O
speed	O
on	O
the	O
CPU	O
.	O
The	O
MSCL	O
aims	O
at	O
enriching	O
the	O
receptive	O
fields	O
and	O
discretizing	O
anchors	O
over	O
different	O
layers	O
to	O
handle	O
faces	O
of	O
various	O
scales	O
.	O
Besides	O
,	O
we	O
propose	O
a	O
new	O
anchor	O
densification	O
strategy	O
to	O
make	O
different	O
types	O
of	O
anchors	O
have	O
the	O
same	O
density	O
on	O
the	O
image	O
,	O
which	O
significantly	O
improves	O
the	O
recall	O
rate	O
of	O
small	O
faces	O
.	O
As	O
a	O
consequence	O
,	O
the	O
proposed	O
detector	O
runs	O
at	O
20	O
FPS	O
on	O
a	O
single	O
CPU	O
core	O
and	O
125	O
FPS	O
using	O
a	O
GPU	O
for	O
VGA	O
-	O
resolution	O
images	O
.	O

Face	B-RESEARCH_PROBLEM
detection	E-RESEARCH_PROBLEM
has	O
been	O
well	O
studied	O
for	O
many	O
years	O
and	O
one	O
of	O
remaining	O
challenges	O
is	O
to	O
detect	O
small	O
,	O
blurred	O
and	O
partially	O
occluded	O
faces	O
in	O
uncontrolled	O
environment	O
.	O
This	O
paper	O
proposes	O
a	O
novel	O
contextassisted	O
single	O
shot	O
face	O
detector	O
,	O
named	O
PyramidBox	O
to	O
handle	O
the	O
hard	O
face	O
detection	O
problem	O
.	O
Observing	O
the	O
importance	O
of	O
the	O
context	O
,	O
we	O
improve	O
the	O
utilization	O
of	O
contextual	O
information	O
in	O
the	O
following	O
three	O
aspects	O
.	O
First	O
,	O
we	O
design	O
a	O
novel	O
context	O
anchor	O
to	O
supervise	O
high	O
-	O
level	O
contextual	O
feature	O
learning	O
by	O
a	O
semi-supervised	O
method	O
,	O
which	O
we	O
call	O
it	O
PyramidAnchors	O
.	O
Second	O
,	O
we	O
propose	O
the	O
Low	O
-	O
level	O
Feature	O
Pyramid	O
Network	O
to	O
combine	O
adequate	O
high	O
-	O
level	O
context	O
semantic	O
feature	O
and	O
Low	O
-	O
level	O
facial	O
feature	O
together	O
,	O
which	O
also	O
allows	O
the	O
PyramidBox	O
to	O
predict	O
faces	O
of	O
all	O
scales	O
in	O
a	O
single	O
shot	O
.	O
Third	O
,	O
we	O
introduce	O
a	O
contextsensitive	O
structure	O
to	O
increase	O
the	O
capacity	O
of	O
prediction	O
network	O
to	O
improve	O
the	O
final	O
accuracy	O
of	O
output	O
.	O
In	O
addition	O
,	O
we	O
use	O
the	O
method	O
of	O
Data	O
-	O
anchor	O
-	O
sampling	O
to	O
augment	O
the	O
training	O
samples	O
across	O
different	O
scales	O
,	O
which	O
increases	O
the	O
diversity	O
of	O
training	O
data	O
for	O
smaller	O
faces	O
.	O

Robust	O
face	B-RESEARCH_PROBLEM
detection	E-RESEARCH_PROBLEM
in	O
the	O
wild	O
is	O
one	O
of	O
the	O
ultimate	O
components	O
to	O
support	O
various	O
facial	O
related	O
problems	O
,	O
i.e.	O
unconstrained	O
face	O
recognition	O
,	O
facial	O
periocular	O
recognition	O
,	O
facial	O
landmarking	O
and	O
pose	O
estimation	O
,	O
facial	O
expression	O
recognition	O
,	O
3	O
D	O
facial	O
model	O
construction	O
,	O
etc	O
.	O
Although	O
the	O
face	B-RESEARCH_PROBLEM
detection	E-RESEARCH_PROBLEM
problem	O
has	O
been	O
intensely	O
studied	O
for	O
decades	O
with	O
various	O
commercial	O
applications	O
,	O
it	O
still	O
meets	O
problems	O
in	O
some	O
real	O
-	O
world	O
scenarios	O
due	O
to	O
numerous	O
challenges	O
,	O
e.g.	O
heavy	O
facial	O
occlusions	O
,	O
extremely	O
low	O
resolutions	O
,	O
strong	O
illumination	O
,	O
exceptionally	O
pose	O
variations	O
,	O
image	O
or	O
video	O
compression	O
artifacts	O
,	O
etc	O
.	O
In	O
this	O
paper	O
,	O
we	O
present	O
a	O
face	B-RESEARCH_PROBLEM
detection	E-RESEARCH_PROBLEM
approach	O
named	O
Contextual	O
Multi	O
-	O
Scale	O
Region	O
-	O
based	O
Convolution	O
Neural	O
Network	O
(	O
CMS	O
-	O
RCNN	O
)	O
to	O
robustly	O
solve	O
the	O
problems	O
mentioned	O
above	O
.	O
Similar	O
to	O
the	O
region	O
-	O
based	O
CNNs	O
,	O
our	O
proposed	O
network	O
consists	O
of	O
the	O
region	O
proposal	O
component	O
and	O
the	O
region	O
-	O
of	O
-	O
interest	O
(	O
RoI	O
)	O
detection	O
component	O
.	O
However	O
,	O
far	O
apart	O
of	O
that	O
network	O
,	O
there	O
are	O
two	O
main	O
contributions	O
in	O
our	O
proposed	O
network	O
that	O
play	O
a	O
significant	O
role	O
to	O
achieve	O
the	O
state	O
-	O
of	O
-	O
theart	O
performance	O
in	O
face	O
detection	O
.	O
Firstly	O
,	O
the	O
multi-scale	O
information	O
is	O
grouped	O
both	O
in	O
region	O
proposal	O
and	O
RoI	O
detection	O
to	O
deal	O
with	O
tiny	O
face	O
regions	O
.	O
Secondly	O
,	O
our	O
proposed	O
network	O
allows	O
explicit	O
body	O
contextual	O
reasoning	O
in	O
the	O
network	O
inspired	O
from	O
the	O
intuition	O
of	O
human	O
vision	O
system	O
.	O

High	O
performance	O
face	B-RESEARCH_PROBLEM
detection	E-RESEARCH_PROBLEM
remains	O
a	O
very	O
challenging	O
problem	O
,	O
especially	O
when	O
there	O
exists	O
many	O
tiny	O
faces	O
.	O
This	O
paper	O
presents	O
a	O
novel	O
single	O
-	O
shot	O
face	O
detector	O
,	O
named	O
Selective	O
Refinement	O
Network	O
(	O
SRN	O
)	O
,	O
which	O
introduces	O
novel	O
twostep	O
classification	O
and	O
regression	O
operations	O
selectively	O
into	O
an	O
anchor-	O
based	O
face	O
detector	O
to	O
reduce	O
false	O
positives	O
and	O
improve	O
location	O
accuracy	O
simultaneously	O
.	O
In	O
particular	O
,	O
the	O
SRN	O
consists	O
of	O
two	O
modules	O
:	O
the	O
Selective	O
Two	O
-	O
step	O
Classification	O
(	O
STC	O
)	O
module	O
and	O
the	O
Selective	O
Two	O
-	O
step	O
Regression	O
(	O
STR	O
)	O
module	O
.	O
The	O
STC	O
aims	O
to	O
filter	O
out	O
most	O
simple	O
negative	O
anchors	O
from	O
low	O
level	O
detection	O
layers	O
to	O
reduce	O
the	O
search	O
space	O
for	O
the	O
subsequent	O
classifier	O
,	O
while	O
the	O
STR	O
is	O
designed	O
to	O
coarsely	O
adjust	O
the	O
locations	O
and	O
sizes	O
of	O
anchors	O
from	O
high	O
level	O
detection	O
layers	O
to	O
provide	O
better	O
initialization	O
for	O
the	O
subsequent	O
regressor	O
.	O
Moreover	O
,	O
we	O
design	O
a	O
Receptive	O
Field	O
Enhancement	O
(	O
RFE	O
)	O
block	O
to	O
provide	O
more	O
diverse	O
receptive	O
field	O
,	O
which	O
helps	O
to	O
better	O
capture	O
faces	O
in	O
some	O
extreme	O
poses	O
.	O
As	O
a	O
consequence	O
,	O
the	O
proposed	O
SRN	O
detector	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
all	O
the	O
widely	O
used	O
face	B-RESEARCH_PROBLEM
detection	E-RESEARCH_PROBLEM
benchmarks	O
,	O
including	O
AFW	O
,	O
PASCAL	O
face	O
,	O
FDDB	O
,	O
and	O
WIDER	O
FACE	O
datasets	O
.	O
Codes	O
will	O
be	O
released	O
to	O
facilitate	O
further	O
studies	O
on	O
the	O
face	B-RESEARCH_PROBLEM
detection	E-RESEARCH_PROBLEM
problem	O
.	O

Face	B-RESEARCH_PROBLEM
detection	E-RESEARCH_PROBLEM
has	O
drawn	O
much	O
attention	O
in	O
recent	O
decades	O
since	O
the	O
seminal	O
work	O
by	O
Viola	O
and	O
Jones	O
.	O
While	O
many	O
subsequences	O
have	O
improved	O
the	O
work	O
with	O
more	O
powerful	O
learning	O
algorithms	O
,	O
the	O
feature	O
representation	O
used	O
for	O
face	O
detection	O
still	O
ca	O
n't	O
meet	O
the	O
demand	O
for	O
effectively	O
and	O
efficiently	O
handling	O
faces	O
with	O
large	O
appearance	O
variance	O
in	O
the	O
wild	O
.	O
To	O
solve	O
this	O
bottleneck	O
,	O
we	O
borrow	O
the	O
concept	O
of	O
channel	O
features	O
to	O
the	O
face	O
detection	O
domain	O
,	O
which	O
extends	O
the	O
image	O
channel	O
to	O
diverse	O
types	O
like	O
gradient	O
magnitude	O
and	O
oriented	O
gradient	O
histograms	O
and	O
therefore	O
encodes	O
rich	O
information	O
in	O
a	O
simple	O
form	O
.	O
We	O
adopt	O
a	O
novel	O
variant	O
called	O
aggregate	O
channel	O
features	O
,	O
make	O
a	O
full	O
exploration	O
of	O
feature	O
design	O
,	O
and	O
discover	O
a	O
multiscale	O
version	O
of	O
features	O
with	O
better	O
performance	O
.	O
To	O
deal	O
with	O
poses	O
of	O
faces	O
in	O
the	O
wild	O
,	O
we	O
propose	O
a	O
multi-view	O
detection	O
approach	O
featuring	O
score	O
re-ranking	O
and	O
detection	O
adjustment	O
.	O
Following	O
the	O
learning	O
pipelines	O
in	O
Viola	O
-	O
Jones	O
framework	O
,	O
the	O
multi-view	O
face	O
detector	O
using	O
aggregate	O
channel	O
features	O
shows	O
competitive	O
performance	O
against	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
algorithms	O
on	O
AFW	O
and	O
FDDB	O
testsets	O
,	O
while	O
runs	O
at	O
42	O
FPS	O
on	O
VGA	O
images	O
.	O

Large	O
pose	O
variations	O
remain	O
to	O
be	O
a	O
challenge	O
that	O
confronts	O
real	B-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
word	I-RESEARCH_PROBLEM
face	I-RESEARCH_PROBLEM
detection	E-RESEARCH_PROBLEM
.	O
We	O
propose	O
a	O
new	O
cascaded	O
Convolutional	O
Neural	O
Network	O
,	O
dubbed	O
the	O
name	O
Supervised	O
Transformer	O
Network	O
,	O
to	O
address	O
this	O
challenge	O
.	O
The	O
first	O
stage	O
is	O
a	O
multi-task	O
Region	O
Proposal	O
Network	O
(	O
RPN	O
)	O
,	O
which	O
simultaneously	O
predicts	O
candidate	O
face	O
regions	O
along	O
with	O
associated	O
facial	O
landmarks	O
.	O
The	O
candidate	O
regions	O
are	O
then	O
warped	O
by	O
mapping	O
the	O
detected	O
facial	O
landmarks	O
to	O
their	O
canonical	O
positions	O
to	O
better	O
normalize	O
the	O
face	O
patterns	O
.	O
The	O
second	O
stage	O
,	O
which	O
is	O
a	O
RCNN	O
,	O
then	O
verifies	O
if	O
the	O
warped	O
candidate	O
regions	O
are	O
valid	O
faces	O
or	O
not	O
.	O
We	O
conduct	O
end	O
-	O
to	O
-	O
end	O
learning	O
of	O
the	O
cascaded	O
network	O
,	O
including	O
optimizing	O
the	O
canonical	O
positions	O
of	O
the	O
facial	O
landmarks	O
.	O
This	O
supervised	O
learning	O
of	O
the	O
transformations	O
automatically	O
selects	O
the	O
best	O
scale	O
to	O
differentiate	O
face	O
/	O
non	O
-	O
face	O
patterns	O
.	O

We	O
propose	O
a	O
method	O
to	O
address	O
challenges	O
in	O
unconstrained	O
face	B-RESEARCH_PROBLEM
detection	E-RESEARCH_PROBLEM
,	O
such	O
as	O
arbitrary	O
pose	O
variations	O
and	O
occlusions	O
.	O
First	O
,	O
a	O
new	O
image	O
feature	O
called	O
Normalized	O
Pixel	O
Difference	O
(	O
NPD	O
)	O
is	O
proposed	O
.	O
NPD	O
feature	O
is	O
computed	O
as	O
the	O
difference	O
to	O
sum	O
ratio	O
between	O
two	O
pixel	O
values	O
,	O
inspired	O
by	O
the	O
Weber	O
Fraction	O
in	O
experimental	O
psychology	O
.	O
The	O
new	O
feature	O
is	O
scale	O
invariant	O
,	O
bounded	O
,	O
and	O
is	O
able	O
to	O
reconstruct	O
the	O
original	O
image	O
.	O
Second	O
,	O
we	O
propose	O
a	O
deep	O
quadratic	O
tree	O
to	O
learn	O
the	O
optimal	O
subset	O
of	O
NPD	O
features	O
and	O
their	O
combinations	O
,	O
so	O
that	O
complex	O
face	O
manifolds	O
can	O
be	O
partitioned	O
by	O
the	O
learned	O
rules	O
.	O
This	O
way	O
,	O
only	O
a	O
single	O
soft	O
-	O
cascade	O
classifier	O
is	O
needed	O
to	O
handle	O
unconstrained	O
face	B-RESEARCH_PROBLEM
detection	E-RESEARCH_PROBLEM
.	O
Furthermore	O
,	O
we	O
show	O
that	O
the	O
NPD	O
features	O
can	O
be	O
efficiently	O
obtained	O
from	O
a	O
lookup	O
table	O
,	O
and	O
the	O
detection	O
template	O
can	O
be	O
easily	O
scaled	O
,	O
making	O
the	O
proposed	O
face	O
detector	O
very	O
fast	O
.	O

Face	B-RESEARCH_PROBLEM
detection	E-RESEARCH_PROBLEM
,	O
as	O
a	O
fundamental	O
technology	O
for	O
various	O
applications	O
,	O
is	O
always	O
deployed	O
on	O
edge	O
devices	O
which	O
have	O
limited	O
memory	O
storage	O
and	O
low	O
computing	O
power	O
.	O
This	O
paper	O
introduces	O
a	O
Light	O
and	O
Fast	O
Face	O
Detector	O
(	O
LFFD	O
)	O
for	O
edge	O
devices	O
.	O
The	O
proposed	O
method	O
is	O
anchorfree	O
and	O
belongs	O
to	O
the	O
one	O
-	O
stage	O
category	O
.	O
Specifically	O
,	O
we	O
rethink	O
the	O
importance	O
of	O
receptive	O
field	O
(	O
RF	O
)	O
and	O
effective	O
receptive	O
field	O
(	O
ERF	O
)	O
in	O
the	O
background	O
of	O
face	O
detection	O
.	O
Essentially	O
,	O
the	O
RFs	O
of	O
neurons	O
in	O
a	O
certain	O
layer	O
are	O
distributed	O
regularly	O
in	O
the	O
input	O
image	O
and	O
theses	O
RFs	O
are	O
natural	O
"	O
anchors	O
"	O
.	O
Combining	O
RF	O
"	O
anchors	O
"	O
and	O
appropriate	O
RF	O
strides	O
,	O
the	O
proposed	O
method	O
can	O
detect	O
a	O
large	O
range	O
of	O
continuous	O
face	O
scales	O
with	O
100	O
%	O
coverage	O
in	O
theory	O
.	O
The	O
insightful	O
understanding	O
of	O
relations	O
between	O
ERF	O
and	O
face	O
scales	O
motivates	O
an	O
efficient	O
backbone	O
for	O
onestage	O
detection	O
.	O

narrative	O
?	O
Abstract	O
-	O
Face	B-RESEARCH_PROBLEM
detection	I-RESEARCH_PROBLEM
and	I-RESEARCH_PROBLEM
alignment	I-RESEARCH_PROBLEM
in	I-RESEARCH_PROBLEM
unconstrained	I-RESEARCH_PROBLEM
environment	E-RESEARCH_PROBLEM
are	O
challenging	O
due	O
to	O
various	O
poses	O
,	O
illuminations	O
and	O
occlusions	O
.	O
Recent	O
studies	O
show	O
that	O
deep	O
learning	O
approaches	O
can	O
achieve	O
impressive	O
performance	O
on	O
these	O
two	O
tasks	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
deep	O
cascaded	O
multi-task	O
framework	O
which	O
exploits	O
the	O
inherent	O
correlation	O
between	O
them	O
to	O
boost	O
up	O
their	O
performance	O
.	O
In	O
particular	O
,	O
our	O
framework	O
adopts	O
a	O
cascaded	O
structure	O
with	O
three	O
stages	O
of	O
carefully	O
designed	O
deep	O
convolutional	O
networks	O
that	O
predict	O
face	O
and	O
landmark	O
location	O
in	O
a	O
coarse	O
-	O
to	O
-	O
fine	O
manner	O
.	O
In	O
addition	O
,	O
in	O
the	O
learning	O
process	O
,	O
we	O
propose	O
a	O
new	O
online	O
hard	O
sample	O
mining	O
strategy	O
that	O
can	O
improve	O
the	O
performance	O
automatically	O
without	O
manual	O
sample	O
selection	O
.	O
Our	O
method	O
achieves	O
superior	O
accuracy	O
over	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
techniques	O
on	O
the	O
challenging	O
FDDB	O
and	O
WIDER	O
FACE	O
benchmark	O
for	O
face	O
detection	O
,	O
and	O
AFLW	O
benchmark	O
for	O
face	O
alignment	O
,	O
while	O
keeps	O
real	O
time	O
performance	O
.	O

Since	O
convolutional	O
neural	O
network	O
(	O
CNN	O
)	O
lacks	O
an	O
inherent	O
mechanism	O
to	O
handle	O
large	O
scale	O
variations	O
,	O
we	O
always	O
need	O
to	O
compute	O
feature	O
maps	O
multiple	O
times	O
for	O
multiscale	B-RESEARCH_PROBLEM
object	I-RESEARCH_PROBLEM
detection	E-RESEARCH_PROBLEM
,	O
which	O
has	O
the	O
bottleneck	O
of	O
computational	O
cost	O
in	O
practice	O
.	O
To	O
address	O
this	O
,	O
we	O
devise	O
a	O
recurrent	O
scale	O
approximation	O
(	O
RSA	O
)	O
to	O
compute	O
feature	O
map	O
once	O
only	O
,	O
and	O
only	O
through	O
this	O
map	O
can	O
we	O
approximate	O
the	O
rest	O
maps	O
on	O
other	O
levels	O
.	O
At	O
the	O
core	O
of	O
RSA	O
is	O
the	O
recursive	O
rolling	O
out	O
mechanism	O
:	O
given	O
an	O
initial	O
map	O
at	O
a	O
particular	O
scale	O
,	O
it	O
generates	O
the	O
prediction	O
at	O
a	O
smaller	O
scale	O
that	O
is	O
half	O
the	O
size	O
of	O
input	O
.	O
To	O
further	O
increase	O
efficiency	O
and	O
accuracy	O
,	O
we	O
(	O
a	O
)	O
:	O
design	O
a	O
scale	O
-	O
forecast	O
network	O
to	O
globally	O
predict	O
potential	O
scales	O
in	O
the	O
image	O
since	O
there	O
is	O
no	O
need	O
to	O
compute	O
maps	O
on	O
all	O
levels	O
of	O
the	O
pyramid	O
.	O
(	O
b	O
)	O
:	O
propose	O
a	O
landmark	O
retracing	O
network	O
(	O
LRN	O
)	O
to	O
trace	O
back	O
locations	O
of	O
the	O
regressed	O
landmarks	O
and	O
generate	O
a	O
confidence	O
score	O
for	O
each	O
landmark	O
;	O
LRN	O
can	O
effectively	O
alleviate	O
false	O
positives	O
caused	O
by	O
the	O
accumulated	O
error	O
in	O
RSA	O
.	O
The	O
whole	O
system	O
can	O
be	O
trained	O
end	O
-	O
to	O
-	O
end	O
in	O
a	O
unified	O
CNN	O
framework	O
.	O
Experiments	O
demonstrate	O
that	O
our	O
proposed	O
algorithm	O
is	O
superior	O
against	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
on	O
face	O
detection	O
benchmarks	O
and	O
achieves	O
comparable	O
results	O
for	O
generic	O
proposal	O
generation	O
.	O

Face	B-RESEARCH_PROBLEM
detection	E-RESEARCH_PROBLEM
has	O
achieved	O
great	O
success	O
using	O
the	O
region	O
-	O
based	O
methods	O
.	O
In	O
this	O
report	O
,	O
we	O
propose	O
a	O
region	O
-	O
based	O
face	O
detector	O
applying	O
deep	O
networks	O
in	O
a	O
fully	O
convolutional	O
fashion	O
,	O
named	O
Face	O
R	O
-	O
FCN	O
.	O
Based	O
on	O
Region	O
-	O
based	O
Fully	O
Convolutional	O
Networks	O
(	O
R	O
-	O
FCN	O
)	O
,	O
our	O
face	O
detector	O
is	O
more	O
accurate	O
and	O
computationally	O
efficient	O
compared	O
with	O
the	O
previous	O
R	O
-	O
CNN	O
based	O
face	O
detectors	O
.	O
In	O
our	O
approach	O
,	O
we	O
adopt	O
the	O
fully	O
convolutional	O
Residual	O
Network	O
(	O
ResNet	O
)	O
as	O
the	O
backbone	O
network	O
.	O
Particularly	O
,	O
we	O
exploit	O
several	O
new	O
techniques	O
including	O
position	O
-	O
sensitive	O
average	O
pooling	O
,	O
multi-scale	O
training	O
and	O
testing	O
and	O
on	O
-	O
line	O
hard	O
example	O
mining	O
strategy	O
to	O
improve	O
the	O
detection	O
accuracy	O
.	O
Over	O
two	O
most	O
popular	O
and	O
challenging	O
face	O
detection	O
benchmarks	O
,	O
FDDB	O
and	O
WIDER	O
FACE	O
,	O
Face	O
R	O
-	O
FCN	O
achieves	O
superior	O
performance	O
over	O
state	O
-	O
of	O
-	O
the	O
-	O
arts	O
.	O

:	O
We	O
describe	O
a	O
detector	O
that	O
can	O
find	O
around	O
800	O
faces	O
out	O
of	O
the	O
reportedly	O
1000	O
present	O
,	O
by	O
making	O
use	O
of	O
novel	O
characterizations	O
of	O
scale	O
,	O
resolution	O
,	O
and	O
context	O
to	O
find	O
small	O
objects	O
.	O
Detector	O
confidence	O
is	O
given	O
by	O
the	O
colorbar	O
on	O
the	O
right	O
:	O
can	O
you	O
confidently	O
identify	O
errors	O
?	O
Though	O
tremendous	O
strides	O
have	O
been	O
made	O
in	O
object	O
recognition	O
,	O
one	O
of	O
the	O
remaining	O
open	O
challenges	O
is	O
detecting	B-RESEARCH_PROBLEM
small	I-RESEARCH_PROBLEM
objects	E-RESEARCH_PROBLEM
.	O
We	O
explore	O
three	O
aspects	O
of	O
the	O
problem	O
in	O
the	O
context	O
of	O
finding	B-RESEARCH_PROBLEM
small	I-RESEARCH_PROBLEM
faces	E-RESEARCH_PROBLEM
:	O
the	O
role	O
of	O
scale	O
invariance	O
,	O
image	O
resolution	O
,	O
and	O
contextual	O
reasoning	O
.	O
While	O
most	O
recognition	O
approaches	O
aim	O
to	O
be	O
scale	O
-	O
invariant	O
,	O
the	O
cues	O
for	O
recognizing	O
a	O
3	O
px	O
tall	O
face	O
are	O
fundamentally	O
different	O
than	O
those	O
for	O
recognizing	O
a	O
300	O
px	O
tall	O
face	O
.	O
We	O
take	O
a	O
different	O
approach	O
and	O
train	O
separate	O
detectors	O
for	O
different	O
scales	O
.	O

This	O
paper	O
describes	O
a	O
simple	O
but	O
competitive	O
unsupervised	O
system	O
for	O
hypernym	B-RESEARCH_PROBLEM
discovery	E-RESEARCH_PROBLEM
.	O
The	O
system	O
uses	O
skip	O
-	O
gram	O
word	O
embeddings	O
with	O
negative	O
sampling	O
,	O
trained	O
on	O
specialised	O
corpora	O
.	O
Candidate	O
hypernyms	O
for	O
an	O
input	O
word	O
are	O
predicted	O
based	O
on	O
cosine	O
similarity	O
scores	O
.	O
Two	O
sets	O
of	O
word	O
embedding	O
models	O
were	O
trained	O
separately	O
on	O
two	O
specialised	O
corpora	O
:	O
a	O
medical	O
corpus	O
and	O
a	O
music	O
industry	O
corpus	O
.	O
Our	O
system	O
scored	O
highest	O
in	O
the	O
medical	O
domain	O
among	O
the	O
competing	O
unsupervised	O
systems	O
but	O
performed	O
poorly	O
on	O
the	O
music	O
industry	O
domain	O
.	O
Our	O
approach	O
does	O
not	O
depend	O
on	O
any	O
external	O
data	O
other	O
than	O
raw	O
specialised	O
corpora	O
.	O

This	O
paper	O
describes	O
a	O
hypernym	B-RESEARCH_PROBLEM
discovery	E-RESEARCH_PROBLEM
system	O
for	O
our	O
participation	O
in	O
the	O
SemEval	O
-	O
2018	O
Task	O
9	O
,	O
which	O
aims	O
to	O
discover	O
the	O
best	O
(	O
set	O
of	O
)	O
candidate	O
hypernyms	O
for	O
input	O
concepts	O
or	O
entities	O
,	O
given	O
the	O
search	O
space	O
of	O
a	O
pre-defined	O
vocabulary	O
.	O
We	O
introduce	O
a	O
neural	O
network	O
architecture	O
for	O
the	O
concerned	O
task	O
and	O
empirically	O
study	O
various	O
neural	O
network	O
models	O
to	O
build	O
the	O
representations	O
in	O
latent	O
space	O
for	O
words	O
and	O
phrases	O
.	O
The	O
evaluated	O
models	O
include	O
convolutional	O
neural	O
network	O
,	O
long	O
-	O
short	O
term	O
memory	O
network	O
,	O
gated	O
recurrent	O
unit	O
and	O
recurrent	O
convolutional	O
neural	O
network	O
.	O
We	O
also	O
explore	O
different	O
embedding	O
methods	O
,	O
including	O
word	O
embedding	O
and	O
sense	O
embedding	O
for	O
better	O
performance	O
.	O
Recently	O
,	O
neural	O
network	O
(	O
NN	O
)	O
models	O
have	O
shown	O
competitive	O
or	O
even	O
better	O
results	O
than	O
traditional	O
linear	O
models	O
with	O
handcrafted	O
sparse	O
fea	O
-	O

Hypernym	B-RESEARCH_PROBLEM
discovery	E-RESEARCH_PROBLEM
aims	O
to	O
discover	O
the	O
hypernym	O
word	O
sets	O
given	O
a	O
hyponym	O
word	O
and	O
proper	O
corpus	O
.	O
This	O
paper	O
proposes	O
a	O
simple	O
but	O
effective	O
method	O
for	O
the	O
discovery	O
of	O
hypernym	O
sets	O
based	O
on	O
word	O
embedding	O
,	O
which	O
can	O
be	O
used	O
to	O
measure	O
the	O
contextual	O
similarities	O
between	O
words	O
.	O
Given	O
a	O
test	O
hyponym	O
word	O
,	O
we	O
get	O
its	O
hypernym	O
lists	O
by	O
computing	O
the	O
similarities	O
between	O
the	O
hyponym	O
word	O
and	O
words	O
in	O
the	O
training	O
data	O
,	O
and	O
fill	O
the	O
test	O
word	O
's	O
hypernym	O
lists	O
with	O
the	O
hypernym	O
list	O
in	O
the	O
training	O
set	O
of	O
the	O
nearest	O
similarity	O
distance	O
to	O
the	O
test	O
word	O
.	O
In	O
SemEval	O
2018	O
task9	O
,	O
our	O
results	O
,	O
achieve	O
1st	O
on	O
Spanish	O
,	O
2nd	O
on	O
Italian	O
,	O
6th	O
on	O
English	O
in	O
the	O
metric	O
of	O
MAP	O
.	O

This	O
paper	O
describes	O
300	O
-	O
sparsans	O
'	O
participation	O
in	O
SemEval	O
-	O
2018	O
Task	O
9	O
:	O
Hypernym	B-RESEARCH_PROBLEM
Discovery	E-RESEARCH_PROBLEM
,	O
with	O
a	O
system	O
based	O
on	O
sparse	O
coding	O
and	O
a	O
formal	O
concept	O
hierarchy	O
obtained	O
from	O
word	O
embeddings	O
.	O
Our	O
system	O
took	O
first	O
place	O
in	O
subtasks	O
(	O
1B	O
)	O
Italian	O
(	O
all	O
and	O
entities	O
)	O
,	O
(	O
1C	O
)	O
Spanish	O
entities	O
,	O
and	O
(	O
2B	O
)	O
music	O
entities	O
.	O

In	O
recent	O
years	O
many	O
deep	O
neural	O
networks	O
have	O
been	O
proposed	O
to	O
solve	O
Reading	B-RESEARCH_PROBLEM
Comprehension	E-RESEARCH_PROBLEM
(	O
RC	S-RESEARCH_PROBLEM
)	O
tasks	O
.	O
Most	O
of	O
these	O
models	O
suffer	O
from	O
reasoning	O
overlong	O
documents	O
and	O
do	O
not	O
trivially	O
generalize	O
to	O
cases	O
where	O
the	O
answer	O
is	O
not	O
present	O
as	O
a	O
span	O
in	O
a	O
given	O
document	O
.	O
We	O
present	O
a	O
novel	O
neural	O
-	O
based	O
architecture	O
that	O
is	O
capable	O
of	O
extracting	O
relevant	O
regions	O
based	O
on	O
a	O
given	O
question	O
-	O
document	O
pair	O
and	O
generating	O
a	O
well	O
-	O
formed	O
answer	O
.	O
To	O
show	O
the	O
effectiveness	O
of	O
our	O
architecture	O
,	O
we	O
conducted	O
several	O
experiments	O
on	O
the	O
recently	O
proposed	O
and	O
challenging	O
RC	S-RESEARCH_PROBLEM
dataset	O
'	O
Nar	O
-	O
rative	O
QA	O
'	O
.	O
The	O
proposed	O
architecture	O
outperforms	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
(	O
Tay	O
et	O
al.	O
,	O
2018	O
)	O
by	O
12.62	O
%	O
(	O
ROUGE	O
-	O
L	O
)	O
relative	O
improvement	O
.	O

In	O
the	O
Story	B-RESEARCH_PROBLEM
Cloze	I-RESEARCH_PROBLEM
Test	E-RESEARCH_PROBLEM
,	O
a	O
system	O
is	O
presented	O
with	O
a	O
4	O
-	O
sentence	O
prompt	O
to	O
a	O
story	O
,	O
and	O
must	O
determine	O
which	O
one	O
of	O
two	O
potential	O
endings	O
is	O
the	O
'	O
right	O
'	O
ending	O
to	O
the	O
story	O
.	O
Previous	O
work	O
has	O
shown	O
that	O
ignoring	O
the	O
training	O
set	O
and	O
training	O
a	O
model	O
on	O
the	O
validation	O
set	O
can	O
achieve	O
high	O
accuracy	O
on	O
this	O
task	O
due	O
to	O
stylistic	O
differences	O
between	O
the	O
story	O
endings	O
in	O
the	O
training	O
set	O
and	O
validation	O
and	O
test	O
sets	O
.	O
Following	O
this	O
approach	O
,	O
we	O
present	O
a	O
simpler	O
fully	O
-	O
neural	O
approach	O
to	O
the	O
Story	B-RESEARCH_PROBLEM
Cloze	I-RESEARCH_PROBLEM
Test	E-RESEARCH_PROBLEM
using	O
skip	O
-	O
thought	O
embeddings	O
of	O
the	O
stories	O
in	O
a	O
feed	O
-	O
forward	O
network	O
that	O
achieves	O
close	O
to	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
this	O
task	O
without	O
any	O
feature	O
engineering	O
.	O
We	O
also	O
find	O
that	O
considering	O
just	O
the	O
last	O
sentence	O
of	O
the	O
prompt	O
instead	O
of	O
the	O
whole	O
prompt	O
yields	O
higher	O
accuracy	O
with	O
our	O
approach	O
.	O
narrative	O
1	O
Introduction	O
introduced	O
the	O
Story	B-RESEARCH_PROBLEM
Cloze	I-RESEARCH_PROBLEM
Test	E-RESEARCH_PROBLEM
:	O
given	O
a	O
four	O
-	O
sentence	O
story	O
prompt	O
(	O
or	O
'	O
context	O
'	O
)	O
,	O
the	O
task	O
is	O
to	O
pick	O
the	O
'	O
right	O
'	O
commonsense	O
ending	O
from	O
two	O
options	O
.	O
The	O
Cloze	O

To	O
solve	O
the	O
text	B-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
based	I-RESEARCH_PROBLEM
question	I-RESEARCH_PROBLEM
and	I-RESEARCH_PROBLEM
answering	E-RESEARCH_PROBLEM
task	O
that	O
requires	O
relational	O
reasoning	O
,	O
it	O
is	O
necessary	O
to	O
memorize	O
a	O
large	O
amount	O
of	O
information	O
and	O
find	O
out	O
the	O
question	O
relevant	O
information	O
from	O
the	O
memory	O
.	O
Most	O
approaches	O
were	O
based	O
on	O
external	O
memory	O
and	O
four	O
components	O
proposed	O
by	O
Memory	O
Network	O
.	O
The	O
distinctive	O
component	O
among	O
them	O
was	O
the	O
way	O
of	O
finding	O
the	O
necessary	O
information	O
and	O
it	O
contributes	O
to	O
the	O
performance	O
.	O
Recently	O
,	O
a	O
simple	O
but	O
powerful	O
neural	O
network	O
module	O
for	O
reasoning	O
called	O
Relation	O
Network	O
(	O
RN	O
)	O
has	O
been	O
introduced	O
.	O
We	O
analyzed	O
RN	O
from	O
the	O
view	O
of	O
Memory	O
Network	O
,	O
and	O
realized	O
that	O
its	O
MLP	O
component	O
is	O
able	O
to	O
reveal	O
the	O
complicate	O
relation	O
between	O
question	O
and	O
object	O
pair	O
.	O
Motivated	O
from	O
it	O
,	O
we	O
introduce	O
Relation	O
Memory	O
Network	O
(	O
RMN	O
)	O
which	O
uses	O
MLP	O
to	O
find	O
out	O
relevant	O
information	O
on	O
Memory	O
Network	O
architecture	O
.	O
It	O
shows	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
in	O
jointly	O
trained	O
b	O
AbI	O
-	O
10	O
k	O
story	O
-	O
based	O
question	O
answering	O
tasks	O
and	O
bAbI	O
dialog	O
-	O
based	O
question	O
answering	O
tasks	O
.	O

We	O
present	O
the	O
EpiReader	O
,	O
a	O
novel	O
model	O
for	O
machine	B-RESEARCH_PROBLEM
comprehension	E-RESEARCH_PROBLEM
of	O
text	O
.	O
Machine	B-RESEARCH_PROBLEM
comprehension	I-RESEARCH_PROBLEM
of	I-RESEARCH_PROBLEM
unstructured	I-RESEARCH_PROBLEM
,	I-RESEARCH_PROBLEM
real	I-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
world	I-RESEARCH_PROBLEM
text	E-RESEARCH_PROBLEM
is	O
a	O
major	O
research	O
goal	O
for	O
natural	O
language	O
processing	O
.	O
Current	O
tests	O
of	O
machine	B-RESEARCH_PROBLEM
comprehension	E-RESEARCH_PROBLEM
pose	O
questions	O
whose	O
answers	O
can	O
be	O
inferred	O
from	O
some	O
supporting	O
text	O
,	O
and	O
evaluate	O
a	O
model	O
's	O
response	O
to	O
the	O
questions	O
.	O
The	O
EpiReader	O
is	O
an	O
end	O
-	O
to	O
-	O
end	O
neural	O
model	O
comprising	O
two	O
components	O
:	O
the	O
first	O
component	O
proposes	O
a	O
small	O
set	O
of	O
candidate	O
answers	O
after	O
comparing	O
a	O
question	O
to	O
its	O
supporting	O
text	O
,	O
and	O
the	O
second	O
component	O
formulates	O
hypotheses	O
using	O
the	O
proposed	O
candidates	O
and	O
the	O
question	O
,	O
then	O
reranks	O
the	O
hypotheses	O
based	O
on	O
their	O
estimated	O
concordance	O
with	O
the	O
supporting	O
text	O
.	O
We	O
present	O
experiments	O
demonstrating	O
that	O
the	O
EpiReader	O
sets	O
anew	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
on	O
the	O
CNN	O
and	O
Children	O
's	O
Book	O
Test	O
machine	O
comprehension	O
benchmarks	O
,	O
outperforming	O
previous	O
neural	O
models	O
by	O
a	O
significant	O
margin	O
.	O

We	O
introduce	O
a	O
neural	O
network	O
with	O
a	O
recurrent	O
attention	O
model	O
over	O
a	O
possibly	O
large	O
external	O
memory	O
.	O
The	O
architecture	O
is	O
a	O
form	O
of	O
Memory	O
Network	O
[	O
23	O
]	O
but	O
unlike	O
the	O
model	O
in	O
that	O
work	O
,	O
it	O
is	O
trained	O
end	O
-	O
to	O
-	O
end	O
,	O
and	O
hence	O
requires	O
significantly	O
less	O
supervision	O
during	O
training	O
,	O
making	O
it	O
more	O
generally	O
applicable	O
in	O
realistic	O
settings	O
.	O
It	O
can	O
also	O
be	O
seen	O
as	O
an	O
extension	O
of	O
RNNsearch	O
[	O
2	O
]	O
to	O
the	O
case	O
where	O
multiple	O
computational	O
steps	O
(	O
hops	O
)	O
are	O
performed	O
per	O
output	O
symbol	O
.	O
The	O
flexibility	O
of	O
the	O
model	O
allows	O
us	O
to	O
apply	O
it	O
to	O
tasks	O
as	O
diverse	O
as	O
(	O
synthetic	O
)	O
question	O
answering	O
[	O
22	O
]	O
and	O
to	O
language	O
modeling	O
.	O
For	O
the	O
former	O
our	O
approach	O
is	O
competitive	O
with	O
Memory	B-RESEARCH_PROBLEM
Networks	E-RESEARCH_PROBLEM
,	O
but	O
with	O
less	O
supervision	O
.	O
For	O
the	O
latter	O
,	O
on	O
the	O
Penn	O
TreeBank	O
and	O
Text8	O
datasets	O
our	O
approach	O
demonstrates	O
comparable	O
performance	O
to	O
RNNs	O
and	O
LSTMs	O
.	O
In	O
both	O
cases	O
we	O
show	O
that	O
the	O
key	O
concept	O
of	O
multiple	O
computational	O
hops	O
yields	O
improved	O
results	O
.	O

Modeling	O
natural	B-RESEARCH_PROBLEM
language	I-RESEARCH_PROBLEM
inference	E-RESEARCH_PROBLEM
is	O
a	O
very	O
challenging	O
task	O
.	O
With	O
the	O
availability	O
of	O
large	O
annotated	O
data	O
,	O
it	O
has	O
recently	O
become	O
feasible	O
to	O
train	O
complex	O
models	O
such	O
as	O
neural	B-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
network	I-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
based	I-RESEARCH_PROBLEM
inference	E-RESEARCH_PROBLEM
models	O
,	O
which	O
have	O
shown	O
to	O
achieve	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
.	O
Although	O
there	O
exist	O
relatively	O
large	O
annotated	O
data	O
,	O
can	O
machines	O
learn	O
all	O
knowledge	O
needed	O
to	O
perform	O
natural	B-RESEARCH_PROBLEM
language	I-RESEARCH_PROBLEM
inference	E-RESEARCH_PROBLEM
(	O
NLI	S-RESEARCH_PROBLEM
)	O
from	O
these	O
data	O
?	O
If	O
not	O
,	O
how	O
can	O
neural	O
-	O
network	O
-	O
based	O
NLI	S-RESEARCH_PROBLEM
models	O
benefit	O
from	O
external	O
knowledge	O
and	O
how	O
to	O
build	O
NLI	S-RESEARCH_PROBLEM
models	O
to	O
leverage	O
it	O
?	O
In	O
this	O
paper	O
,	O
we	O
enrich	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
neural	O
natural	B-RESEARCH_PROBLEM
language	I-RESEARCH_PROBLEM
inference	E-RESEARCH_PROBLEM
models	O
with	O
external	O
knowledge	O
.	O
We	O
demonstrate	O
that	O
the	O
proposed	O
models	O
improve	O
neural	O
NLI	S-RESEARCH_PROBLEM
models	O
to	O
achieve	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
the	O
ST6	O
and	O
MultiT7	O
datasets	O
.	O

For	O
natural	O
language	O
understanding	O
(	O
NLU	S-RESEARCH_PROBLEM
)	O
technology	O
to	O
be	O
maximally	O
useful	O
,	O
it	O
must	O
be	O
able	O
to	O
process	O
language	O
in	O
away	O
that	O
is	O
not	O
exclusive	O
to	O
a	O
single	O
task	O
,	O
genre	O
,	O
or	O
dataset	O
.	O
In	O
pursuit	O
of	O
this	O
objective	O
,	O
we	O
introduce	O
the	O
General	O
Language	O
Understanding	O
Evaluation	O
(	O
GLUE	O
)	O
benchmark	O
,	O
a	O
collection	O
of	O
tools	O
for	O
evaluating	O
the	O
performance	O
of	O
models	O
across	O
a	O
diverse	O
set	O
of	O
existing	O
NLU	S-RESEARCH_PROBLEM
tasks	O
.	O
By	O
including	O
tasks	O
with	O
limited	O
training	O
data	O
,	O
GLUE	O
is	O
designed	O
to	O
favor	O
and	O
encourage	O
models	O
that	O
share	O
general	O
linguistic	O
knowledge	O
across	O
tasks	O
.	O
GLUE	O
also	O
includes	O
a	O
hand	O
-	O
crafted	O
diagnostic	O
test	O
suite	O
that	O
enables	O
detailed	O
linguistic	O
analysis	O
of	O
models	O
.	O
We	O
evaluate	O
baselines	O
based	O
on	O
current	O
methods	O
for	O
transfer	O
and	O
representation	O
learning	O
and	O
find	O
that	O
multi-task	O
training	O
on	O
all	O
tasks	O
performs	O
better	O
than	O
training	O
a	O
separate	O
model	O
per	O
task	O
.	O
However	O
,	O
the	O
low	O
absolute	O
performance	O
of	O
our	O
best	O
model	O
indicates	O
the	O
need	O
for	O
improved	O
general	O
NLU	S-RESEARCH_PROBLEM
systems	O
.	O
Published	O
as	O
a	O
conference	O
paper	O
at	O
ICLR	O
2019	O

Optimal	B-RESEARCH_PROBLEM
parameter	I-RESEARCH_PROBLEM
initialization	E-RESEARCH_PROBLEM
remains	O
a	O
crucial	O
problem	O
for	O
neural	O
network	O
training	O
.	O
A	O
poor	O
weight	O
initialization	O
may	O
take	O
longer	O
to	O
train	O
and	O
/	O
or	O
converge	O
to	O
sub-optimal	O
solutions	O
.	O
Here	O
,	O
we	O
propose	O
a	O
method	O
of	O
weight	O
re-initialization	O
by	O
repeated	O
annealing	O
and	O
injection	O
of	O
noise	O
in	O
the	O
training	O
process	O
.	O
We	O
implement	O
this	O
through	O
a	O
cyclical	O
batch	O
size	O
schedule	O
motivated	O
by	O
a	O
Bayesian	O
perspective	O
of	O
neural	O
network	O
training	O
.	O
We	O
evaluate	O
our	O
methods	O
through	O
extensive	O
experiments	O
on	O
tasks	O
in	O
language	O
modeling	O
,	O
natural	O
language	O
inference	O
,	O
and	O
image	O
classification	O
.	O
We	O
demonstrate	O
the	O
ability	O
of	O
our	O
method	O
to	O
improve	O
language	O
modeling	O
performance	O
by	O
up	O
to	O
7.91	O
perplexity	O
and	O
reduce	O
training	O
iterations	O
by	O
up	O
to	O
61	O
%	O
,	O
in	O
addition	O
to	O
its	O
flexibility	O
in	O
enabling	O
snapshot	O
ensembling	O
and	O
use	O
with	O
adversarial	O
training	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
the	O
TBCNNpair	O
model	O
to	O
recognize	B-RESEARCH_PROBLEM
entailment	I-RESEARCH_PROBLEM
and	I-RESEARCH_PROBLEM
contradiction	E-RESEARCH_PROBLEM
between	O
two	O
sentences	O
.	O
In	O
our	O
model	O
,	O
a	O
tree	O
-	O
based	O
convolutional	O
neural	O
network	O
(	O
TBCNN	O
)	O
captures	O
sentencelevel	O
semantics	O
;	O
then	O
heuristic	O
matching	O
layers	O
like	O
concatenation	O
,	O
element	O
-	O
wise	O
product	O
/	O
difference	O
combine	O
the	O
information	O
in	O
individual	O
sentences	O
.	O
Experimental	O
results	O
show	O
that	O
our	O
model	O
outperforms	O
existing	O
sentence	O
encoding	O
-	O
based	O
approaches	O
by	O
a	O
large	O
margin	O
.	O

We	O
utilize	O
a	O
stochastic	O
answer	O
network	O
(	O
SAN	O
)	O
to	O
explore	O
multi-step	O
inference	O
strategies	O
in	O
Natural	B-RESEARCH_PROBLEM
Language	I-RESEARCH_PROBLEM
Inference	E-RESEARCH_PROBLEM
.	O
Rather	O
than	O
directly	O
predicting	O
the	O
results	O
given	O
the	O
inputs	O
,	O
the	O
model	O
maintains	O
a	O
state	O
and	O
iteratively	O
refines	O
its	O
predictions	O
.	O
This	O
can	O
potentially	O
model	O
more	O
complex	O
inferences	O
than	O
the	O
existing	O
single	O
-	O
step	O
inference	O
methods	O
.	O
Our	O
experiments	O
show	O
that	O
SAN	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
four	O
benchmarks	O
:	O
Stanford	O
Natural	B-RESEARCH_PROBLEM
Language	I-RESEARCH_PROBLEM
Inference	E-RESEARCH_PROBLEM
(	O
SNLI	O
)	O
,	O
MultiGenre	O
Natural	B-RESEARCH_PROBLEM
Language	I-RESEARCH_PROBLEM
Inference	E-RESEARCH_PROBLEM
(	O
MultiNLI	O
)	O
,	O
SciTail	O
,	O
and	O
Quora	O
Question	O
Pairs	O
datasets	O
.	O
Motivation	O
The	O
natural	O
language	O
inference	O
task	O
,	O
also	O
known	O
as	O
recognizing	O
textual	O
entailment	O
(	O
RTE	O
)	O
,	O
is	O
to	O
infer	O
the	O
relation	O
between	O
a	O
pair	O
of	O
sentences	O
(	O
e.g.	O
,	O
premise	O
and	O
hypothesis	O
)	O
.	O
This	O
task	O
is	O
challenging	O
,	O
since	O
it	O
requires	O
a	O
model	O
to	O
fully	O
understand	O
the	O
sentence	O
meaning	O
,	O
(	O
i.e.	O
,	O
lexical	O
and	O
compositional	O
semantics	O
)	O
.	O

Cloze	B-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
style	I-RESEARCH_PROBLEM
reading	I-RESEARCH_PROBLEM
comprehension	E-RESEARCH_PROBLEM
is	O
a	O
representative	O
problem	O
in	O
mining	O
relationship	O
between	O
document	O
and	O
query	O
.	O
In	O
this	O
paper	O
,	O
we	O
present	O
a	O
simple	O
but	O
novel	O
model	O
called	O
attention	O
-	O
over	O
-	O
attention	O
reader	O
for	O
better	O
solving	O
cloze	O
-	O
style	O
reading	O
comprehension	O
task	O
.	O
The	O
proposed	O
model	O
aims	O
to	O
place	O
another	O
attention	O
mechanism	O
over	O
the	O
document	O
-	O
level	O
attention	O
and	O
induces	O
"	O
attended	O
attention	O
"	O
for	O
final	O
answer	O
predictions	O
.	O
One	O
advantage	O
of	O
our	O
model	O
is	O
that	O
it	O
is	O
simpler	O
than	O
related	O
works	O
while	O
giving	O
excellent	O
performance	O
.	O
In	O
addition	O
to	O
the	O
primary	O
model	O
,	O
we	O
also	O
propose	O
an	O
N	O
-	O
best	O
re-ranking	O
strategy	O
to	O
double	O
check	O
the	O
validity	O
of	O
the	O
candidates	O
and	O
further	O
improve	O
the	O
performance	O
.	O
Experimental	O
results	O
show	O
that	O
the	O
proposed	O
methods	O
significantly	O
outperform	O
various	O
state	O
-	O
of	O
the	O
-	O
art	O
systems	O
by	O
a	O
large	O
margin	O
in	O
public	O
datasets	O
,	O
such	O
as	O
CNN	O
and	O
Children	O
's	O
Book	O
Test	O
.	O

Question	B-RESEARCH_PROBLEM
Answering	E-RESEARCH_PROBLEM
(	O
QA	S-RESEARCH_PROBLEM
)	O
systems	O
are	O
used	O
to	O
provide	O
proper	O
responses	O
to	O
users	O
'	O
questions	O
automatically	O
.	O
Sentence	O
matching	O
is	O
an	O
essential	O
task	O
in	O
the	O
QA	S-RESEARCH_PROBLEM
systems	O
and	O
is	O
usually	O
reformulated	O
as	O
a	O
Paraphrase	O
Identification	O
(	O
PI	O
)	O
problem	O
.	O
Given	O
a	O
question	O
,	O
the	O
aim	O
of	O
the	O
task	O
is	O
to	O
find	O
the	O
most	O
similar	O
question	O
from	O
a	O
QA	S-RESEARCH_PROBLEM
knowledge	O
base	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
Multi	O
-	O
task	O
Sentence	O
Encoding	O
Model	O
(	O
MSEM	O
)	O
for	O
the	O
PI	O
problem	O
,	O
wherein	O
a	O
connected	O
graph	O
is	O
employed	O
to	O
depict	O
the	O
relation	O
between	O
sentences	O
,	O
and	O
a	O
multi-task	O
learning	O
model	O
is	O
applied	O
to	O
address	O
both	O
the	O
sentence	O
matching	O
and	O
sentence	O
intent	O
classification	O
problem	O
.	O
In	O
addition	O
,	O
we	O
implement	O
a	O
general	O
semantic	O
retrieval	O
framework	O
that	O
combines	O
our	O
proposed	O
model	O
and	O
the	O
Approximate	O
Nearest	O
Neighbor	O
(	O
ANN	O
)	O
technology	O
,	O
which	O
enables	O
us	O
to	O
find	O
the	O
most	O
similar	O
question	O
from	O
all	O
available	O
candidates	O
very	O
quickly	O
during	O
online	O
serving	O
.	O
The	O
experiments	O
show	O
the	O
superiority	O
of	O
our	O
proposed	O
method	O
as	O
compared	O
with	O
the	O
existing	O
sentence	O
matching	O
models	O
.	O
I.	O
INTRODUCTION	O

A	O
fundamental	O
trade	O
-	O
off	O
between	O
effectiveness	O
and	O
efficiency	O
needs	O
to	O
be	O
balanced	O
when	O
designing	O
an	O
online	O
question	O
answering	O
system	O
.	O
Effectiveness	O
comes	O
from	O
sophisticated	O
functions	O
such	O
as	O
extractive	O
machine	O
reading	O
comprehension	O
(	O
MRC	S-RESEARCH_PROBLEM
)	O
,	O
while	O
efficiency	O
is	O
obtained	O
from	O
improvements	O
in	O
preliminary	O
retrieval	O
components	O
such	O
as	O
candidate	O
document	O
selection	O
and	O
paragraph	O
ranking	O
.	O
Given	O
the	O
complexity	O
of	O
the	O
real	O
-	O
world	O
multi-document	O
MRC	S-RESEARCH_PROBLEM
scenario	O
,	O
it	O
is	O
difficult	O
to	O
jointly	O
optimize	O
both	O
in	O
an	O
end	O
-	O
to	O
-	O
end	O
system	O
.	O
To	O
address	O
this	O
problem	O
,	O
we	O
develop	O
a	O
novel	O
deep	O
cascade	O
learning	O
model	O
,	O
which	O
progressively	O
evolves	O
from	O
the	O
documentlevel	O
and	O
paragraph	O
-	O
level	O
ranking	O
of	O
candidate	O
texts	O
to	O
more	O
precise	O
answer	O
extraction	O
with	O
machine	O
reading	O
comprehension	O
.	O
Specifically	O
,	O
irrelevant	O
documents	O
and	O
paragraphs	O
are	O
first	O
filtered	O
outwith	O
simple	O
functions	O
for	O
efficiency	O
consideration	O
.	O
Then	O
we	O
jointly	O
train	O
three	O
modules	O
on	O
the	O
remaining	O
texts	O
for	O
better	O
tracking	O
the	O
answer	O
:	O
the	O
document	O
extraction	O
,	O
the	O
paragraph	O
extraction	O
and	O
the	O
answer	O
extraction	O
.	O
Experiment	O
results	O
show	O
that	O
the	O
proposed	O
method	O
outperforms	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
on	O
two	O
large	O
-	O
scale	O
multidocument	O
benchmark	O
datasets	O
,	O
i.e.	O
,	O
TriviaQA	O
and	O
DuReader	O
.	O

Conversational	O
question	O
answering	O
(	O
CQA	S-RESEARCH_PROBLEM
)	O
is	O
a	O
novel	O
QA	O
task	O
that	O
requires	O
understanding	O
of	O
dialogue	O
context	O
.	O
Different	O
from	O
traditional	O
single	O
-	O
turn	O
machine	O
reading	O
comprehension	O
(	O
MRC	O
)	O
tasks	O
,	O
CQA	S-RESEARCH_PROBLEM
includes	O
passage	O
comprehension	O
,	O
coreference	O
resolution	O
,	O
and	O
contextual	O
understanding	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
an	O
innovated	O
contextualized	O
attention	O
-	O
based	O
deep	O
neural	O
network	O
,	O
SDNet	O
,	O
to	O
fuse	O
context	O
into	O
traditional	O
MRC	O
models	O
.	O
Our	O
model	O
leverages	O
both	O
inter-attention	O
and	O
self	O
-	O
attention	O
to	O
comprehend	O
conversation	O
context	O
and	O
extract	O
relevant	O
information	O
from	O
passage	O
.	O
Furthermore	O
,	O
we	O
demonstrated	O
a	O
novel	O
method	O
to	O
integrate	O
the	O
latest	O
BERT	O
contextual	O
model	O
.	O
Empirical	O
results	O
show	O
the	O
effectiveness	O
of	O
our	O
model	O
,	O
which	O
sets	O
the	O
new	O
state	O
of	O
the	O
art	O
result	O
in	O
CoQA	O
leaderboard	O
,	O
outperforming	O
the	O
previous	O
best	O
model	O
by	O
1.6	O
%	O
F	O
1	O
.	O
Our	O
ensemble	O
model	O
further	O
improves	O
the	O
result	O
by	O
2.7	O
%	O
F	O
1	O
.	O

Semantic	B-RESEARCH_PROBLEM
matching	E-RESEARCH_PROBLEM
is	O
of	O
central	O
importance	O
to	O
many	O
natural	O
language	O
tasks	O
[	O
2,28	O
]	O
.	O
A	O
successful	O
matching	O
algorithm	O
needs	O
to	O
adequately	O
model	O
the	O
internal	O
structures	O
of	O
language	O
objects	O
and	O
the	O
interaction	O
between	O
them	O
.	O
As	O
a	O
step	O
toward	O
this	O
goal	O
,	O
we	O
propose	O
convolutional	O
neural	O
network	O
models	O
for	O
matching	O
two	O
sentences	O
,	O
by	O
adapting	O
the	O
convolutional	O
strategy	O
in	O
vision	O
and	O
speech	O
.	O
The	O
proposed	O
models	O
not	O
only	O
nicely	O
represent	O
the	O
hierarchical	O
structures	O
of	O
sentences	O
with	O
their	O
layerby	O
-	O
layer	O
composition	O
and	O
pooling	O
,	O
but	O
also	O
capture	O
the	O
rich	O
matching	O
patterns	O
at	O
different	O
levels	O
.	O
Our	O
models	O
are	O
rather	O
generic	O
,	O
requiring	O
no	O
prior	O
knowledge	O
on	O
language	O
,	O
and	O
can	O
hence	O
be	O
applied	O
to	O
matching	O
tasks	O
of	O
different	O
nature	O
and	O
in	O
different	O
languages	O
.	O
The	O
empirical	O
study	O
on	O
a	O
variety	O
of	O
matching	O
tasks	O
demonstrates	O
the	O
efficacy	O
of	O
the	O
proposed	O
model	O
on	O
a	O
variety	O
of	O
matching	O
tasks	O
and	O
its	O
superiority	O
to	O
competitor	O
models	O
.	O

Machine	B-RESEARCH_PROBLEM
reading	I-RESEARCH_PROBLEM
comprehension	E-RESEARCH_PROBLEM
helps	O
machines	O
learn	O
to	O
utilize	O
most	O
of	O
the	O
human	O
knowledge	O
written	O
in	O
the	O
form	O
of	O
text	O
.	O
Existing	O
approaches	O
made	O
a	O
significant	O
progress	O
comparable	O
to	O
human	O
-	O
level	O
performance	O
,	O
but	O
they	O
are	O
still	O
limited	O
in	O
understanding	O
,	O
up	O
to	O
a	O
few	O
paragraphs	O
,	O
failing	O
to	O
properly	O
comprehend	O
lengthy	O
document	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
novel	O
deep	O
neural	O
network	O
architecture	O
to	O
handle	O
a	O
long	O
-	O
range	O
dependency	O
in	O
RC	S-RESEARCH_PROBLEM
tasks	O
.	O
In	O
detail	O
,	O
our	O
method	O
has	O
two	O
novel	O
aspects	O
:	O
(	O
1	O
)	O
an	O
advanced	O
memory	O
-	O
augmented	O
architecture	O
and	O
(	O
2	O
)	O
an	O
expanded	O
gated	O
recurrent	O
unit	O
with	O
dense	O
connections	O
that	O
mitigate	O
potential	O
information	O
distortion	O
occurring	O
in	O
the	O
memory	O
.	O
Our	O
proposed	O
architecture	O
is	O
widely	O
applicable	O
to	O
other	O
models	O
.	O
We	O
have	O
performed	O
extensive	O
experiments	O
with	O
well	O
-	O
known	O
benchmark	O
datasets	O
such	O
as	O
TriviaQA	O
,	O
QUASAR	O
-	O
T	O
,	O
and	O
SQ	O
u	O
AD	O
.	O
The	O
experimental	O
results	O
demonstrate	O
that	O
the	O
proposed	O
method	O
outperforms	O
existing	O
methods	O
,	O
especially	O
for	O
lengthy	O
documents	O
.	O

Most	O
conventional	O
sentence	B-RESEARCH_PROBLEM
similarity	E-RESEARCH_PROBLEM
methods	O
only	O
focus	O
on	O
similar	O
parts	O
of	O
two	O
input	O
sentences	O
,	O
and	O
simply	O
ignore	O
the	O
dissimilar	O
parts	O
,	O
which	O
usually	O
give	O
us	O
some	O
clues	O
and	O
semantic	O
meanings	O
about	O
the	O
sentences	O
.	O
In	O
this	O
work	O
,	O
we	O
propose	O
a	O
model	O
to	O
take	O
into	O
account	O
both	O
the	O
similarities	O
and	O
dissimilarities	O
by	O
decomposing	O
and	O
composing	O
lexical	O
semantics	O
over	O
sentences	O
.	O
The	O
model	O
represents	O
each	O
word	O
as	O
a	O
vector	O
,	O
and	O
calculates	O
a	O
semantic	O
matching	O
vector	O
for	O
each	O
word	O
based	O
on	O
all	O
words	O
in	O
the	O
other	O
sentence	O
.	O
Then	O
,	O
each	O
word	O
vector	O
is	O
decomposed	O
into	O
a	O
similar	O
component	O
and	O
a	O
dissimilar	O
component	O
based	O
on	O
the	O
semantic	O
matching	O
vector	O
.	O
After	O
this	O
,	O
a	O
two	O
-	O
channel	O
CNN	O
model	O
is	O
employed	O
to	O
capture	O
features	O
by	O
composing	O
the	O
similar	O
and	O
dissimilar	O
components	O
.	O
Finally	O
,	O
a	O
similarity	O
score	O
is	O
estimated	O
over	O
the	O
composed	O
feature	O
vectors	O
.	O
Experimental	O
results	O
show	O
that	O
our	O
model	O
gets	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
the	O
answer	O
sentence	O
selection	O
task	O
,	O
and	O
achieves	O
a	O
comparable	O
result	O
on	O
the	O
paraphrase	O
identification	O
task	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
Dynamic	B-RESEARCH_PROBLEM
Self	I-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
Attention	E-RESEARCH_PROBLEM
(	O
DSA	S-RESEARCH_PROBLEM
)	O
,	O
a	O
new	O
self	O
-	O
attention	O
mechanism	O
for	O
sentence	O
embedding	O
.	O
We	O
design	O
DSA	S-RESEARCH_PROBLEM
by	O
modifying	O
dynamic	O
routing	O
in	O
capsule	O
network	O
(	O
Sabour	O
et	O
al.	O
,	O
2017	O
)	O
for	O
natural	O
language	O
processing	O
.	O
DSA	S-RESEARCH_PROBLEM
attends	O
to	O
informative	O
words	O
with	O
a	O
dynamic	O
weight	O
vector	O
.	O
We	O
achieve	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
among	O
sentence	O
encoding	O
methods	O
in	O
Stanford	O
Natural	O
Language	O
Inference	O
(	O
SNLI	O
)	O
dataset	O
with	O
the	O
least	O
number	O
of	O
parameters	O
,	O
while	O
showing	O
comparative	O
results	O
in	O
Stanford	O
Sentiment	O
Treebank	O
(	O
SST	O
)	O
dataset	O
.	O

Teaching	O
machines	O
to	O
read	O
natural	O
language	O
documents	O
remains	O
an	O
elusive	O
challenge	O
.	O
Machine	B-RESEARCH_PROBLEM
reading	E-RESEARCH_PROBLEM
systems	O
can	O
be	O
tested	O
on	O
their	O
ability	O
to	O
answer	O
questions	O
posed	O
on	O
the	O
contents	O
of	O
documents	O
that	O
they	O
have	O
seen	O
,	O
but	O
until	O
now	O
large	O
scale	O
training	O
and	O
test	O
datasets	O
have	O
been	O
missing	O
for	O
this	O
type	O
of	O
evaluation	O
.	O
In	O
this	O
work	O
we	O
define	O
a	O
new	O
methodology	O
that	O
resolves	O
this	O
bottleneck	O
and	O
provides	O
large	O
scale	O
supervised	O
reading	O
comprehension	O
data	O
.	O
This	O
allows	O
us	O
to	O
develop	O
a	O
class	O
of	O
attention	O
based	O
deep	O
neural	O
networks	O
that	O
learn	O
to	O
read	O
real	O
documents	O
and	O
answer	O
complex	O
questions	O
with	O
minimal	O
prior	O
knowledge	O
of	O
language	O
structure	O
.	O

In	O
this	O
paper	O
,	O
we	O
proposed	O
a	O
sentence	O
encoding	O
-	O
based	O
model	O
for	O
recognizing	B-RESEARCH_PROBLEM
text	I-RESEARCH_PROBLEM
entailment	E-RESEARCH_PROBLEM
.	O
In	O
our	O
approach	O
,	O
the	O
encoding	O
of	O
sentence	O
is	O
a	O
two	O
-	O
stage	O
process	O
.	O
Firstly	O
,	O
average	O
pooling	O
was	O
used	O
over	O
word	O
-	O
level	O
bidirectional	O
LSTM	O
(	O
biLSTM	O
)	O
to	O
generate	O
a	O
firststage	O
sentence	O
representation	O
.	O
Secondly	O
,	O
attention	O
mechanism	O
was	O
employed	O
to	O
replace	O
average	O
pooling	O
on	O
the	O
same	O
sentence	O
for	O
better	O
representations	O
.	O
Instead	O
of	O
using	O
target	O
sentence	O
to	O
attend	O
words	O
in	O
source	O
sentence	O
,	O
we	O
utilized	O
the	O
sentence	O
's	O
first	O
-	O
stage	O
representation	O
to	O
attend	O
words	O
appeared	O
in	O
itself	O
,	O
which	O
is	O
called	O
"	O
Inner-Attention	O
"	O
in	O
our	O
paper	O
.	O
Experiments	O
conducted	O
on	O
Stanford	O
Natural	B-RESEARCH_PROBLEM
Language	I-RESEARCH_PROBLEM
Inference	E-RESEARCH_PROBLEM
(	O
SNLI	O
)	O
Corpus	O
has	O
proved	O
the	O
effectiveness	O
of	O
"	O
Inner-Attention	O
"	O
mechanism	O
.	O

