Training	O
large	B-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
scale	I-RESEARCH_PROBLEM
question	I-RESEARCH_PROBLEM
answering	E-RESEARCH_PROBLEM
systems	O
is	O
complicated	O
because	O
training	O
sources	O
usually	O
cover	O
a	O
small	O
portion	O
of	O
the	O
range	O
of	O
possible	O
questions	O
.	O
This	O
paper	O
studies	O
the	O
impact	O
of	O
multitask	O
and	O
transfer	O
learning	O
for	O
simple	B-RESEARCH_PROBLEM
question	I-RESEARCH_PROBLEM
answering	E-RESEARCH_PROBLEM
;	O
a	O
setting	O
for	O
which	O
the	O
reasoning	O
required	O
to	O
answer	O
is	O
quite	O
easy	O
,	O
as	O
long	O
as	O
one	O
can	O
retrieve	O
the	O
correct	O
evidence	O
given	O
a	O
question	O
,	O
which	O
can	O
be	O
difficult	O
in	O
large	O
-	O
scale	O
conditions	O
.	O
To	O
this	O
end	O
,	O
we	O
introduce	O
a	O
new	O
dataset	O
of	O
100	O
k	O
questions	O
that	O
we	O
use	O
in	O
conjunction	O
with	O
existing	O
benchmarks	O
.	O
We	O
conduct	O
our	O
study	O
within	O
the	O
framework	O
of	O
Memory	O
Networks	O
(	O
Weston	O
et	O
al.	O
,	O
2015	O
)	O
because	O
this	O
perspective	O
allows	O
us	O
to	O
eventually	O
scale	O
up	O
to	O
more	O
complex	O
reasoning	O
,	O
and	O
show	O
that	O
Memory	O
Networks	O
can	O
be	O
successfully	O
trained	O
to	O
achieve	O
excellent	O
performance	O
.	O

For	O
years	O
,	O
recursive	O
neural	O
networks	O
(	O
Rv	O
NNs	O
)	O
have	O
been	O
shown	O
to	O
be	O
suitable	O
for	O
representing	O
text	O
into	O
fixed	O
-	O
length	O
vectors	O
and	O
achieved	O
good	O
performance	O
on	O
several	O
natural	O
language	O
processing	O
tasks	O
.	O
However	O
,	O
the	O
main	O
drawback	O
of	O
RvNNs	O
is	O
that	O
they	O
require	O
structured	O
input	O
,	O
which	O
makes	O
data	O
preparation	O
and	O
model	O
implementation	O
hard	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
Gumbel	O
Tree	O
-	O
LSTM	O
,	O
a	O
novel	O
tree	O
-	O
structured	O
long	O
short	O
-	O
term	O
memory	O
architecture	O
that	O
learns	O
how	O
to	O
compose	O
task	B-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
specific	I-RESEARCH_PROBLEM
tree	I-RESEARCH_PROBLEM
structures	I-RESEARCH_PROBLEM
only	I-RESEARCH_PROBLEM
from	I-RESEARCH_PROBLEM
plain	I-RESEARCH_PROBLEM
text	I-RESEARCH_PROBLEM
data	E-RESEARCH_PROBLEM
efficiently	O
.	O
Our	O
model	O
uses	O
Straight	O
-	O
Through	O
Gumbel	O
-	O
Softmax	O
estimator	O
to	O
decide	O
the	O
parent	O
node	O
among	O
candidates	O
dynamically	O
and	O
to	O
calculate	O
gradients	O
of	O
the	O
discrete	O
decision	O
.	O
We	O
evaluate	O
the	O
proposed	O
model	O
on	O
natural	O
language	O
inference	O
and	O
sentiment	O
analysis	O
,	O
and	O
show	O
that	O
our	O
model	O
outperforms	O
or	O
is	O
at	O
least	O
comparable	O
to	O
previous	O
models	O
.	O
We	O
also	O
find	O
that	O
our	O
model	O
converges	O
significantly	O
faster	O
than	O
other	O
models	O
.	O

As	O
an	O
alternative	O
to	O
question	B-RESEARCH_PROBLEM
answering	E-RESEARCH_PROBLEM
methods	O
based	O
on	O
feature	O
engineering	O
,	O
deep	O
learning	O
approaches	O
such	O
as	O
convolutional	O
neural	O
networks	O
(	O
CNNs	O
)	O
and	O
Long	O
Short	O
-	O
Term	O
Memory	O
Models	O
(	O
LSTMs	O
)	O
have	O
recently	O
been	O
proposed	O
for	O
semantic	O
matching	O
of	O
questions	O
and	O
answers	O
.	O
To	O
achieve	O
good	O
results	O
,	O
however	O
,	O
these	O
models	O
have	O
been	O
combined	O
with	O
additional	O
features	O
such	O
as	O
word	O
overlap	O
or	O
BM25	O
scores	O
.	O
Without	O
this	O
combination	O
,	O
these	O
models	O
perform	O
significantly	O
worse	O
than	O
methods	O
based	O
on	O
linguistic	O
feature	O
engineering	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
an	O
attention	O
based	O
neural	O
matching	O
model	O
for	O
ranking	O
short	O
answer	O
text	O
.	O
We	O
adopt	O
value	O
-	O
shared	O
weighting	O
scheme	O
instead	O
of	O
position	O
-	O
shared	O
weighting	O
scheme	O
for	O
combining	O
different	O
matching	O
signals	O
and	O
incorporate	O
question	O
term	O
importance	O
learning	O
using	O
question	O
attention	O
network	O
.	O
Using	O
the	O
popular	O
benchmark	O
TREC	O
QA	S-RESEARCH_PROBLEM
data	O
,	O
we	O
show	O
that	O
the	O
relatively	O
simple	O
a	O
NMM	O
model	O
can	O
significantly	O
outperform	O
other	O
neural	O
network	O
models	O
that	O
have	O
been	O
used	O
for	O
the	O
question	B-RESEARCH_PROBLEM
answering	E-RESEARCH_PROBLEM
task	O
,	O
and	O
is	O
competitive	O
with	O
models	O
thatare	O
combined	O
with	O
additional	O
features	O
.	O
When	O
a	O
NMM	O
is	O
combined	O
with	O
additional	O
features	O
,	O
it	O
outperforms	O
all	O
baselines	O
.	O

We	O
present	O
a	O
simple	O
sequential	O
sentence	O
encoder	O
for	O
multi-domain	B-RESEARCH_PROBLEM
natural	I-RESEARCH_PROBLEM
language	I-RESEARCH_PROBLEM
inference	E-RESEARCH_PROBLEM
.	O
Our	O
encoder	O
is	O
based	O
on	O
stacked	O
bidirectional	O
LSTM	O
-	O
RNNs	O
with	O
shortcut	O
connections	O
and	O
fine	O
-	O
tuning	O
of	O
word	O
embeddings	O
.	O
The	O
over	O
all	O
supervised	O
model	O
uses	O
the	O
above	O
encoder	O
to	O
encode	O
two	O
input	O
sentences	O
into	O
two	O
vectors	O
,	O
and	O
then	O
uses	O
a	O
classifier	O
over	O
the	O
vector	O
combination	O
to	O
label	O
the	O
relationship	O
between	O
these	O
two	O
sentences	O
as	O
that	O
of	O
entailment	O
,	O
contradiction	O
,	O
or	O
neural	O
.	O
Our	O
Shortcut	O
-	O
Stacked	O
sentence	O
encoders	O
achieve	O
strong	O
improvements	O
over	O
existing	O
encoders	O
on	O
matched	O
and	O
mismatched	O
multi-domain	B-RESEARCH_PROBLEM
natural	I-RESEARCH_PROBLEM
language	I-RESEARCH_PROBLEM
inference	E-RESEARCH_PROBLEM
(	O
top	O
non-ensemble	O
single	O
-	O
model	O
result	O
in	O
the	O
EMNLP	O
RepEval	O
2017	O
Shared	O
Task	O
(	O
Nangia	O
et	O
al.	O
,	O
2017	O
)	O
)	O
.	O
Moreover	O
,	O
they	O
achieve	O
the	O
new	O
state	O
-	O
of	O
-	O
theart	O
encoding	O
result	O
on	O
the	O
original	O
SNLI	O
dataset	O
(	O
Bowman	O
et	O
al.	O
,	O
2015	O
)	O
.	O
Introduction	O
and	O
Background	O
Natural	O
language	O
inference	O
(	O
NLI	O
)	O
or	O
recognizing	O
textual	O
entailment	O
(	O
RTE	O
)	O
is	O
a	O
fundamental	O
semantic	O
task	O
in	O
the	O
field	O
of	O
natural	O
language	O
processing	O
.	O

We	O
propose	O
MRU	O
(	O
Multi	O
-	O
Range	O
Reasoning	O
Units	O
)	O
,	O
a	O
new	O
fast	O
compositional	O
encoder	O
for	O
machine	O
comprehension	O
(	O
MC	S-RESEARCH_PROBLEM
)	O
.	O
Our	O
proposed	O
MRU	O
encoders	O
are	O
characterized	O
by	O
multi-ranged	O
gating	O
,	O
executing	O
a	O
series	O
of	O
parameterized	O
contractand	O
-	O
expand	O
layers	O
for	O
learning	O
gating	O
vectors	O
that	O
benefit	O
from	O
long	O
and	O
short	O
-	O
term	O
dependencies	O
.	O
The	O
aims	O
of	O
our	O
approach	O
are	O
as	O
follows	O
:	O
(	O
1	O
)	O
learning	O
representations	O
thatare	O
concurrently	O
aware	O
of	O
long	O
and	O
short	O
-	O
term	O
context	O
,	O
(	O
2	O
)	O
modeling	O
relationships	O
between	O
intra-document	O
blocks	O
and	O
(	O
3	O
)	O
fast	O
and	O
efficient	O
sequence	O
encoding	O
.	O
We	O
show	O
that	O
our	O
proposed	O
encoder	O
demonstrates	O
promising	O
results	O
both	O
as	O
a	O
standalone	O
encoder	O
and	O
as	O
well	O
as	O
a	O
complementary	O
building	O
block	O
.	O
We	O
conduct	O
extensive	O
experiments	O
on	O
three	O
challenging	O
MC	S-RESEARCH_PROBLEM
datasets	O
,	O
namely	O
RACE	O
,	O
Search	O
QA	O
and	O
Narrative	O
QA	O
,	O
achieving	O
highly	O
competitive	O
performance	O
on	O
all	O
.	O
On	O
the	O
RACE	O
benchmark	O
,	O
our	O
model	O
outperforms	O
DFN	O
(	O
Dynamic	O
Fusion	O
Networks	O
)	O
by	O
1.5	O
%	O
?	O
6	O
%	O
without	O
using	O
any	O
recurrent	O
or	O
convolution	O
layers	O
.	O

Commonsense	B-RESEARCH_PROBLEM
reasoning	E-RESEARCH_PROBLEM
is	O
a	O
critical	O
AI	O
capability	O
,	O
but	O
it	O
is	O
difficult	O
to	O
construct	O
challenging	O
datasets	O
that	O
test	O
commonsense	O
.	O
Recent	O
neural	O
question	O
answering	O
systems	O
,	O
based	O
on	O
large	O
pre-trained	O
models	O
of	O
language	O
,	O
have	O
already	O
achieved	O
near	O
-	O
human	O
-	O
level	O
performance	O
on	O
commonsense	O
knowledge	O
benchmarks	O
.	O
These	O
systems	O
do	O
not	O
possess	O
human	O
-	O
level	O
commonsense	O
,	O
but	O
are	O
able	O
to	O
exploit	O
limitations	O
of	O
the	O
datasets	O
to	O
achieve	O
human	O
-	O
level	O
scores	O
.	O
narrative	O
We	O
introduce	O
the	O
CODAH	O
dataset	O
,	O
an	O
adversarially	O
-	O
constructed	O
evaluation	O
dataset	O
for	O
testing	O
commonsense	O
.	O
CODAH	O
forms	O
a	O
challenging	O
extension	O
to	O
the	O
recently	O
-	O
proposed	O
SWAG	O
dataset	O
,	O
which	O
tests	O
commonsense	O
knowledge	O
using	O
sentence	O
-	O
completion	O
questions	O
that	O
describe	O
situations	O
observed	O
in	O
video	O
.	O
To	O
produce	O
a	O
more	O
difficult	O
dataset	O
,	O
we	O
introduce	O
a	O
novel	O
procedure	O
for	O
question	O
acquisition	O
in	O
which	O
workers	O
author	O
questions	O
designed	O
to	O
target	O
weaknesses	O
of	O
state	O
-	O
of	O
the	O
-	O
art	O
neural	O
question	O
answering	O
systems	O
.	O

Sentence	B-RESEARCH_PROBLEM
matching	E-RESEARCH_PROBLEM
is	O
widely	O
used	O
in	O
various	O
natural	O
language	O
tasks	O
such	O
as	O
natural	O
language	O
inference	O
,	O
paraphrase	O
identification	O
,	O
and	O
question	O
answering	O
.	O
For	O
these	O
tasks	O
,	O
understanding	O
logical	O
and	O
semantic	O
relationship	O
between	O
two	O
sentences	O
is	O
required	O
but	O
it	O
is	O
yet	O
challenging	O
.	O
Although	O
attention	O
mechanism	O
is	O
useful	O
to	O
capture	O
the	O
semantic	O
relationship	O
and	O
to	O
properly	O
align	O
the	O
elements	O
of	O
two	O
sentences	O
,	O
previous	O
methods	O
of	O
attention	O
mechanism	O
simply	O
use	O
a	O
summation	O
operation	O
which	O
does	O
not	O
retain	O
original	O
features	O
enough	O
.	O
Inspired	O
by	O
DenseNet	O
,	O
a	O
densely	O
connected	O
convolutional	O
network	O
,	O
we	O
propose	O
a	O
densely	O
-	O
connected	O
co-attentive	O
recurrent	O
neural	O
network	O
,	O
each	O
layer	O
of	O
which	O
uses	O
concatenated	O
information	O
of	O
attentive	O
features	O
as	O
well	O
as	O
hidden	O
features	O
of	O
all	O
the	O
preceding	O
recurrent	O
layers	O
.	O
It	O
enables	O
preserving	O
the	O
original	O
and	O
the	O
co-attentive	O
feature	O
information	O
from	O
the	O
bottommost	O
word	O
embedding	O
layer	O
to	O
the	O
uppermost	O
recurrent	O
layer	O
.	O
To	O
alleviate	O
the	O
problem	O
of	O
an	O
ever	O
-	O
increasing	O
size	O
of	O
feature	O
vectors	O
due	O
to	O
dense	O
concatenation	O
operations	O
,	O
we	O
also	O
propose	O
to	O
use	O
an	O
autoencoder	O
after	O
dense	O
concatenation	O
.	O
We	O
evaluate	O
our	O
proposed	O
architecture	O
on	O
highly	O
competitive	O
benchmark	O
datasets	O
related	O
to	O
sentence	O
matching	O
.	O

Natural	B-RESEARCH_PROBLEM
language	I-RESEARCH_PROBLEM
sentence	I-RESEARCH_PROBLEM
matching	E-RESEARCH_PROBLEM
is	O
a	O
fundamental	O
technology	O
for	O
a	O
variety	O
of	O
tasks	O
.	O
Previous	O
approaches	O
either	O
match	O
sentences	O
from	O
a	O
single	O
direction	O
or	O
only	O
apply	O
single	O
granular	O
(	O
wordby	O
-	O
word	O
or	O
sentence	O
-	O
by-	O
sentence	O
)	O
matching	O
.	O
In	O
this	O
work	O
,	O
we	O
propose	O
a	O
bilateral	O
multi-perspective	O
matching	O
(	O
BiMPM	O
)	O
model	O
.	O
Given	O
two	O
sentences	O
P	O
and	O
Q	O
,	O
our	O
model	O
first	O
encodes	O
them	O
with	O
a	O
BiL	O
-	O
STM	O
encoder	O
.	O
Next	O
,	O
we	O
match	O
the	O
two	O
encoded	O
sentences	O
in	O
two	O
directions	O
P	O
against	O
Q	O
and	O
Q	O
against	O
P	O
.	O
In	O
each	O
matching	O
direction	O
,	O
each	O
time	O
step	O
of	O
one	O
sentence	O
is	O
matched	O
against	O
all	O
timesteps	O
of	O
the	O
other	O
sentence	O
from	O
multiple	O
perspectives	O
.	O
Then	O
,	O
another	O
BiLSTM	O
layer	O
is	O
utilized	O
to	O
aggregate	O
the	O
matching	O
results	O
into	O
a	O
fixed	O
-	O
length	O
matching	O
vector	O
.	O

Attention	O
mechanism	O
has	O
been	O
used	O
as	O
an	O
ancillary	O
means	O
to	O
help	O
RNN	O
or	O
CNN	O
.	O
However	O
,	O
the	O
Transformer	O
(	O
Vaswani	O
et	O
al.	O
,	O
2017	O
)	O
recently	O
recorded	O
the	O
state	O
-	O
of	O
-	O
theart	O
performance	O
in	O
machine	O
translation	O
with	O
a	O
dramatic	O
reduction	O
in	O
training	O
time	O
by	O
solely	O
using	O
attention	O
.	O
Motivated	O
by	O
the	O
Transformer	O
,	O
Directional	O
Self	O
Attention	O
Network	O
(	O
Shen	O
et	O
al.	O
,	O
2017	O
)	O
,	O
a	O
fully	O
attention	O
-	O
based	O
sentence	O
encoder	O
,	O
was	O
proposed	O
.	O
It	O
showed	O
good	O
performance	O
with	O
various	O
data	O
by	O
using	O
forward	O
and	O
backward	O
directional	O
information	O
in	O
a	O
sentence	O
.	O
But	O
in	O
their	O
study	O
,	O
not	O
considered	O
at	O
all	O
was	O
the	O
distance	O
between	O
words	O
,	O
an	O
important	O
feature	O
when	O
learning	O
the	O
local	O
dependency	O
to	O
help	O
understand	O
the	O
context	O
of	O
input	O
text	O
.	O
We	O
propose	O
Distance	O
-	O
based	O
Self	O
-	O
Attention	O
Network	O
,	O
which	O
considers	O
the	O
word	O
distance	O
by	O
using	O
a	O
simple	O
distance	O
mask	O
in	O
order	O
to	O
model	O
the	O
local	O
dependency	O
without	O
losing	O
the	O
ability	O
of	O
modeling	O
global	O
dependency	O
which	O
attention	O
has	O
inherent	O
.	O
Our	O
model	O
shows	O
good	O
performance	O
with	O
NLI	S-RESEARCH_PROBLEM
data	O
,	O
and	O
it	O
records	O
the	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
result	O
with	O
ST0	O
data	O
.	O

Neural	O
network	O
models	O
recently	O
proposed	O
for	O
question	O
answering	O
(	O
QA	S-RESEARCH_PROBLEM
)	O
primarily	O
focus	O
on	O
capturing	O
the	O
passagequestion	O
relation	O
.	O
However	O
,	O
they	O
have	O
minimal	O
capability	O
to	O
link	O
relevant	O
facts	O
distributed	O
across	O
multiple	O
sentences	O
which	O
is	O
crucial	O
in	O
achieving	O
deeper	O
understanding	O
,	O
such	O
as	O
performing	O
multi-sentence	O
reasoning	O
,	O
co-reference	O
resolution	O
,	O
etc	O
.	O
They	O
also	O
do	O
not	O
explicitly	O
focus	O
on	O
the	O
question	O
and	O
answer	O
type	O
which	O
often	O
plays	O
a	O
critical	O
role	O
in	O
QA	S-RESEARCH_PROBLEM
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
novel	O
end	O
-	O
to	O
-	O
end	O
question	O
-	O
focused	O
multi-factor	O
attention	O
network	O
for	O
answer	O
extraction	O
.	O
Multi-factor	O
attentive	O
encoding	O
using	O
tensor	O
-	O
based	O
transformation	O
aggregates	O
meaningful	O
facts	O
even	O
when	O
they	O
are	O
located	O
in	O
multiple	O
sentences	O
.	O
To	O
implicitly	O
infer	O
the	O
answer	O
type	O
,	O
we	O
also	O
propose	O
a	O
max-attentional	O
question	O
aggregation	O
mechanism	O
to	O
encode	O
a	O
question	O
vector	O
based	O
on	O
the	O
important	O
words	O
in	O
a	O
question	O
.	O
During	O
prediction	O
,	O
we	O
incorporate	O
sequence	O
-	O
level	O
encoding	O
of	O
the	O
first	O
wh-word	O
and	O
its	O
immediately	O
following	O
word	O
as	O
an	O
additional	O
source	O
of	O
question	O
type	O
information	O
.	O

Sentence	B-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
level	I-RESEARCH_PROBLEM
representations	E-RESEARCH_PROBLEM
are	O
necessary	O
for	O
various	O
NLP	O
tasks	O
.	O
Recurrent	O
neural	O
networks	O
have	O
proven	O
to	O
be	O
very	O
effective	O
in	O
learning	O
distributed	O
representations	O
and	O
can	O
be	O
trained	O
efficiently	O
on	O
natural	O
language	O
inference	O
tasks	O
.	O
We	O
build	O
on	O
top	O
of	O
one	O
such	O
model	O
and	O
propose	O
a	O
hierarchy	O
of	O
BiLSTM	O
and	O
max	O
pooling	O
layers	O
that	O
implements	O
an	O
iterative	O
refinement	O
strategy	O
and	O
yields	O
state	O
of	O
the	O
art	O
results	O
on	O
the	O
SciTail	O
dataset	O
as	O
well	O
as	O
strong	O
results	O
for	O
SNLI	O
and	O
MultiNLI	O
.	O
We	O
can	O
show	O
that	O
the	O
sentence	O
embeddings	O
learned	O
in	O
this	O
way	O
can	O
be	O
utilized	O
in	O
a	O
wide	O
variety	O
of	O
transfer	O
learning	O
tasks	O
,	O
outperforming	O
InferSent	O
on	O
7	O
out	O
of	O
10	O
and	O
SkipThought	O
on	O
8	O
out	O
of	O
9	O
SentEval	O
sentence	O
embedding	O
evaluation	O
tasks	O
.	O
Furthermore	O
,	O
our	O
model	O
beats	O
the	O
InferSent	O
model	O
in	O
8	O
out	O
of	O
10	O
recently	O
published	O
SentEval	O
probing	O
tasks	O
designed	O
to	O
evaluate	O
sentence	O
embeddings	O
'	O
ability	O
to	O
capture	O
some	O
of	O
the	O
important	O
linguistic	O
properties	O
of	O
sentences	O
.	O

The	O
latest	O
work	O
on	O
language	B-RESEARCH_PROBLEM
representations	E-RESEARCH_PROBLEM
carefully	O
integrates	O
contextualized	O
features	O
into	O
language	O
model	O
training	O
,	O
which	O
enables	O
a	O
series	O
of	O
success	O
especially	O
in	O
various	O
machine	O
reading	O
comprehension	O
and	O
natural	O
language	O
inference	O
tasks	O
.	O
However	O
,	O
the	O
existing	O
language	O
representation	O
models	O
including	O
ELMo	O
,	O
GPT	O
and	O
BERT	O
only	O
exploit	O
plain	O
context	O
-	O
sensitive	O
features	O
such	O
as	O
character	O
or	O
word	O
embeddings	O
.	O
They	O
rarely	O
consider	O
incorporating	O
structured	O
semantic	O
information	O
which	O
can	O
provide	O
rich	O
semantics	O
for	O
language	O
representation	O
.	O
To	O
promote	O
natural	O
language	O
understanding	O
,	O
we	O
propose	O
to	O
incorporate	O
explicit	O
contextual	O
semantics	O
from	O
pre-trained	O
semantic	O
role	O
labeling	O
,	O
and	O
introduce	O
an	O
improved	O
language	O
representation	O
model	O
,	O
Semanticsaware	O
BERT	O
(	O
Sem	O
BERT	O
)	O
,	O
which	O
is	O
capable	O
of	O
explicitly	O
absorbing	O
contextual	O
semantics	O
over	O
a	O
BERT	O
backbone	O
.	O
Sem	O
-	O
BERT	O
keeps	O
the	O
convenient	O
usability	O
of	O
its	O
BERT	O
precursor	O
in	O
alight	O
fine	O
-	O
tuning	O
way	O
without	O
substantial	O
task	O
-	O
specific	O
modifications	O
.	O
Compared	O
with	O
BERT	O
,	O
semantics	O
-	O
aware	O
BERT	O
is	O
as	O
simple	O
in	O
concept	O
but	O
more	O
powerful	O
.	O
It	O
obtains	O
new	O
state	O
-	O
of	O
the	O
-	O
art	O
or	O
substantially	O
improves	O
results	O
on	O
ten	O
reading	O
comprehension	O
and	O
language	O
inference	O
tasks	O
.	O

COMBINING	O
LOCAL	O
CONVOLUTION	O
WITH	O
GLOBAL	O
SELF	O
-	O
ATTENTION	O
FOR	O
READING	B-RESEARCH_PROBLEM
COMPRE	I-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
HENSION	E-RESEARCH_PROBLEM
Current	O
end	O
-	O
to	O
-	O
end	O
machine	O
reading	O
and	O
question	O
answering	O
(	O
Q&A	O
)	O
models	O
are	O
primarily	O
based	O
on	O
recurrent	O
neural	O
networks	O
(	O
RNNs	O
)	O
with	O
attention	O
.	O
Despite	O
their	O
success	O
,	O
these	O
models	O
are	O
often	O
slow	O
for	O
both	O
training	O
and	O
inference	O
due	O
to	O
the	O
sequential	O
nature	O
of	O
RNNs	O
.	O
We	O
propose	O
a	O
new	O
Q&A	O
architecture	O
called	O
QANet	O
,	O
which	O
does	O
not	O
require	O
recurrent	O
networks	O
:	O
Its	O
encoder	O
consists	O
exclusively	O
of	O
convolution	O
and	O
self	O
-	O
attention	O
,	O
where	O
convolution	O
models	O
local	O
interactions	O
and	O
self	O
-	O
attention	O
models	O
global	O
interactions	O
.	O
On	O
the	O
SQuAD	O
dataset	O
,	O
our	O
model	O
is	O
3x	O
to	O
13x	O
faster	O
in	O
training	O
and	O
4x	O
to	O
9	O
x	O
faster	O
in	O
inference	O
,	O
while	O
achieving	O
equivalent	O
accuracy	O
to	O
recurrent	O
models	O
.	O
The	O
speed	O
-	O
up	O
gain	O
allows	O
us	O
to	O
train	O
the	O
model	O
with	O
much	O
more	O
data	O
.	O

The	O
concepts	O
of	O
unitary	O
evolution	O
matrices	O
and	O
associative	O
memory	O
have	O
boosted	O
the	O
field	O
of	O
Recurrent	O
Neural	O
Networks	O
(	O
RNN	S-RESEARCH_PROBLEM
)	O
to	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
in	O
a	O
variety	O
of	O
sequential	O
tasks	O
.	O
However	O
,	O
RNN	S-RESEARCH_PROBLEM
still	O
have	O
a	O
limited	O
capacity	O
to	O
manipulate	O
long	O
-	O
term	O
memory	O
.	O
To	O
bypass	O
this	O
weakness	O
the	O
most	O
successful	O
applications	O
of	O
RNN	S-RESEARCH_PROBLEM
use	O
external	O
techniques	O
such	O
as	O
attention	O
mechanisms	O
.	O
In	O
this	O
paper	O
we	O
propose	O
a	O
novel	O
RNN	S-RESEARCH_PROBLEM
model	O
that	O
unifies	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
approaches	O
:	O
Rotational	O
Unit	O
of	O
Memory	O
(	O
RUM	O
)	O
.	O
The	O
core	O
of	O
RUM	O
is	O
its	O
rotational	O
operation	O
,	O
which	O
is	O
,	O
naturally	O
,	O
a	O
unitary	O
matrix	O
,	O
providing	O
architectures	O
with	O
the	O
power	O
to	O
learn	O
long	O
-	O
term	O
dependencies	O
by	O
overcoming	O
the	O
vanishing	O
and	O
exploding	O
gradients	O
problem	O
.	O
Moreover	O
,	O
the	O
rotational	O
unit	O
also	O
serves	O
as	O
associative	O
memory	O
.	O
We	O
evaluate	O
our	O
model	O
on	O
synthetic	O
memorization	O
,	O
question	O
answering	O
and	O
language	O
modeling	O
tasks	O
.	O

Recently	O
,	O
there	O
is	O
rising	O
interest	O
in	O
modelling	B-RESEARCH_PROBLEM
the	I-RESEARCH_PROBLEM
interactions	I-RESEARCH_PROBLEM
of	I-RESEARCH_PROBLEM
two	I-RESEARCH_PROBLEM
sentences	E-RESEARCH_PROBLEM
with	O
deep	O
neural	O
networks	O
.	O
However	O
,	O
most	O
of	O
the	O
existing	O
methods	O
encode	O
two	O
sequences	O
with	O
separate	O
encoders	O
,	O
in	O
which	O
a	O
sentence	O
is	O
encoded	O
with	O
little	O
or	O
no	O
information	O
from	O
the	O
other	O
sentence	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
deep	O
architecture	O
to	O
model	O
the	O
strong	O
interaction	O
of	O
sentence	O
pair	O
with	O
two	O
coupled	O
-	O
LSTMs	O
.	O
Specifically	O
,	O
we	O
introduce	O
two	O
coupled	O
ways	O
to	O
model	O
the	O
interdependences	O
of	O
two	O
LSTMs	O
,	O
coupling	O
the	O
local	O
contextualized	O
interactions	O
of	O
two	O
sentences	O
.	O
We	O
then	O
aggregate	O
these	O
interactions	O
and	O
use	O
a	O
dynamic	O
pooling	O
to	O
select	O
the	O
most	O
informative	O
features	O
.	O
Experiments	O
on	O
two	O
very	O
large	O
datasets	O
demonstrate	O
the	O
efficacy	O
of	O
our	O
proposed	O
architecture	O
and	O
its	O
superiority	O
to	O
state	O
-	O
of	O
the	O
-	O
art	O
methods	O
.	O

A	O
lot	O
of	O
the	O
recent	O
success	O
in	O
natural	O
language	O
processing	O
(	O
NLP	O
)	O
has	O
been	O
driven	O
by	O
distributed	B-RESEARCH_PROBLEM
vector	I-RESEARCH_PROBLEM
representations	I-RESEARCH_PROBLEM
of	I-RESEARCH_PROBLEM
words	E-RESEARCH_PROBLEM
trained	O
on	O
large	O
amounts	O
of	O
text	O
in	O
an	O
unsupervised	O
manner	O
.	O
These	O
representations	O
are	O
typically	O
used	O
as	O
general	O
purpose	O
features	O
for	O
words	O
across	O
a	O
range	O
of	O
NLP	O
problems	O
.	O
However	O
,	O
extending	O
this	O
success	O
to	O
learning	B-RESEARCH_PROBLEM
representations	I-RESEARCH_PROBLEM
of	I-RESEARCH_PROBLEM
sequences	I-RESEARCH_PROBLEM
of	I-RESEARCH_PROBLEM
words	E-RESEARCH_PROBLEM
,	O
such	O
as	O
sentences	O
,	O
remains	O
an	O
open	O
problem	O
.	O
Recent	O
work	O
has	O
explored	O
unsupervised	O
as	O
well	O
as	O
supervised	O
learning	O
techniques	O
with	O
different	O
training	O
objectives	O
to	O
learn	O
general	O
purpose	O
fixed	O
-	O
length	O
sentence	O
representations	O
.	O
In	O
this	O
work	O
,	O
we	O
present	O
a	O
simple	O
,	O
effective	O
multi-task	O
learning	O
framework	O
for	O
sentence	O
representations	O
that	O
combines	O
the	O
inductive	O
biases	O
of	O
diverse	O
training	O
objectives	O
in	O
a	O
single	O
model	O
.	O
We	O
train	O
this	O
model	O
on	O
several	O
data	O
sources	O
with	O
multiple	O
training	O
objectives	O
on	O
over	O
100	O
million	O
sentences	O
.	O
Extensive	O
experiments	O
demonstrate	O
that	O
sharing	O
a	O
single	O
recurrent	O
sentence	O
encoder	O
across	O
weakly	O
related	O
tasks	O
leads	O
to	O
consistent	O
improvements	O
over	O
previous	O
methods	O
.	O

Text	O
-	O
based	O
question	O
answering	O
(	O
TBT0	O
)	O
has	O
been	O
studied	O
extensively	O
in	O
recent	O
years	O
.	O
Most	O
existing	O
approaches	O
focus	O
on	O
finding	O
the	O
answer	O
to	O
a	O
question	O
within	O
a	O
single	O
paragraph	O
.	O
However	O
,	O
many	O
difficult	O
questions	O
require	O
multiple	O
supporting	O
evidence	O
from	O
scattered	O
text	O
across	O
two	O
or	O
more	O
documents	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
the	O
Dynamically	O
Fused	O
Graph	O
Network	O
(	O
DFGN	O
)	O
,	O
a	O
novel	O
method	O
to	O
answer	O
those	O
questions	O
requiring	O
multiple	O
scattered	O
evidence	O
and	O
reasoning	O
over	O
them	O
.	O
Inspired	O
by	O
human	O
's	O
step	O
-	O
by	O
-	O
step	O
reasoning	O
behavior	O
,	O
DFGN	O
includes	O
a	O
dynamic	O
fusion	O
layer	O
that	O
starts	O
from	O
the	O
entities	O
mentioned	O
in	O
the	O
given	O
query	O
,	O
explores	O
along	O
the	O
entity	O
graph	O
dynamically	O
built	O
from	O
the	O
text	O
,	O
and	O
gradually	O
finds	O
relevant	O
supporting	O
entities	O
from	O
the	O
given	O
documents	O
.	O
We	O
evaluate	O
DFGN	O
on	O
HotpotT2	O
,	O
a	O
public	O
TBT1	O
dataset	O
requiring	O
multi-hop	O
reasoning	O
.	O
DFGN	O
achieves	O
competitive	O
results	O
on	O
the	O
public	O
board	O
.	O

This	O
study	O
tackles	O
generative	O
reading	O
comprehension	O
(	O
RC	S-RESEARCH_PROBLEM
)	O
,	O
which	O
consists	O
of	O
answering	O
questions	O
based	O
on	O
textual	O
evidence	O
and	O
natural	O
language	O
generation	O
(	O
NLG	O
)	O
.	O
We	O
propose	O
a	O
multi-style	O
abstractive	O
summarization	O
model	O
for	O
question	O
answering	O
,	O
called	O
Masque	O
.	O
The	O
proposed	O
model	O
has	O
two	O
key	O
characteristics	O
.	O
First	O
,	O
unlike	O
most	O
studies	O
on	O
RC	S-RESEARCH_PROBLEM
that	O
have	O
focused	O
on	O
extracting	O
an	O
answer	O
span	O
from	O
the	O
provided	O
passages	O
,	O
our	O
model	O
instead	O
focuses	O
on	O
generating	O
a	O
summary	O
from	O
the	O
question	O
and	O
multiple	O
passages	O
.	O
This	O
serves	O
to	O
cover	O
various	O
answer	O
styles	O
required	O
for	O
real	O
-	O
world	O
applications	O
.	O
Second	O
,	O
whereas	O
previous	O
studies	O
built	O
a	O
specific	O
model	O
for	O
each	O
answer	O
style	O
because	O
of	O
the	O
difficulty	O
of	O
acquiring	O
one	O
general	O
model	O
,	O
our	O
approach	O
learns	O
multi-style	O
answers	O
within	O
a	O
model	O
to	O
improve	O
the	O
NLG	O
capability	O
for	O
all	O
styles	O
involved	O
.	O
This	O
also	O
enables	O
our	O
model	O
to	O
give	O
an	O
answer	O
in	O
the	O
target	O
style	O
.	O

We	O
consider	O
the	O
problem	O
of	O
adapting	O
neural	B-RESEARCH_PROBLEM
paragraph	I-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
level	I-RESEARCH_PROBLEM
question	I-RESEARCH_PROBLEM
answering	E-RESEARCH_PROBLEM
models	O
to	O
the	O
case	O
where	O
entire	O
documents	O
are	O
given	O
as	O
input	O
.	O
Our	O
proposed	O
solution	O
trains	O
models	O
to	O
produce	O
well	O
calibrated	O
confidence	O
scores	O
for	O
their	O
results	O
on	O
individual	O
paragraphs	O
.	O
We	O
sample	O
multiple	O
paragraphs	O
from	O
the	O
documents	O
during	O
training	O
,	O
and	O
use	O
a	O
sharednormalization	O
training	O
objective	O
that	O
encourages	O
the	O
model	O
to	O
produce	O
globally	O
correct	O
output	O
.	O
We	O
combine	O
this	O
method	O
with	O
a	O
stateof	O
-	O
the	O
-	O
art	O
pipeline	O
for	O
training	O
models	O
on	O
document	O
QA	O
data	O
.	O
Experiments	O
demonstrate	O
strong	O
performance	O
on	O
several	O
document	O
QA	O
datasets	O
.	O
Overall	O
,	O
we	O
are	O
able	O
to	O
achieve	O
a	O
score	O
of	O
71.3	O
F1	O
on	O
the	O
web	O
portion	O
of	O
Triv	O
-	O
iaQA	O
,	O
a	O
large	O
improvement	O
from	O
the	O
56.7	O
F1	O
of	O
the	O
previous	O
best	O
system	O
.	O

Textual	B-RESEARCH_PROBLEM
similarity	I-RESEARCH_PROBLEM
measurement	E-RESEARCH_PROBLEM
is	O
a	O
challenging	O
problem	O
,	O
as	O
it	O
requires	O
understanding	O
the	O
semantics	O
of	O
input	O
sentences	O
.	O
Most	O
previous	O
neural	O
network	O
models	O
use	O
coarse	O
-	O
grained	O
sentence	O
modeling	O
,	O
which	O
has	O
difficulty	O
capturing	O
fine	O
-	O
grained	O
word	O
-	O
level	O
information	O
for	O
semantic	O
comparisons	O
.	O
As	O
an	O
alternative	O
,	O
we	O
propose	O
to	O
explicitly	O
model	O
pairwise	O
word	O
interactions	O
and	O
present	O
a	O
novel	O
similarity	O
focus	O
mechanism	O
to	O
identify	O
important	O
correspondences	O
for	O
better	O
similarity	O
measurement	O
.	O
Our	O
ideas	O
are	O
implemented	O
in	O
a	O
novel	O
neural	O
network	O
architecture	O
that	O
demonstrates	O
state	O
-	O
of	O
the	O
-	O
art	O
accuracy	O
on	O
three	O
SemEval	O
tasks	O
and	O
two	O
answer	O
selection	O
tasks	O
.	O

Training	B-RESEARCH_PROBLEM
recurrent	I-RESEARCH_PROBLEM
neural	I-RESEARCH_PROBLEM
networks	I-RESEARCH_PROBLEM
to	I-RESEARCH_PROBLEM
model	I-RESEARCH_PROBLEM
long	I-RESEARCH_PROBLEM
term	I-RESEARCH_PROBLEM
dependencies	E-RESEARCH_PROBLEM
is	O
difficult	O
.	O
Hence	O
,	O
we	O
propose	O
to	O
use	O
external	O
linguistic	O
knowledge	O
as	O
an	O
explicit	O
signal	O
to	O
inform	O
the	O
model	O
which	O
memories	O
it	O
should	O
utilize	O
.	O
Specifically	O
,	O
external	O
knowledge	O
is	O
used	O
to	O
augment	O
a	O
sequence	O
with	O
typed	O
edges	O
between	O
arbitrarily	O
distant	O
elements	O
,	O
and	O
the	O
resulting	O
graph	O
is	O
decomposed	O
into	O
directed	O
acyclic	O
subgraphs	O
.	O
We	O
introduce	O
a	O
model	O
that	O
encodes	O
such	O
graphs	O
as	O
explicit	O
memory	O
in	O
recurrent	O
neural	O
networks	O
,	O
and	O
use	O
it	O
to	O
model	O
coreference	O
relations	O
in	O
text	O
.	O
We	O
apply	O
our	O
model	O
to	O
several	O
text	O
comprehension	O
tasks	O
and	O
achieve	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
all	O
considered	O
benchmarks	O
,	O
including	O
CNN	O
,	O
bAbi	O
,	O
and	O
LAMBADA	O
.	O
On	O
the	O
bAbi	O
QA	O
tasks	O
,	O
our	O
model	O
solves	O
15	O
out	O
of	O
the	O
20	O
tasks	O
with	O
only	O
1000	O
training	O
examples	O
per	O
task	O
.	O
Analysis	O
of	O
the	O
learned	O
representations	O
further	O
demonstrates	O
the	O
ability	O
of	O
our	O
model	O
to	O
encode	O
fine	O
-	O
grained	O
entity	O
information	O
across	O
a	O
document	O
.	O

We	O
present	O
BART	O
,	O
a	O
denoising	O
autoencoder	O
for	O
pretraining	B-RESEARCH_PROBLEM
sequence	I-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
to	I-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
sequence	I-RESEARCH_PROBLEM
models	E-RESEARCH_PROBLEM
.	O
BART	O
is	O
trained	O
by	O
(	O
1	O
)	O
corrupting	O
text	O
with	O
an	O
arbitrary	O
noising	O
function	O
,	O
and	O
(	O
2	O
)	O
learning	O
a	O
model	O
to	O
reconstruct	O
the	O
original	O
text	O
.	O
It	O
uses	O
a	O
standard	O
Tranformer	O
-	O
based	O
neural	O
machine	O
translation	O
architecture	O
which	O
,	O
despite	O
its	O
simplicity	O
,	O
can	O
be	O
seen	O
as	O
generalizing	O
BERT	O
(	O
due	O
to	O
the	O
bidirectional	O
encoder	O
)	O
,	O
GPT	O
(	O
with	O
the	O
left	O
-	O
to	O
-	O
right	O
decoder	O
)	O
,	O
and	O
many	O
other	O
more	O
recent	O
pretraining	O
schemes	O
.	O
We	O
evaluate	O
a	O
number	O
of	O
noising	O
approaches	O
,	O
finding	O
the	O
best	O
performance	O
by	O
both	O
randomly	O
shuffling	O
the	O
order	O
of	O
the	O
original	O
sentences	O
and	O
using	O
a	O
novel	O
in	O
-	O
filling	O
scheme	O
,	O
where	O
spans	O
of	O
text	O
are	O
replaced	O
with	O
a	O
single	O
mask	O
token	O
.	O
BART	O
is	O
particularly	O
effective	O
when	O
fine	O
tuned	O
for	O
text	O
generation	O
but	O
also	O
works	O
well	O
for	O
comprehension	O
tasks	O
.	O
It	O
matches	O
the	O
performance	O
of	O
RoBERTa	O
with	O
comparable	O
training	O
resources	O
on	O
GLUE	O
and	O
SQuAD	O
,	O
achieves	O
new	O
stateof	O
-	O
the	O
-	O
art	O
results	O
on	O
a	O
range	O
of	O
abstractive	O
dialogue	O
,	O
question	O
answering	O
,	O
and	O
summarization	O
tasks	O
,	O
with	O
gains	O
of	O
up	O
to	O
6	O
ROUGE	O
.	O
BART	O
also	O
provides	O
a	O
1.1	O
BLEU	O
increase	O
over	O
a	O
back	O
-	O
translation	O
system	O
for	O
machine	O
translation	O
,	O
with	O
only	O
target	O
language	O
pretraining	O
.	O

Recurrent	O
neural	O
networks	O
are	O
now	O
the	O
state	O
-	O
of	O
the	O
-	O
art	O
in	O
natural	O
language	O
processing	O
because	O
they	O
can	O
build	O
rich	O
contextual	O
representations	O
and	O
process	O
texts	O
of	O
arbitrary	O
length	O
.	O
However	O
,	O
recent	O
developments	O
on	O
attention	O
mechanisms	O
have	O
equipped	O
feedforward	O
networks	O
with	O
similar	O
capabilities	O
,	O
hence	O
enabling	O
faster	O
computations	O
due	O
to	O
the	O
increase	O
in	O
the	O
number	O
of	O
operations	O
that	O
can	O
be	O
parallelized	O
.	O
We	O
explore	O
this	O
new	O
type	O
of	O
architecture	O
in	O
the	O
domain	O
of	O
question	O
-	O
answering	O
and	O
propose	O
a	O
novel	O
approach	O
that	O
we	O
call	O
Fully	O
Attention	O
Based	O
Information	B-RESEARCH_PROBLEM
Retriever	E-RESEARCH_PROBLEM
(	O
FABIR	O
)	O
.	O
We	O
show	O
that	O
FABIR	O
achieves	O
competitive	O
results	O
in	O
the	O
Stanford	O
Question	O
Answering	O
Dataset	O
(	O
SQuAD	O
)	O
while	O
having	O
fewer	O
parameters	O
and	O
being	O
faster	O
at	O
both	O
learning	O
and	O
inference	O
than	O
rival	O
methods	O
.	O
I.	O
INTRODUCTION	O
Question	O
-	O
answering	O
(	O
QA	S-RESEARCH_PROBLEM
)	O
systems	O
that	O
can	O
answer	O
queries	O
expressed	O
in	O
natural	O
language	O
have	O
been	O
a	O
perennial	O
goal	O
of	O
the	O
artificial	O
intelligence	O
community	O
.	O
An	O
interesting	O
strategy	O
in	O
the	O
design	O
of	O
such	O
systems	O
is	O
information	O
extraction	O
,	O
where	O
the	O
answer	O
is	O
sought	O
in	O
a	O
set	O
of	O
support	O
documents	O
.	O

Neural	O
models	O
for	O
question	O
answering	O
(	O
QA	S-RESEARCH_PROBLEM
)	O
over	O
documents	O
have	O
achieved	O
significant	O
performance	O
improvements	O
.	O
Although	O
effective	O
,	O
these	O
models	O
do	O
not	O
scale	O
to	O
large	O
corpora	O
due	O
to	O
their	O
complex	O
modeling	O
of	O
interactions	O
between	O
the	O
document	O
and	O
the	O
question	O
.	O
Moreover	O
,	O
recent	O
work	O
has	O
shown	O
that	O
such	O
models	O
are	O
sensitive	O
to	O
adversarial	O
inputs	O
.	O
In	O
this	O
paper	O
,	O
we	O
study	O
the	O
minimal	O
context	O
required	O
to	O
answer	O
the	O
question	O
,	O
and	O
find	O
that	O
most	O
questions	O
in	O
existing	O
datasets	O
can	O
be	O
answered	O
with	O
a	O
small	O
set	O
of	O
sentences	O
.	O
Inspired	O
by	O
this	O
observation	O
,	O
we	O
propose	O
a	O
simple	O
sentence	O
selector	O
to	O
select	O
the	O
minimal	O
set	O
of	O
sentences	O
to	O
feed	O
into	O
the	O
QA	S-RESEARCH_PROBLEM
model	O
.	O
Our	O
over	O
all	O
system	O
achieves	O
significant	O
reductions	O
in	O
training	O
(	O
up	O
to	O
15	O
times	O
)	O
and	O
inference	O
times	O
(	O
up	O
to	O
13	O
times	O
)	O
,	O
with	O
accuracy	O
comparable	O
to	O
or	O
better	O
than	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
on	O
SQuAD	O
,	O
News	O
QA	S-RESEARCH_PROBLEM
,	O
Trivia	O
QA	S-RESEARCH_PROBLEM
and	O
SQuAD	O
-	O
Open	O
.	O
Furthermore	O
,	O
our	O
experimental	O
results	O
and	O
analyses	O
show	O
that	O
our	O
approach	O
is	O
more	O
robust	O
to	O
adversarial	O
inputs	O
.	O

Reading	O
comprehension	O
(	O
RC	S-RESEARCH_PROBLEM
)	O
-	O
in	O
contrast	O
to	O
information	O
retrieval	O
-	O
requires	O
integrating	O
information	O
and	O
reasoning	O
about	O
events	O
,	O
entities	O
,	O
and	O
their	O
relations	O
across	O
a	O
full	O
document	O
.	O
Question	O
answering	O
is	O
conventionally	O
used	O
to	O
assess	O
RC	S-RESEARCH_PROBLEM
ability	O
,	O
in	O
both	O
artificial	O
agents	O
and	O
children	O
learning	O
to	O
read	O
.	O
However	O
,	O
existing	O
RC	S-RESEARCH_PROBLEM
datasets	O
and	O
tasks	O
are	O
dominated	O
by	O
questions	O
that	O
can	O
be	O
solved	O
by	O
selecting	O
answers	O
using	O
superficial	O
information	O
(	O
e.g.	O
,	O
local	O
context	O
similarity	O
or	O
global	O
term	O
frequency	O
)	O
;	O
they	O
thus	O
fail	O
to	O
test	O
for	O
the	O
essential	O
integrative	O
aspect	O
of	O
RC	S-RESEARCH_PROBLEM
.	O
To	O
encourage	O
progress	O
on	O
deeper	O
comprehension	O
of	O
language	O
,	O
we	O
present	O
a	O
new	O
dataset	O
and	O
set	O
of	O
tasks	O
in	O
which	O
the	O
reader	O
must	O
answer	O
questions	O
about	O
stories	O
by	O
reading	O
entire	O
books	O
or	O
movie	O
scripts	O
.	O
These	O
tasks	O
are	O
designed	O
so	O
that	O
successfully	O
answering	O
their	O
questions	O
requires	O
understanding	O
the	O
underlying	O
narrative	O
rather	O
than	O
relying	O
on	O
shallow	O
pattern	O
matching	O
or	O
salience	O
.	O
We	O
show	O
that	O
although	O
humans	O
solve	O
the	O
tasks	O
easily	O
,	O
standard	O
RC	S-RESEARCH_PROBLEM
models	O
struggle	O
on	O
the	O
tasks	O
presented	O
here	O
.	O
We	O
provide	O
an	O
analysis	O
of	O
the	O
dataset	O
and	O
the	O
challenges	O
it	O
presents	O
.	O

Understanding	O
entailment	O
and	O
contradiction	O
is	O
fundamental	O
to	O
understanding	O
natural	O
language	O
,	O
and	O
inference	O
about	O
entailment	O
and	O
contradiction	O
is	O
a	O
valuable	O
testing	O
ground	O
for	O
the	O
development	O
of	O
semantic	O
representations	O
.	O
However	O
,	O
machine	O
learning	O
research	O
in	O
this	O
are	O
a	O
has	O
been	O
dramatically	O
limited	O
by	O
the	O
lack	O
of	O
large	O
-	O
scale	O
resources	O
.	O
To	O
address	O
this	O
,	O
we	O
introduce	O
the	O
Stanford	O
Natural	O
Language	O
Inference	O
corpus	O
,	O
a	O
new	O
,	O
freely	O
available	O
collection	O
of	O
labeled	O
sentence	O
pairs	O
,	O
written	O
by	O
humans	O
doing	O
a	O
novel	O
grounded	O
task	O
based	O
on	O
image	O
captioning	O
.	O
At	O
570	O
K	O
pairs	O
,	O
it	O
is	O
two	O
orders	O
of	O
magnitude	O
larger	O
than	O
all	O
other	O
resources	O
of	O
its	O
type	O
.	O
This	O
increase	O
in	O
scale	O
allows	O
lexicalized	O
classifiers	O
to	O
outperform	O
some	O
sophisticated	O
existing	O
entailment	O
models	O
,	O
and	O
it	O
allows	O
a	O
neural	O
network	O
-	O
based	O
model	O
to	O
perform	O
competitively	O
on	O
natural	B-RESEARCH_PROBLEM
language	I-RESEARCH_PROBLEM
inference	E-RESEARCH_PROBLEM
benchmarks	O
for	O
the	O
first	O
time	O
.	O

We	O
propose	O
a	O
novel	O
neural	O
attention	O
architecture	O
to	O
tackle	O
machine	B-RESEARCH_PROBLEM
comprehension	E-RESEARCH_PROBLEM
tasks	O
,	O
such	O
as	O
answering	O
Cloze	O
-	O
style	O
queries	O
with	O
respect	O
to	O
a	O
document	O
.	O
Unlike	O
previous	O
models	O
,	O
we	O
do	O
not	O
collapse	O
the	O
query	O
into	O
a	O
single	O
vector	O
,	O
instead	O
we	O
deploy	O
an	O
iterative	O
alternating	O
attention	O
mechanism	O
that	O
allows	O
a	O
fine	O
-	O
grained	O
exploration	O
of	O
both	O
the	O
query	O
and	O
the	O
document	O
.	O
Our	O
model	O
outperforms	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
baselines	O
in	O
standard	O
machine	B-RESEARCH_PROBLEM
comprehension	E-RESEARCH_PROBLEM
benchmarks	O
such	O
as	O
CNN	O
news	O
articles	O
and	O
the	O
Children	O
's	O
Book	O
Test	O
(	O
CBT	O
)	O
dataset	O
.	O

Natural	O
Language	O
Inference	O
(	O
NLI	S-RESEARCH_PROBLEM
)	O
task	O
requires	O
an	O
agent	O
to	O
determine	O
the	O
logical	O
relationship	O
between	O
a	O
natural	O
language	O
premise	O
and	O
a	O
natural	O
language	O
hypothesis	O
.	O
We	O
introduce	O
Interactive	O
Inference	O
Network	O
(	O
IIN	O
)	O
,	O
a	O
novel	O
class	O
of	O
neural	O
network	O
architectures	O
that	O
is	O
able	O
to	O
achieve	O
high	O
-	O
level	O
understanding	O
of	O
the	O
sentence	O
pair	O
by	O
hierarchically	O
extracting	O
semantic	O
features	O
from	O
interaction	O
space	O
.	O
We	O
show	O
that	O
an	O
interaction	O
tensor	O
(	O
attention	O
weight	O
)	O
contains	O
semantic	O
information	O
to	O
solve	O
natural	O
language	O
inference	O
,	O
and	O
a	O
denser	O
interaction	O
tensor	O
contains	O
richer	O
semantic	O
information	O
.	O
One	O
instance	O
of	O
such	O
architecture	O
,	O
Densely	O
Interactive	O
Inference	O
Network	O
(	O
DIIN	O
)	O
,	O
demonstrates	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
large	O
scale	O
NLI	S-RESEARCH_PROBLEM
copora	O
and	O
large	O
-	O
scale	O
NLI	S-RESEARCH_PROBLEM
alike	O
corpus	O
.	O
It	O
's	O
noteworthy	O
that	O
DIIN	O
achieve	O
a	O
greater	O
than	O
20	O
%	O
error	O
reduction	O
on	O
the	O
challenging	O
Multi	O
-	O
Genre	O
NLI	S-RESEARCH_PROBLEM
(	O
MultiT6	O
;	O
Williams	O
et	O
al.	O
2017	O
)	O
dataset	O
with	O
respect	O
to	O
the	O
strongest	O
published	O
system	O
.	O
INTRODUCTION	O
Natural	O
Language	O
Inference	O
(	O
NLI	S-RESEARCH_PROBLEM
also	O
known	O
as	O
recognizing	B-RESEARCH_PROBLEM
textual	I-RESEARCH_PROBLEM
entiailment	E-RESEARCH_PROBLEM
,	O
or	O
RTE	O
)	O
task	O
requires	O
one	O
to	O
determine	O
whether	O
the	O
logical	O
relationship	O
between	O
two	O
sentences	O
is	O
among	O
entailment	O
(	O
if	O
the	O
premise	O
is	O
true	O
,	O
then	O
the	O
hypothesis	O
must	O
be	O
true	O
)	O
,	O
contradiction	O
(	O
if	O
the	O
premise	O
is	O
true	O
,	O
then	O
the	O
hypothesis	O
must	O
be	O
false	O
)	O
and	O
neutral	O
(	O
neither	O
entailment	O
nor	O
contradiction	O
)	O
.	O

In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
novel	O
method	O
for	O
a	O
sentence	B-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
level	I-RESEARCH_PROBLEM
answer-	I-RESEARCH_PROBLEM
selection	E-RESEARCH_PROBLEM
task	O
that	O
is	O
a	O
fundamental	O
problem	O
in	O
natural	O
language	O
processing	O
.	O
First	O
,	O
we	O
explore	O
the	O
effect	O
of	O
additional	O
information	O
by	O
adopting	O
a	O
pretrained	O
language	O
model	O
to	O
compute	O
the	O
vector	O
representation	O
of	O
the	O
input	O
text	O
and	O
by	O
applying	O
transfer	O
learning	O
from	O
a	O
large	O
-	O
scale	O
corpus	O
.	O
Second	O
,	O
we	O
enhance	O
the	O
compare	O
-	O
aggregate	O
model	O
by	O
proposing	O
a	O
novel	O
latent	O
clustering	O
method	O
to	O
compute	O
additional	O
information	O
within	O
the	O
target	O
corpus	O
and	O
by	O
changing	O
the	O
objective	O
function	O
from	O
listwise	O
to	O
pointwise	O
.	O
To	O
evaluate	O
the	O
performance	O
of	O
the	O
proposed	O
approaches	O
,	O
experiments	O
are	O
performed	O
with	O
the	O
WikiQA	O
and	O
TREC	O
-	O
QA	O
datasets	O
.	O
The	O
empirical	O
results	O
demonstrate	O
the	O
superiority	O
of	O
our	O
proposed	O
approach	O
,	O
which	O
achieve	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
for	O
both	O
datasets	O
.	O
INTRODUCTION	O
Automatic	O
question	O
answering	O
(	O
QA	O
)	O
is	O
a	O
primary	O
objective	O
of	O
artificial	O
intelligence	O
.	O

Neural	O
networks	O
powered	O
with	O
external	O
memory	O
simulate	O
computer	O
behaviors	O
.	O
These	O
models	O
,	O
which	O
use	O
the	O
memory	O
to	O
store	O
data	O
for	O
a	O
neural	O
controller	O
,	O
can	O
learn	O
algorithms	O
and	O
other	O
complex	O
tasks	O
.	O
In	O
this	O
paper	O
,	O
we	O
introduce	O
a	O
new	O
memory	O
to	O
store	O
weights	O
for	O
the	O
controller	O
,	O
analogous	O
to	O
the	O
stored	B-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
program	I-RESEARCH_PROBLEM
memory	E-RESEARCH_PROBLEM
in	O
modern	O
computer	O
architectures	O
.	O
The	O
proposed	O
model	O
,	O
dubbed	O
Neural	B-RESEARCH_PROBLEM
Stored	I-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
program	I-RESEARCH_PROBLEM
Memory	E-RESEARCH_PROBLEM
,	O
augments	O
current	O
memory	O
-	O
augmented	O
neural	O
networks	O
,	O
creating	O
differentiable	O
machines	O
that	O
can	O
switch	O
programs	O
through	O
time	O
,	O
adapt	O
to	O
variable	O
contexts	O
and	O
thus	O
resemble	O
the	O
Universal	O
Turing	O
Machine	O
.	O
A	O
wide	O
range	O
of	O
experiments	O
demonstrate	O
that	O
the	O
resulting	O
machines	O
not	O
only	O
excel	O
in	O
classical	O
algorithmic	O
problems	O
,	O
but	O
also	O
have	O
potential	O
for	O
compositional	O
,	O
continual	O
,	O
few	O
-	O
shot	O
learning	O
and	O
question	O
-	O
answering	O
tasks	O
.	O

This	O
paper	O
presents	O
a	O
system	O
which	O
learns	O
to	O
answer	B-RESEARCH_PROBLEM
questions	I-RESEARCH_PROBLEM
on	I-RESEARCH_PROBLEM
a	I-RESEARCH_PROBLEM
broad	I-RESEARCH_PROBLEM
range	I-RESEARCH_PROBLEM
of	I-RESEARCH_PROBLEM
topics	E-RESEARCH_PROBLEM
from	O
a	O
knowledge	O
base	O
using	O
few	O
handcrafted	O
features	O
.	O
Our	O
model	O
learns	O
low	O
-	O
dimensional	O
embeddings	O
of	O
words	O
and	O
knowledge	O
base	O
constituents	O
;	O
these	O
representations	O
are	O
used	O
to	O
score	O
natural	O
language	O
questions	O
against	O
candidate	O
answers	O
.	O
Training	O
our	O
system	O
using	O
pairs	O
of	O
questions	O
and	O
structured	O
representations	O
of	O
their	O
answers	O
,	O
and	O
pairs	O
of	O
question	O
paraphrases	O
,	O
yields	O
competitive	O
results	O
on	O
a	O
recent	O
benchmark	O
of	O
the	O
literature	O
.	O

This	O
paper	O
is	O
concerned	O
with	O
learning	O
to	O
solve	O
tasks	O
that	O
require	O
a	O
chain	O
of	O
interdependent	O
steps	O
of	O
relational	O
inference	O
,	O
like	O
answering	O
complex	O
questions	O
about	O
the	O
relationships	O
between	O
objects	O
,	O
or	O
solving	O
puzzles	O
where	O
the	O
smaller	O
elements	O
of	O
a	O
solution	O
mutually	O
constrain	O
each	O
other	O
.	O
We	O
introduce	O
the	O
recurrent	B-RESEARCH_PROBLEM
relational	I-RESEARCH_PROBLEM
network	E-RESEARCH_PROBLEM
,	O
a	O
general	O
purpose	O
module	O
that	O
operates	O
on	O
a	O
graph	O
representation	O
of	O
objects	O
.	O
As	O
a	O
generalization	O
of	O
Santoro	O
et	O
al	O
.	O
[	O
2017	O
]	O
's	O
relational	O
network	O
,	O
it	O
can	O
augment	O
any	O
neural	O
network	O
model	O
with	O
the	O
capacity	O
to	O
do	O
many	O
-	O
step	O
relational	O
reasoning	O
.	O
We	O
achieve	O
state	O
of	O
the	O
art	O
results	O
on	O
the	O
bAbI	O
textual	O
question	O
-	O
answering	O
dataset	O
with	O
the	O
recurrent	B-RESEARCH_PROBLEM
relational	I-RESEARCH_PROBLEM
network	E-RESEARCH_PROBLEM
,	O
consistently	O
solving	O
20	O
/	O
20	O
tasks	O
.	O
As	O
bAbI	O
is	O
not	O
particularly	O
challenging	O
from	O
a	O
relational	O
reasoning	O
point	O
of	O
view	O
,	O
we	O
introduce	O
Pretty	O
-	O
CLEVR	O
,	O
a	O
new	O
diagnostic	O
dataset	O
for	O
relational	O
reasoning	O
.	O
In	O
the	O
Pretty	O
-	O
CLEVR	O
set	O
-	O
up	O
,	O
we	O
can	O
vary	O
the	O
question	O
to	O
control	O
for	O
the	O
number	O
of	O
relational	O
reasoning	O
steps	O
thatare	O
required	O
to	O
obtain	O
the	O
answer	O
.	O

Hypernymy	O
,	O
textual	O
entailment	O
,	O
and	O
image	O
captioning	O
can	O
be	O
seen	O
as	O
special	O
cases	O
of	O
a	O
single	B-RESEARCH_PROBLEM
visual	I-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
semantic	I-RESEARCH_PROBLEM
hierarchy	I-RESEARCH_PROBLEM
over	I-RESEARCH_PROBLEM
words	I-RESEARCH_PROBLEM
,	I-RESEARCH_PROBLEM
sentences	I-RESEARCH_PROBLEM
,	I-RESEARCH_PROBLEM
and	I-RESEARCH_PROBLEM
images	E-RESEARCH_PROBLEM
.	O
In	O
this	O
paper	O
we	O
advocate	O
for	O
explicitly	O
modeling	O
the	O
partial	O
order	O
structure	O
of	O
this	O
hierarchy	O
.	O
Towards	O
this	O
goal	O
,	O
we	O
introduce	O
a	O
general	O
method	O
for	O
learning	O
ordered	O
representations	O
,	O
and	O
show	O
how	O
it	O
can	O
be	O
applied	O
to	O
a	O
variety	O
of	O
tasks	O
involving	O
images	O
and	O
language	O
.	O
We	O
show	O
that	O
the	O
resulting	O
representations	O
improve	O
performance	O
over	O
current	O
approaches	O
for	O
hypernym	O
prediction	O
and	O
image	O
-	O
caption	O
retrieval	O
.	O
INTRODUCTION	O
Computer	O
vision	O
and	O
natural	O
language	O
processing	O
are	O
becoming	O
increasingly	O
intertwined	O
.	O
Recent	O
work	O
in	O
vision	O
has	O
moved	O
beyond	O
discriminating	O
between	O
a	O
fixed	O
set	O
of	O
object	O
classes	O
,	O
to	O
automatically	O
generating	O
open	O
-	O
ended	O
lingual	O
descriptions	O
of	O
images	O
.	O

We	O
present	O
a	O
solution	O
to	O
the	O
problem	O
of	O
paraphrase	B-RESEARCH_PROBLEM
identification	I-RESEARCH_PROBLEM
of	I-RESEARCH_PROBLEM
questions	E-RESEARCH_PROBLEM
.	O
We	O
focus	O
on	O
a	O
recent	O
dataset	O
of	O
question	O
pairs	O
annotated	O
with	O
binary	O
paraphrase	O
labels	O
and	O
show	O
that	O
a	O
variant	O
of	O
the	O
decomposable	O
attention	O
model	O
(	O
Parikh	O
et	O
al.	O
,	O
2016	O
)	O
results	O
in	O
accurate	O
performance	O
on	O
this	O
task	O
,	O
while	O
being	O
far	O
simpler	O
than	O
many	O
competing	O
neural	O
architectures	O
.	O
Furthermore	O
,	O
when	O
the	O
model	O
is	O
pretrained	O
on	O
a	O
noisy	O
dataset	O
of	O
automatically	O
collected	O
question	O
paraphrases	O
,	O
it	O
obtains	O
the	O
best	O
reported	O
performance	O
on	O
the	O
dataset	O
.	O

We	O
introduce	O
an	O
architecture	O
to	O
learn	O
joint	B-RESEARCH_PROBLEM
multilingual	I-RESEARCH_PROBLEM
sentence	I-RESEARCH_PROBLEM
representations	E-RESEARCH_PROBLEM
for	O
93	O
languages	O
,	O
belonging	O
to	O
more	O
than	O
30	O
different	O
families	O
and	O
written	O
in	O
28	O
different	O
scripts	O
.	O
Our	O
system	O
uses	O
a	O
single	O
BiLSTM	O
encoder	O
with	O
a	O
shared	O
BPE	O
vocabulary	O
for	O
all	O
languages	O
,	O
which	O
is	O
coupled	O
with	O
an	O
auxiliary	O
decoder	O
and	O
trained	O
on	O
publicly	O
available	O
parallel	O
corpora	O
.	O
This	O
enables	O
us	O
to	O
learn	O
a	O
classifier	O
on	O
top	O
of	O
the	O
resulting	O
embeddings	O
using	O
English	O
annotated	O
data	O
only	O
,	O
and	O
transfer	O
it	O
to	O
any	O
of	O
the	O
93	O
languages	O
without	O
any	O
modification	O
.	O
Our	O
experiments	O
in	O
cross-lingual	O
natural	O
language	O
inference	O
(	O
XNLI	O
dataset	O
)	O
,	O
cross	O
-	O
lingual	O
document	O
classification	O
(	O
ML	O
-	O
Doc	O
dataset	O
)	O
and	O
parallel	O
corpus	O
mining	O
(	O
BUCC	O
dataset	O
)	O
show	O
the	O
effectiveness	O
of	O
our	O
approach	O
.	O
We	O
also	O
introduce	O
a	O
new	O
test	O
set	O
of	O
aligned	O
sentences	O
in	O
112	O
languages	O
,	O
and	O
show	O
that	O
our	O
sentence	O
embeddings	O
obtain	O
strong	O
results	O
in	O
multilingual	O
similarity	O
search	O
even	O
for	O
low	O
-	O
resource	O
languages	O
.	O
Our	O
implementation	O
,	O
the	O
pretrained	O
encoder	O
and	O
the	O
multilingual	O
test	O
set	O
are	O
available	O
at	O
https://github.com	O
/	O
facebookresearch/LASER	O
.	O
.	O
2018	O
.	O
Achieving	O
human	O
parity	O
on	O
automatic	O
Chinese	O
to	O
English	O
news	O
translation	O
.	O

A	O
ention	O
mechanism	O
has	O
been	O
proven	O
e	O
ective	O
on	O
natural	O
language	O
processing	O
.	O
is	O
paper	O
proposes	O
an	O
a	O
ention	O
boosted	O
natural	B-RESEARCH_PROBLEM
language	I-RESEARCH_PROBLEM
inference	E-RESEARCH_PROBLEM
model	O
named	O
a	O
ESIM	O
by	O
adding	O
word	O
a	O
ention	O
and	O
adaptive	O
direction	O
-	O
oriented	O
a	O
ention	O
mechanisms	O
to	O
the	O
traditional	O
Bi	O
-	O
LSTM	O
layer	O
of	O
natural	B-RESEARCH_PROBLEM
language	I-RESEARCH_PROBLEM
inference	E-RESEARCH_PROBLEM
models	O
,	O
e.g.	O
ESIM	O
.	O
is	O
makes	O
the	O
inference	O
model	O
a	O
ESIM	O
has	O
the	O
ability	O
toe	O
ectively	O
learn	O
the	O
representation	O
of	O
words	O
and	O
model	O
the	O
local	O
subsentential	O
inference	O
between	O
pairs	O
of	O
premise	O
and	O
hypothesis	O
.	O
e	O
empirical	O
studies	O
on	O
the	O
ST4	O
,	O
MultiT3	O
and	O
ora	O
benchmarks	O
manifest	O
that	O
a	O
ESIM	O
is	O
superior	O
to	O
the	O
original	O
ESIM	O
model	O
.	O
natural	O
language	O
processing	O
,	O
deep	O
learning	O
,	O
natural	B-RESEARCH_PROBLEM
language	I-RESEARCH_PROBLEM
inference	E-RESEARCH_PROBLEM
,	O
Bi	O
-	O
LSTM	O
INTRODUCTION	O
Natural	O
language	O
inference	O
(	O
NLI	S-RESEARCH_PROBLEM
)	O
is	O
an	O
important	O
and	O
signicant	O
task	O
in	O
natural	O
language	O
processing	O
(	O
NLP	O
)	O
.	O
It	O
concerns	O
whether	O
a	O
hypothesis	O
can	O
be	O
inferred	O
from	O
a	O
premise	O
,	O
requiring	O
understanding	O
of	O
the	O
semantic	O
similarity	O
between	O
the	O
hypothesis	O
and	O
the	O
premise	O
to	O
discriminate	O
their	O
relation	O
.	O

To	O
bridge	O
the	O
gap	O
between	O
Machine	B-RESEARCH_PROBLEM
Reading	I-RESEARCH_PROBLEM
Comprehension	E-RESEARCH_PROBLEM
(	O
MRC	S-RESEARCH_PROBLEM
)	O
models	O
and	O
human	O
beings	O
,	O
which	O
is	O
mainly	O
reflected	O
in	O
the	O
hunger	O
for	O
data	O
and	O
the	O
robustness	O
to	O
noise	O
,	O
in	O
this	O
paper	O
,	O
we	O
explore	O
how	O
to	O
integrate	O
the	O
neural	O
networks	O
of	O
MRC	S-RESEARCH_PROBLEM
models	O
with	O
the	O
general	O
knowledge	O
of	O
human	O
beings	O
.	O
On	O
the	O
one	O
hand	O
,	O
we	O
propose	O
a	O
data	O
enrichment	O
method	O
,	O
which	O
uses	O
WordNet	O
to	O
extract	O
inter-word	O
semantic	O
connections	O
as	O
general	O
knowledge	O
from	O
each	O
given	O
passage	O
-	O
question	O
pair	O
.	O
On	O
the	O
other	O
hand	O
,	O
we	O
propose	O
an	O
end	O
-	O
to	O
-	O
end	O
MRC	S-RESEARCH_PROBLEM
model	O
named	O
as	O
Knowledge	O
Aided	O
Reader	O
(	O
KAR	O
)	O
,	O
which	O
explicitly	O
uses	O
the	O
above	O
extracted	O
general	O
knowledge	O
to	O
assist	O
its	O
attention	O
mechanisms	O
.	O
Based	O
on	O
the	O
data	O
enrichment	O
method	O
,	O
KAR	O
is	O
comparable	O
in	O
performance	O
with	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
MRC	S-RESEARCH_PROBLEM
models	O
,	O
and	O
significantly	O
more	O
robust	O
to	O
noise	O
than	O
them	O
.	O
When	O
only	O
a	O
subset	O
(	O
20	O
%	O
-	O
80	O
%	O
)	O
of	O
the	O
training	O
examples	O
are	O
available	O
,	O
KAR	O
outperforms	O
the	O
state	O
-	O
of	O
the	O
-	O
art	O
MRC	S-RESEARCH_PROBLEM
models	O
by	O
a	O
large	O
margin	O
,	O
and	O
is	O
still	O
reasonably	O
robust	O
to	O
noise	O
.	O

Reading	O
Comprehension	O
(	O
RC	S-RESEARCH_PROBLEM
)	O
of	O
text	O
is	O
one	O
of	O
the	O
fundamental	O
tasks	O
in	O
natural	O
language	O
processing	O
.	O
In	O
recent	O
years	O
,	O
several	O
end	O
-	O
to	O
-	O
end	O
neural	O
network	O
models	O
have	O
been	O
proposed	O
to	O
solve	O
RC	S-RESEARCH_PROBLEM
tasks	O
.	O
However	O
,	O
most	O
of	O
these	O
models	O
suffer	O
in	O
reasoning	O
overlong	O
documents	O
.	O
In	O
this	O
work	O
,	O
we	O
propose	O
a	O
novel	O
Memory	O
Augmented	O
Machine	O
Comprehension	O
Network	O
(	O
MAMCN	O
)	O
to	O
address	O
long	O
-	O
range	O
dependencies	O
present	O
in	O
machine	O
reading	O
comprehension	O
.	O
We	O
perform	O
extensive	O
experiments	O
to	O
evaluate	O
proposed	O
method	O
with	O
the	O
renowned	O
benchmark	O
datasets	O
such	O
as	O
SQuAD	O
,	O
QUASAR	O
-	O
T	O
,	O
and	O
Trivia	O
QA	O
.	O
We	O
achieve	O
the	O
state	O
of	O
the	O
art	O
performance	O
on	O
both	O
the	O
document	O
-	O
level	O
(	O
QUASAR	O
-	O
T	O
,	O
TriviaQA	O
)	O
and	O
paragraph	O
-	O
level	O
(	O
SQuAD	O
)	O
datasets	O
compared	O
to	O
all	O
the	O
previously	O
published	O
approaches	O
.	O

We	O
propose	O
TANDA	O
,	O
an	O
effective	O
technique	O
for	O
fine	O
-	O
tuning	O
pre-trained	O
Transformer	O
models	O
for	O
natural	O
language	O
tasks	O
.	O
Specifically	O
,	O
we	O
first	O
transfer	O
a	O
pre-trained	O
model	O
into	O
a	O
model	O
for	O
a	O
general	O
task	O
by	O
fine	O
-	O
tuning	O
it	O
with	O
a	O
large	O
and	O
highquality	O
dataset	O
.	O
We	O
then	O
perform	O
a	O
second	O
fine	O
-	O
tuning	O
step	O
to	O
adapt	O
the	O
transferred	O
model	O
to	O
the	O
target	O
domain	O
.	O
We	O
demonstrate	O
the	O
benefits	O
of	O
our	O
approach	O
for	O
answer	O
sentence	O
selection	O
,	O
which	O
is	O
a	O
well	O
-	O
known	O
inference	O
task	O
in	O
Question	B-RESEARCH_PROBLEM
Answering	E-RESEARCH_PROBLEM
.	O
We	O
built	O
a	O
large	O
scale	O
dataset	O
to	O
enable	O
the	O
transfer	O
step	O
,	O
exploiting	O
the	O
Natural	O
Questions	O
dataset	O
.	O
Our	O
approach	O
establishes	O
the	O
state	O
of	O
the	O
art	O
on	O
two	O
well	O
-	O
known	O
benchmarks	O
,	O
WikiT1	O
and	O
TREC	O
-	O
QA	S-RESEARCH_PROBLEM
,	O
achieving	O
MAP	O
scores	O
of	O
92	O
%	O
and	O
94.3	O
%	O
,	O
respectively	O
,	O
which	O
largely	O
outperform	O
the	O
previous	O
highest	O
scores	O
of	O
83.4	O
%	O
and	O
87.5	O
%	O
,	O
obtained	O
in	O
very	O
recent	O
work	O
.	O
We	O
empirically	O
show	O
that	O
TANDA	O
generates	O
more	O
stable	O
and	O
robust	O
models	O
reducing	O
the	O
effort	O
required	O
for	O
selecting	O
optimal	O
hyper	O
-	O
parameters	O
.	O

Natural	O
language	O
inference	O
(	O
NLI	S-RESEARCH_PROBLEM
)	O
is	O
a	O
fundamentally	O
important	O
task	O
in	O
natural	O
language	O
processing	O
that	O
has	O
many	O
applications	O
.	O
The	O
recently	O
released	O
Stanford	O
Natural	B-RESEARCH_PROBLEM
Language	I-RESEARCH_PROBLEM
Inference	E-RESEARCH_PROBLEM
(	O
ST3	O
)	O
corpus	O
has	O
made	O
it	O
possible	O
to	O
develop	O
and	O
evaluate	O
learning	O
-	O
centered	O
methods	O
such	O
as	O
deep	O
neural	O
networks	O
for	O
natural	O
language	O
inference	O
(	O
NLI	S-RESEARCH_PROBLEM
)	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
special	O
long	O
short	O
-	O
term	O
memory	O
(	O
LSTM	O
)	O
architecture	O
for	O
NLI	S-RESEARCH_PROBLEM
.	O
Our	O
model	O
builds	O
on	O
top	O
of	O
a	O
recently	O
proposed	O
neural	O
attention	O
model	O
for	O
NLI	S-RESEARCH_PROBLEM
but	O
is	O
based	O
on	O
a	O
significantly	O
different	O
idea	O
.	O
Instead	O
of	O
deriving	O
sentence	O
embeddings	O
for	O
the	O
premise	O
and	O
the	O
hypothesis	O
to	O
be	O
used	O
for	O
classification	O
,	O
our	O
solution	O
uses	O
a	O
match	O
-	O
LSTM	O
to	O
perform	O
wordby	O
-	O
word	O
matching	O
of	O
the	O
hypothesis	O
with	O
the	O
premise	O
.	O
This	O
LSTM	O
is	O
able	O
to	O
place	O
more	O
emphasis	O
on	O
important	O
word	O
-	O
level	O
matching	O
results	O
.	O
In	O
particular	O
,	O
we	O
observe	O
that	O
this	O
LSTM	O
remembers	O
important	O
mismatches	O
thatare	O
critical	O
for	O
predicting	O
the	O
contradiction	O
or	O
the	O
neutral	O
relationship	O
label	O
.	O

Machine	B-RESEARCH_PROBLEM
comprehension	I-RESEARCH_PROBLEM
of	I-RESEARCH_PROBLEM
text	E-RESEARCH_PROBLEM
is	O
an	O
important	O
problem	O
in	O
natural	O
language	O
processing	O
.	O
A	O
recently	O
released	O
dataset	O
,	O
the	O
Stanford	O
Question	O
Answering	O
Dataset	O
(	O
SQuAD	O
)	O
,	O
offers	O
a	O
large	O
number	O
of	O
real	O
questions	O
and	O
their	O
answers	O
created	O
by	O
humans	O
through	O
crowdsourcing	O
.	O
SQuAD	O
provides	O
a	O
challenging	O
testbed	O
for	O
evaluating	O
machine	O
comprehension	O
algorithms	O
,	O
partly	O
because	O
compared	O
with	O
previous	O
datasets	O
,	O
in	O
SQuAD	O
the	O
answers	O
do	O
not	O
come	O
from	O
a	O
small	O
set	O
of	O
candidate	O
answers	O
and	O
they	O
have	O
variable	O
lengths	O
.	O
We	O
propose	O
an	O
end	O
-	O
to	O
-	O
end	O
neural	O
architecture	O
for	O
the	O
task	O
.	O
The	O
architecture	O
is	O
based	O
on	O
match	O
-	O
LSTM	O
,	O
a	O
model	O
we	O
proposed	O
previously	O
for	O
textual	O
entailment	O
,	O
and	O
Pointer	O
Net	O
,	O
a	O
sequence	O
-	O
to	O
-	O
sequence	O
model	O
proposed	O
by	O
Vinyals	O
et	O
al.	O
(	O
2015	O
)	O
to	O
constrain	O
the	O
output	O
tokens	O
to	O
be	O
from	O
the	O
input	O
sequences	O
.	O
We	O
propose	O
two	O
ways	O
of	O
using	O
Pointer	O
Net	O
for	O
our	O
task	O
.	O
Our	O
experiments	O
show	O
that	O
both	O
of	O
our	O
two	O
models	O
substantially	O
outperform	O
the	O
best	O
results	O
obtained	O
by	O
Rajpurkar	O
et	O
al.	O
(	O
2016	O
)	O
using	O
logistic	O
regression	O
and	O
manually	O
crafted	O
features	O
.	O

Most	O
Reading	B-RESEARCH_PROBLEM
Comprehension	E-RESEARCH_PROBLEM
methods	O
limit	O
themselves	O
to	O
queries	O
which	O
can	O
be	O
answered	O
using	O
a	O
single	O
sentence	O
,	O
paragraph	O
,	O
or	O
document	O
.	O
Enabling	O
models	O
to	O
combine	O
disjoint	O
pieces	O
of	O
textual	O
evidence	O
would	O
extend	O
the	O
scope	O
of	O
machine	O
comprehension	O
methods	O
,	O
but	O
currently	O
no	O
resources	O
exist	O
to	O
train	O
and	O
test	O
this	O
capability	O
.	O
We	O
propose	O
a	O
novel	O
task	O
to	O
encourage	O
the	O
development	O
of	O
models	O
for	O
text	O
understanding	O
across	O
multiple	O
documents	O
and	O
to	O
investigate	O
the	O
limits	O
of	O
existing	O
methods	O
.	O
In	O
our	O
task	O
,	O
a	O
model	O
learns	O
to	O
seek	O
and	O
combine	O
evidence	O
-	O
effectively	O
performing	O
multihop	O
,	O
alias	O
multi-step	O
,	O
inference	O
.	O
We	O
devise	O
a	O
methodology	O
to	O
produce	O
datasets	O
for	O
this	O
task	O
,	O
given	O
a	O
collection	O
of	O
query	O
-	O
answer	O
pairs	O
and	O
thematically	O
linked	O
documents	O
.	O
Two	O
datasets	O
from	O
different	O
domains	O
are	O
induced	O
,	O
1	O
and	O
we	O
identify	O
potential	O
pitfalls	O
and	O
devise	O
circumvention	O
strategies	O
.	O
We	O
evaluate	O
two	O
previously	O
proposed	O
competitive	O
models	O
and	O
find	O
that	O
one	O
can	O
integrate	O
information	O
across	O
documents	O
.	O

The	O
reading	O
comprehension	O
task	O
,	O
that	O
asks	O
questions	O
about	O
a	O
given	O
evidence	O
document	O
,	O
is	O
a	O
central	O
problem	O
in	O
natural	O
language	O
understanding	O
.	O
Recent	O
formulations	O
of	O
this	O
task	O
have	O
typically	O
focused	O
on	O
answer	O
selection	O
from	O
a	O
set	O
of	O
candidates	O
pre-defined	O
manually	O
or	O
through	O
the	O
use	O
of	O
an	O
external	O
NLP	O
pipeline	O
.	O
However	O
,	O
Rajpurkar	O
et	O
al	O
.	O
(	O
2016	O
)	O
recently	O
released	O
the	O
SQUAD	O
dataset	O
in	O
which	O
the	O
answers	O
can	O
be	O
arbitrary	O
strings	O
from	O
the	O
supplied	O
text	O
.	O
In	O
this	O
paper	O
,	O
we	O
focus	O
on	O
this	O
answer	B-RESEARCH_PROBLEM
extraction	E-RESEARCH_PROBLEM
task	O
,	O
presenting	O
a	O
novel	O
model	O
architecture	O
that	O
efficiently	O
builds	O
fixed	O
length	O
representations	O
of	O
all	O
spans	O
in	O
the	O
evidence	O
document	O
with	O
a	O
recurrent	O
network	O
.	O
We	O
show	O
that	O
scoring	O
explicit	O
span	O
representations	O
significantly	O
improves	O
performance	O
over	O
other	O
approaches	O
that	O
factor	O
the	O
prediction	O
into	O
separate	O
predictions	O
about	O
words	O
or	O
start	O
and	O
end	O
markers	O
.	O
Our	O
approach	O
improves	O
upon	O
the	O
best	O
published	O
results	O
of	O
Wang	O
&	O
Jiang	O
(	O
2016	O
)	O
by	O
5	O
%	O
and	O
decreases	O
the	O
error	O
of	O
Rajpurkar	O
et	O
al.	O
's	O
baseline	O
by	O
>	O
50	O
%.	O
Recently	O
,	O
Rajpurkar	O
et	O
al.	O
(	O
2016	O
)	O
released	O
the	O
less	O
restricted	O
SQUAD	O
dataset	O
1	O
that	O
does	O
not	O
place	O
any	O
constraints	O
on	O
the	O
set	O
of	O
allowed	O
answers	O
,	O
other	O
than	O
that	O
they	O
should	O
be	O
drawn	O
from	O
the	O
evidence	O
document	O
.	O

This	O
paper	O
describes	O
the	O
KeLP	O
system	O
participating	O
in	O
the	O
SemEval	O
-	O
2016	O
Community	O
Question	O
Answering	O
(	O
c	B-RESEARCH_PROBLEM
QA	E-RESEARCH_PROBLEM
)	O
task	O
.	O
The	O
challenge	O
tasks	O
are	O
modeled	O
as	O
binary	O
classification	O
problems	O
:	O
kernel	O
-	O
based	O
classifiers	O
are	O
trained	O
on	O
the	O
SemEval	O
datasets	O
and	O
their	O
scores	O
are	O
used	O
to	O
sort	O
the	O
instances	O
and	O
produce	O
the	O
final	O
ranking	O
.	O
All	O
classifiers	O
and	O
kernels	O
have	O
been	O
implemented	O
within	O
the	O
Kernel	O
-	O
based	O
Learning	O
Platform	O
called	O
KeLP	O
.	O
Our	O
primary	O
submission	O
ranked	O
first	O
in	O
Subtask	O
A	O
,	O
third	O
in	O
Subtask	O
B	O
and	O
second	O
in	O
Subtask	O
C	O
.	O
These	O
ranks	O
are	O
based	O
on	O
MAP	O
,	O
which	O
is	O
the	O
referring	O
challenge	O
system	O
score	O
.	O
Our	O
approach	O
outperforms	O
all	O
the	O
other	O
systems	O
with	O
respect	O
to	O
all	O
the	O
other	O
challenge	O
metrics	O
.	O
2	O

We	O
present	O
a	O
novel	O
recurrent	O
neural	O
network	O
(	O
RNN	O
)	O
based	O
model	O
that	O
combines	O
the	O
remembering	O
ability	O
of	O
unitary	O
RNNs	S-RESEARCH_PROBLEM
with	O
the	O
ability	O
of	O
gated	O
RNNs	S-RESEARCH_PROBLEM
to	O
effectively	O
forget	O
redundant	O
/	O
irrelevant	O
information	O
in	O
its	O
memory	O
.	O
We	O
achieve	O
this	O
by	O
extending	O
unitary	O
RNNs	S-RESEARCH_PROBLEM
with	O
a	O
gating	O
mechanism	O
.	O
Our	O
model	O
is	O
able	O
to	O
outperform	O
LSTMs	O
,	O
GRUs	O
and	O
Unitary	O
RNNs	S-RESEARCH_PROBLEM
on	O
several	O
long	O
-	O
term	O
dependency	O
benchmark	O
tasks	O
.	O
We	O
empirically	O
both	O
show	O
the	O
orthogonal	O
/	O
unitary	O
RNNs	S-RESEARCH_PROBLEM
lack	O
the	O
ability	O
to	O
forget	O
and	O
also	O
the	O
ability	O
of	O
GORU	O
to	O
simultaneously	O
remember	O
long	O
term	O
dependencies	O
while	O
forgetting	O
irrelevant	O
information	O
.	O
This	O
plays	O
an	O
important	O
role	O
in	O
recurrent	O
neural	O
networks	O
.	O
We	O
provide	O
competitive	O
results	O
along	O
with	O
an	O
analysis	O
of	O
our	O
model	O
on	O
many	O
natural	O
sequential	O
tasks	O
including	O
the	O
bAbI	O
Question	O
Answering	O
,	O
TIMIT	O
speech	O
spectrum	O
prediction	O
,	O
Penn	O
TreeBank	O
,	O
and	O
synthetic	O
tasks	O
that	O
involve	O
long	O
-	O
term	O
dependencies	O
such	O
as	O
algorithmic	O
,	O
parenthesis	O
,	O
denoising	O
and	O
copying	O
tasks	O
.	O

We	O
present	O
a	O
new	O
dataset	O
for	O
machine	B-RESEARCH_PROBLEM
comprehension	E-RESEARCH_PROBLEM
in	O
the	O
medical	O
domain	O
.	O
Our	O
dataset	O
uses	O
clinical	O
case	O
reports	O
with	O
around	O
100,000	O
gap	O
-	O
filling	O
queries	O
about	O
these	O
cases	O
.	O
We	O
apply	O
several	O
baselines	O
and	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
neural	O
readers	O
to	O
the	O
dataset	O
,	O
and	O
observe	O
a	O
considerable	O
gap	O
in	O
performance	O
(	O
20	O
%	O
F1	O
)	O
between	O
the	O
best	O
human	O
and	O
machine	O
readers	O
.	O
We	O
analyze	O
the	O
skills	O
required	O
for	O
successful	O
answering	O
and	O
show	O
how	O
reader	O
performance	O
varies	O
depending	O
on	O
the	O
applicable	O
skills	O
.	O
We	O
find	O
that	O
inferences	O
using	O
domain	O
knowledge	O
and	O
object	O
tracking	O
are	O
the	O
most	O
frequently	O
required	O
skills	O
,	O
and	O
that	O
recognizing	O
omitted	O
information	O
and	O
spatio-	O
temporal	O
reasoning	O
are	O
the	O
most	O
difficult	O
for	O
the	O
machines	O
.	O

Many	O
natural	O
language	O
processing	O
tasks	O
solely	O
rely	O
on	O
sparse	O
dependencies	O
between	O
a	O
few	O
tokens	O
in	O
a	O
sentence	O
.	O
Soft	O
attention	B-RESEARCH_PROBLEM
mechanisms	E-RESEARCH_PROBLEM
show	O
promising	O
performance	O
in	O
modeling	O
local	O
/	O
global	O
dependencies	O
by	O
soft	O
probabilities	O
between	O
every	O
two	O
tokens	O
,	O
but	O
they	O
are	O
not	O
effective	O
and	O
efficient	O
when	O
applied	O
to	O
long	O
sentences	O
.	O
By	O
contrast	O
,	O
hard	O
attention	B-RESEARCH_PROBLEM
mechanisms	E-RESEARCH_PROBLEM
directly	O
select	O
a	O
subset	O
of	O
tokens	O
but	O
are	O
difficult	O
and	O
inefficient	O
to	O
train	O
due	O
to	O
their	O
combinatorial	O
nature	O
.	O
In	O
this	O
paper	O
,	O
we	O
integrate	O
both	O
soft	O
and	O
hard	O
attention	O
into	O
one	O
context	O
fusion	O
model	O
,	O
"	O
reinforced	O
self	O
-	O
attention	O
(	O
ReSA	O
)	O
"	O
,	O
for	O
the	O
mutual	O
benefit	O
of	O
each	O
other	O
.	O
In	O
ReSA	O
,	O
a	O
hard	O
attention	O
trims	O
a	O
sequence	O
for	O
a	O
soft	O
self	O
-	O
attention	O
to	O
process	O
,	O
while	O
the	O
soft	O
attention	O
feeds	O
reward	O
signals	O
back	O
to	O
facilitate	O
the	O
training	O
of	O
the	O
hard	O
one	O
.	O
For	O
this	O
purpose	O
,	O
we	O
develop	O
a	O
novel	O
hard	O
attention	O
called	O
"	O
reinforced	O
sequence	O
sampling	O
(	O
RSS	O
)	O
"	O
,	O
selecting	O
tokens	O
in	O
parallel	O
and	O
trained	O
via	O
policy	O
gradient	O
.	O
Using	O
two	O
RSS	O
modules	O
,	O
ReSA	O
efficiently	O
extracts	O
the	O
sparse	O
dependencies	O
between	O
each	O
pair	O
of	O
selected	O
tokens	O
.	O

Natural	B-RESEARCH_PROBLEM
Language	I-RESEARCH_PROBLEM
Inference	E-RESEARCH_PROBLEM
(	O
NLI	S-RESEARCH_PROBLEM
)	O
,	O
also	O
known	O
as	O
Recognizing	O
Textual	O
Entailment	O
(	O
RTE	O
)	O
,	O
is	O
one	O
of	O
the	O
most	O
important	O
problems	O
in	O
natural	O
language	O
processing	O
.	O
It	O
requires	O
to	O
infer	O
the	O
logical	O
relationship	O
between	O
two	O
given	O
sentences	O
.	O
While	O
current	O
approaches	O
mostly	O
focus	O
on	O
the	O
interaction	O
architectures	O
of	O
the	O
sentences	O
,	O
in	O
this	O
paper	O
,	O
we	O
propose	O
to	O
transfer	O
knowledge	O
from	O
some	O
important	O
discourse	O
markers	O
to	O
augment	O
the	O
quality	O
of	O
the	O
NLI	S-RESEARCH_PROBLEM
model	O
.	O
We	O
observe	O
that	O
people	O
usually	O
use	O
some	O
discourse	O
markers	O
such	O
as	O
"	O
so	O
"	O
or	O
"	O
but	O
"	O
to	O
represent	O
the	O
logical	O
relationship	O
between	O
two	O
sentences	O
.	O
These	O
words	O
potentially	O
have	O
deep	O
connections	O
with	O
the	O
meanings	O
of	O
the	O
sentences	O
,	O
thus	O
can	O
be	O
utilized	O
to	O
help	O
improve	O
the	O
representations	O
of	O
them	O
.	O
Moreover	O
,	O
we	O
use	O
reinforcement	O
learning	O
to	O
optimize	O
a	O
new	O
objective	O
function	O
with	O
a	O
reward	O
defined	O
by	O
the	O
property	O
of	O
the	O
NLI	S-RESEARCH_PROBLEM
datasets	O
to	O
make	O
full	O
use	O
of	O
the	O
labels	O
information	O
.	O
Experiments	O
show	O
that	O
our	O
method	O
achieves	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
several	O
large	O
-	O
scale	O
datasets	O
.	O

Directly	B-RESEARCH_PROBLEM
reading	I-RESEARCH_PROBLEM
documents	E-RESEARCH_PROBLEM
and	O
being	O
able	O
to	O
answer	O
questions	O
from	O
them	O
is	O
an	O
unsolved	O
challenge	O
.	O
To	O
avoid	O
its	O
inherent	O
difficulty	O
,	O
question	O
answering	O
(	O
QA	S-RESEARCH_PROBLEM
)	O
has	O
been	O
directed	O
towards	O
using	O
Knowledge	O
Bases	O
(	O
KBs	O
)	O
instead	O
,	O
which	O
has	O
proven	O
effective	O
.	O
Unfortunately	O
KBs	O
often	O
suffer	O
from	O
being	O
too	O
restrictive	O
,	O
as	O
the	O
schema	O
can	O
not	O
support	O
certain	O
types	O
of	O
answers	O
,	O
and	O
too	O
sparse	O
,	O
e.g.	O
Wikipedia	O
contains	O
much	O
more	O
information	O
than	O
Freebase	O
.	O
In	O
this	O
work	O
we	O
introduce	O
a	O
new	O
method	O
,	O
Key	O
-	O
Value	O
Memory	O
Networks	O
,	O
that	O
makes	O
reading	O
documents	O
more	O
viable	O
by	O
utilizing	O
different	O
encodings	O
in	O
the	O
addressing	O
and	O
output	O
stages	O
of	O
the	O
memory	O
read	O
operation	O
.	O
To	O
compare	O
using	O
KBs	O
,	O
information	O
extraction	O
or	O
Wikipedia	O
documents	O
directly	O
in	O
a	O
single	O
framework	O
we	O
construct	O
an	O
analysis	O
tool	O
,	O
WIKIMOVIES	O
,	O
a	O
QA	S-RESEARCH_PROBLEM
dataset	O
that	O
contains	O
raw	O
text	O
alongside	O
a	O
preprocessed	O
KB	O
,	O
in	O
the	O
domain	O
of	O
movies	O
.	O
Our	O
method	O
reduces	O
the	O
gap	O
between	O
all	O
three	O
settings	O
.	O
It	O
also	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
the	O
existing	O
WIKIT2	O
benchmark	O
.	O

Recent	O
development	O
of	O
large	O
-	O
scale	O
question	O
answering	O
(	O
QA	S-RESEARCH_PROBLEM
)	O
datasets	O
triggered	O
a	O
substantial	O
amount	O
of	O
research	O
into	O
end	O
-	O
toend	O
neural	O
architectures	O
for	O
QA	S-RESEARCH_PROBLEM
.	O
Increasingly	O
complex	O
systems	O
have	O
been	O
conceived	O
without	O
comparison	O
to	O
simpler	O
neural	O
baseline	O
systems	O
that	O
would	O
justify	O
their	O
complexity	O
.	O
In	O
this	O
work	O
,	O
we	O
propose	O
a	O
simple	O
heuristic	O
that	O
guides	O
the	O
development	O
of	O
neural	O
baseline	O
systems	O
for	O
the	O
extractive	O
QA	S-RESEARCH_PROBLEM
task	O
.	O
We	O
find	O
that	O
there	O
are	O
two	O
ingredients	O
necessary	O
for	O
building	O
a	O
high	O
-	O
performing	O
neural	O
QA	S-RESEARCH_PROBLEM
system	O
:	O
first	O
,	O
the	O
awareness	O
of	O
question	O
words	O
while	O
processing	O
the	O
context	O
and	O
second	O
,	O
a	O
composition	O
function	O
that	O
goes	O
beyond	O
simple	O
bag	O
-	O
of	O
-	O
words	O
modeling	O
,	O
such	O
as	O
recurrent	O
neural	O
networks	O
.	O
Our	O
results	O
show	O
that	O
FastT3	O
,	O
a	O
system	O
that	O
meets	O
these	O
two	O
requirements	O
,	O
can	O
achieve	O
very	O
competitive	O
performance	O
compared	O
with	O
existing	O
models	O
.	O
We	O
argue	O
that	O
this	O
surprising	O
finding	O
puts	O
results	O
of	O
previous	O
systems	O
and	O
the	O
complexity	O
of	O
recent	O
QA	S-RESEARCH_PROBLEM
datasets	O
into	O
perspective	O
.	O

This	O
paper	O
presents	O
a	O
new	O
deep	O
learning	O
architecture	O
for	O
Natural	B-RESEARCH_PROBLEM
Language	I-RESEARCH_PROBLEM
Inference	E-RESEARCH_PROBLEM
(	O
NLI	S-RESEARCH_PROBLEM
)	O
.	O
Firstly	O
,	O
we	O
introduce	O
a	O
new	O
architecture	O
where	O
alignment	O
pairs	O
are	O
compared	O
,	O
compressed	O
and	O
then	O
propagated	O
to	O
upper	O
layers	O
for	O
enhanced	O
representation	O
learning	O
.	O
Secondly	O
,	O
we	O
adopt	O
factorization	O
layers	O
for	O
efficient	O
and	O
expressive	O
compression	O
of	O
alignment	O
vectors	O
into	O
scalar	O
features	O
,	O
which	O
are	O
then	O
used	O
to	O
augment	O
the	O
base	O
word	O
representations	O
.	O
The	O
design	O
of	O
our	O
approach	O
is	O
aimed	O
to	O
be	O
conceptually	O
simple	O
,	O
compact	O
and	O
yet	O
powerful	O
.	O
We	O
conduct	O
experiments	O
on	O
three	O
popular	O
benchmarks	O
,	O
ST1	O
,	O
MultiT2	O
and	O
SciTail	O
,	O
achieving	O
competitive	O
performance	O
on	O
all	O
.	O
A	O
lightweight	O
parameterization	O
of	O
our	O
model	O
also	O
enjoys	O
a	O
?	O
3	O
times	O
reduction	O
in	O
parameter	O
size	O
compared	O
to	O
the	O
existing	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
,	O
e.g.	O
,	O
ESIM	O
and	O
DIIN	O
,	O
while	O
maintaining	O
competitive	O
performance	O
.	O

We	O
present	O
a	O
novel	O
deep	O
learning	O
architecture	O
to	O
address	O
the	O
natural	O
language	O
inference	O
(	O
NLI	S-RESEARCH_PROBLEM
)	O
task	O
.	O
Existing	O
approaches	O
mostly	O
rely	O
on	O
simple	O
reading	O
mechanisms	O
for	O
independent	O
encoding	O
of	O
the	O
premise	O
and	O
hypothesis	O
.	O
Instead	O
,	O
we	O
propose	O
a	O
novel	O
dependent	O
reading	O
bidirectional	O
LSTM	O
network	O
(	O
DR	O
-	O
BiLSTM	O
)	O
to	O
efficiently	O
model	O
the	O
relationship	O
between	O
a	O
premise	O
and	O
a	O
hypothesis	O
during	O
encoding	O
and	O
inference	O
.	O
We	O
also	O
introduce	O
a	O
sophisticated	O
ensemble	O
strategy	O
to	O
combine	O
our	O
proposed	O
models	O
,	O
which	O
noticeably	O
improves	O
final	O
predictions	O
.	O
Finally	O
,	O
we	O
demonstrate	O
how	O
the	O
results	O
can	O
be	O
improved	O
further	O
with	O
an	O
additional	O
preprocessing	O
step	O
.	O
Our	O
evaluation	O
shows	O
that	O
DR	O
-	O
BiLSTM	O
obtains	O
the	O
best	O
single	O
model	O
and	O
ensemble	O
model	O
results	O
achieving	O
the	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
scores	O
on	O
the	O
Stanford	O
NLI	S-RESEARCH_PROBLEM
dataset	O
.	O

End	O
-	O
to	O
-	O
end	O
neural	O
models	O
have	O
made	O
significant	O
progress	O
in	O
question	B-RESEARCH_PROBLEM
answering	E-RESEARCH_PROBLEM
,	O
however	O
recent	O
studies	O
show	O
that	O
these	O
models	O
implicitly	O
assume	O
that	O
the	O
answer	O
and	O
evidence	O
appear	O
close	O
together	O
in	O
a	O
single	O
document	O
.	O
In	O
this	O
work	O
,	O
we	O
propose	O
the	O
Coarse	O
-	O
grain	O
Fine	O
-	O
grain	O
Coattention	O
Network	O
(	O
CFC	O
)	O
,	O
a	O
new	O
question	B-RESEARCH_PROBLEM
answering	E-RESEARCH_PROBLEM
model	O
that	O
combines	O
information	O
from	O
evidence	O
across	O
multiple	O
documents	O
.	O
The	O
CFC	O
consists	O
of	O
a	O
coarse	O
-	O
grain	O
module	O
that	O
interprets	O
documents	O
with	O
respect	O
to	O
the	O
query	O
then	O
finds	O
a	O
relevant	O
answer	O
,	O
and	O
a	O
fine	O
-	O
grain	O
module	O
which	O
scores	O
each	O
candidate	O
answer	O
by	O
comparing	O
its	O
occurrences	O
across	O
all	O
of	O
the	O
documents	O
with	O
the	O
query	O
.	O
We	O
design	O
these	O
modules	O
using	O
hierarchies	O
of	O
coattention	O
and	O
selfattention	O
,	O
which	O
learn	O
to	O
emphasize	O
different	O
parts	O
of	O
the	O
input	O
.	O
On	O
the	O
Qangaroo	O
WikiHop	O
multi-evidence	O
question	B-RESEARCH_PROBLEM
answering	E-RESEARCH_PROBLEM
task	O
,	O
the	O
CFC	O
obtains	O
a	O
new	O
stateof	O
-	O
the	O
-	O
art	O
result	O
of	O
70.6	O
%	O
on	O
the	O
blind	O
test	O
set	O
,	O
outperforming	O
the	O
previous	O
best	O
by	O
3	O
%	O
accuracy	O
despite	O
not	O
using	O
pretrained	O
contextual	O
encoders	O
.	O
Figure	O
1	O
:	O
The	O
Coarse	O
-	O
grain	O
Fine-grain	O
Coattention	O
Network	O
.	O
outputs	O
from	O
a	O
traditional	O
span	O
extraction	O
model	O
(	O
Clark	O
&	O
Gardner	O
,	O
2018	O
)	O
using	O
the	O
CFC	O
improves	O
exact	O
match	O
accuracy	O
by	O
3.1	O
%	O
and	O
F1	O
by	O
3.0	O
%	O
.	O
Our	O
analysis	O
shows	O
that	O
components	O
in	O
the	O
attention	O
hierarchies	O
of	O
the	O
coarse	O
and	O
fine	O
-	O
grain	O
modules	O
learn	O
to	O
focus	O
on	O
distinct	O
parts	O
of	O
the	O
input	O
.	O

Automatic	B-RESEARCH_PROBLEM
story	I-RESEARCH_PROBLEM
comprehension	E-RESEARCH_PROBLEM
is	O
a	O
fundamental	O
challenge	O
in	O
Natural	O
Language	O
Understanding	O
,	O
and	O
can	O
enable	O
computers	O
to	O
learn	O
about	O
social	O
norms	O
,	O
human	O
behavior	O
and	O
commonsense	O
.	O
In	O
this	O
paper	O
,	O
we	O
present	O
a	O
story	O
comprehension	O
model	O
that	O
explores	O
three	O
distinct	O
semantic	O
aspects	O
:	O
(	O
i	O
)	O
the	O
sequence	O
of	O
events	O
described	O
in	O
the	O
story	O
,	O
(	O
ii	O
)	O
its	O
emotional	O
trajectory	O
,	O
and	O
(	O
iii	O
)	O
its	O
plot	O
consistency	O
.	O
We	O
judge	O
the	O
model	O
's	O
understanding	O
of	O
real	O
-	O
world	O
stories	O
by	O
inquiring	O
if	O
,	O
like	O
humans	O
,	O
it	O
can	O
develop	O
an	O
expectation	O
of	O
what	O
will	O
happen	O
next	O
in	O
a	O
given	O
story	O
.	O
Specifically	O
,	O
we	O
use	O
it	O
to	O
predict	O
the	O
correct	O
ending	O
of	O
a	O
given	O
short	O
story	O
from	O
possible	O
alternatives	O
.	O
The	O
model	O
uses	O
a	O
hidden	O
variable	O
to	O
weigh	O
the	O
semantic	O
aspects	O
in	O
the	O
context	O
of	O
the	O
story	O
.	O
Our	O
experiments	O
demonstrate	O
the	O
potential	O
of	O
our	O
approach	O
to	O
characterize	O
these	O
semantic	O
aspects	O
,	O
and	O
the	O
strength	O
of	O
the	O
hidden	O
variable	O
based	O
approach	O
.	O
The	O
model	O
outperforms	O
the	O
stateof	O
-	O
the	O
-	O
art	O
approaches	O
and	O
achieves	O
best	O
results	O
on	O
a	O
publicly	O
available	O
dataset	O
.	O

Reasoning	B-RESEARCH_PROBLEM
and	I-RESEARCH_PROBLEM
inference	E-RESEARCH_PROBLEM
are	O
central	O
to	O
human	O
and	O
artificial	O
intelligence	O
.	O
Modeling	O
inference	O
in	O
human	O
language	O
is	O
very	O
challenging	O
.	O
With	O
the	O
availability	O
of	O
large	O
annotated	O
data	O
(	O
Bowman	O
et	O
al.	O
,	O
2015	O
)	O
,	O
it	O
has	O
recently	O
become	O
feasible	O
to	O
train	O
neural	O
network	O
based	O
inference	O
models	O
,	O
which	O
have	O
shown	O
to	O
be	O
very	O
effective	O
.	O
In	O
this	O
paper	O
,	O
we	O
present	O
a	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
result	O
,	O
achieving	O
the	O
accuracy	O
of	O
88.6	O
%	O
on	O
the	O
Stanford	O
Natural	B-RESEARCH_PROBLEM
Language	I-RESEARCH_PROBLEM
Inference	E-RESEARCH_PROBLEM
Dataset	O
.	O
Unlike	O
the	O
previous	O
top	O
models	O
that	O
use	O
very	O
complicated	O
network	O
architectures	O
,	O
we	O
first	O
demonstrate	O
that	O
carefully	O
designing	O
sequential	O
inference	O
models	O
based	O
on	O
chain	O
LSTMs	O
can	O
outperform	O
all	O
previous	O
models	O
.	O
Based	O
on	O
this	O
,	O
we	O
further	O
show	O
that	O
by	O
explicitly	O
considering	O
recursive	O
architectures	O
in	O
both	O
local	O
inference	O
modeling	O
and	O
inference	O
composition	O
,	O
we	O
achieve	O
additional	O
improvement	O
.	O
Particularly	O
,	O
incorporating	O
syntactic	O
parsing	O
information	O
contributes	O
to	O
our	O
best	O
result	O
-	O
it	O
further	O
improves	O
the	O
performance	O
even	O
when	O
added	O
to	O
the	O
already	O
very	O
strong	O
model	O
.	O

Previous	O
machine	O
comprehension	O
(	O
MC	S-RESEARCH_PROBLEM
)	O
datasets	O
are	O
either	O
too	O
small	O
to	O
train	O
endto	O
-	O
end	O
deep	O
learning	O
models	O
,	O
or	O
not	O
difficult	O
enough	O
to	O
evaluate	O
the	O
ability	O
of	O
current	O
MC	S-RESEARCH_PROBLEM
techniques	O
.	O
The	O
newly	O
released	O
SQuAD	O
dataset	O
alleviates	O
these	O
limitations	O
,	O
and	O
gives	O
us	O
a	O
chance	O
to	O
develop	O
more	O
realistic	O
MC	S-RESEARCH_PROBLEM
models	O
.	O
Based	O
on	O
this	O
dataset	O
,	O
we	O
propose	O
a	O
Multi	O
-	O
Perspective	O
Context	O
Matching	O
(	O
MPCM	O
)	O
model	O
,	O
which	O
is	O
an	O
end	O
-	O
to	O
-	O
end	O
system	O
that	O
directly	O
predicts	O
the	O
answer	O
beginning	O
and	O
ending	O
points	O
in	O
a	O
passage	O
.	O
Our	O
model	O
first	O
adjusts	O
each	O
word	O
-	O
embedding	O
vector	O
in	O
the	O
passage	O
by	O
multiplying	O
a	O
relevancy	O
weight	O
computed	O
against	O
the	O
question	O
.	O
Then	O
,	O
we	O
encode	O
the	O
question	O
and	O
weighted	O
passage	O
by	O
using	O
bi-directional	O
LSTMs	O
.	O
For	O
each	O
point	O
in	O
the	O
passage	O
,	O
our	O
model	O
matches	O
the	O
context	O
of	O
this	O
point	O
against	O
the	O
encoded	O
question	O
from	O
multiple	O
perspectives	O
and	O
produces	O
a	O
matching	O
vector	O
.	O
Given	O
those	O
matched	O
vectors	O
,	O
we	O
employ	O
another	O
bi-directional	O
LSTM	O
to	O
aggregate	O
all	O
the	O
information	O
and	O
predict	O
the	O
beginning	O
and	O
ending	O
points	O
.	O

In	O
this	O
paper	O
,	O
we	O
study	O
the	O
problem	O
of	O
question	B-RESEARCH_PROBLEM
answering	I-RESEARCH_PROBLEM
when	I-RESEARCH_PROBLEM
reasoning	I-RESEARCH_PROBLEM
over	I-RESEARCH_PROBLEM
multiple	I-RESEARCH_PROBLEM
facts	E-RESEARCH_PROBLEM
is	O
required	O
.	O
We	O
propose	O
Query	O
-	O
Reduction	O
Network	O
(	O
QRN	O
)	O
,	O
a	O
variant	O
of	O
Recurrent	O
Neural	O
Network	O
(	O
RNN	O
)	O
that	O
effectively	O
handles	O
both	O
short	O
-	O
term	O
(	O
local	O
)	O
and	O
long	O
-	O
term	O
(	O
global	O
)	O
sequential	O
dependencies	O
to	O
reason	O
over	O
multiple	O
facts	O
.	O
QRN	O
considers	O
the	O
context	O
sentences	O
as	O
a	O
sequence	O
of	O
state	O
-	O
changing	O
triggers	O
,	O
and	O
reduces	O
the	O
original	O
query	O
to	O
a	O
more	O
informed	O
query	O
as	O
it	O
observes	O
each	O
trigger	O
(	O
context	O
sentence	O
)	O
through	O
time	O
.	O
Our	O
experiments	O
show	O
that	O
QRN	O
produces	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
in	O
bAbI	O
QA	O
and	O
dialog	O
tasks	O
,	O
and	O
in	O
are	O
al	O
goal	O
-	O
oriented	O
dialog	O
dataset	O
.	O
In	O
addition	O
,	O
QRN	O
formulation	O
allows	O
parallelization	O
on	O
RNN	O
's	O
time	O
axis	O
,	O
saving	O
an	O
order	O
of	O
magnitude	O
in	O
time	O
complexity	O
for	O
training	O
and	O
inference	O
.	O
INTRODUCTION	O
In	O
this	O
paper	O
,	O
we	O
address	O
the	O
problem	O
of	O
question	O
answering	O
(	O
QA	O
)	O
when	O
reasoning	O
over	O
multiple	O
facts	O
is	O
required	O
.	O

We	O
propose	O
a	O
simple	O
neural	O
architecture	O
for	O
natural	O
language	O
inference	O
.	O
Our	O
approach	O
uses	O
attention	O
to	O
decompose	O
the	O
problem	O
into	O
subproblems	O
that	O
can	O
be	O
solved	O
separately	O
,	O
thus	O
making	O
it	O
trivially	O
parallelizable	O
.	O
On	O
the	O
Stanford	O
Natural	B-RESEARCH_PROBLEM
Language	I-RESEARCH_PROBLEM
Inference	E-RESEARCH_PROBLEM
(	O
ST1	O
)	O
dataset	O
,	O
we	O
obtain	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
with	O
almost	O
an	O
order	O
of	O
magnitude	O
fewer	O
parameters	O
than	O
previous	O
work	O
and	O
without	O
relying	O
on	O
any	O
word	O
-	O
order	O
information	O
.	O
Adding	O
intra-sentence	O
attention	O
that	O
takes	O
a	O
minimum	O
amount	O
of	O
order	O
into	O
account	O
yields	O
further	O
improvements	O
.	O

Many	O
question	O
answering	O
(	O
QA	S-RESEARCH_PROBLEM
)	O
tasks	O
only	O
provide	O
weak	O
supervision	O
for	O
how	O
the	O
answer	O
should	O
be	O
computed	O
.	O
For	O
example	O
,	O
TRIVIAT0	O
answers	O
are	O
entities	O
that	O
can	O
be	O
mentioned	O
multiple	O
times	O
in	O
supporting	O
documents	O
,	O
while	O
DROP	O
answers	O
can	O
be	O
computed	O
by	O
deriving	O
many	O
different	O
equations	O
from	O
numbers	O
in	O
the	O
reference	O
text	O
.	O
In	O
this	O
paper	O
,	O
we	O
show	O
it	O
is	O
possible	O
to	O
convert	O
such	O
tasks	O
into	O
discrete	O
latent	O
variable	O
learning	O
problems	O
with	O
a	O
precomputed	O
,	O
task	O
-	O
specific	O
set	O
of	O
possible	O
solutions	O
(	O
e.g.	O
different	O
mentions	O
or	O
equations	O
)	O
that	O
contains	O
one	O
correct	O
option	O
.	O
We	O
then	O
develop	O
a	O
hard	O
EM	O
learning	O
scheme	O
that	O
computes	O
gradients	O
relative	O
to	O
the	O
most	O
likely	O
solution	O
at	O
each	O
update	O
.	O
Despite	O
its	O
simplicity	O
,	O
we	O
show	O
that	O
this	O
approach	O
significantly	O
outperforms	O
previous	O
methods	O
on	O
six	O
QA	S-RESEARCH_PROBLEM
tasks	O
,	O
including	O
absolute	O
gains	O
of	O
2	O
-	O
10	O
%	O
,	O
and	O
achieves	O
the	O
stateof	O
-	O
the	O
-	O
art	O
on	O
five	O
of	O
them	O
.	O
Using	O
hard	O
updates	O
instead	O
of	O
maximizing	O
marginal	O
likelihood	O
is	O
key	O
to	O
these	O
results	O
as	O
it	O
encourages	O
the	O
model	O
to	O
find	O
the	O
one	O
correct	O
answer	O
,	O
which	O
we	O
show	O
through	O
detailed	O
qualitative	O
analysis	O
.	O
1	O

In	O
this	O
paper	O
,	O
we	O
present	O
the	O
gated	O
selfmatching	O
networks	O
for	O
reading	B-RESEARCH_PROBLEM
comprehension	I-RESEARCH_PROBLEM
style	I-RESEARCH_PROBLEM
question	I-RESEARCH_PROBLEM
answering	E-RESEARCH_PROBLEM
,	O
which	O
aims	O
to	O
answer	O
questions	O
from	O
a	O
given	O
passage	O
.	O
We	O
first	O
match	O
the	O
question	O
and	O
passage	O
with	O
gated	O
attention	O
-	O
based	O
recurrent	O
networks	O
to	O
obtain	O
the	O
question	O
-	O
aware	O
passage	O
representation	O
.	O
Then	O
we	O
propose	O
a	O
self	O
-	O
matching	O
attention	O
mechanism	O
to	O
refine	O
the	O
representation	O
by	O
matching	O
the	O
passage	O
against	O
itself	O
,	O
which	O
effectively	O
encodes	O
information	O
from	O
the	O
whole	O
passage	O
.	O
We	O
finally	O
employ	O
the	O
pointer	O
networks	O
to	O
locate	O
the	O
positions	O
of	O
answers	O
from	O
the	O
passages	O
.	O
We	O
conduct	O
extensive	O
experiments	O
on	O
the	O
SQuAD	O
dataset	O
.	O
The	O
single	O
model	O
achieves	O
71.3	O
%	O
on	O
the	O
evaluation	O
metrics	O
of	O
exact	O
match	O
on	O
the	O
hidden	O
test	O
set	O
,	O
while	O
the	O
ensemble	O
model	O
further	O
boosts	O
the	O
results	O
to	O
75.9	O
%	O
.	O
At	O
the	O
time	O
of	O
submission	O
of	O
the	O
paper	O
,	O
our	O
model	O
holds	O
the	O
first	O
place	O
on	O
the	O
SQuAD	O
leaderboard	O
for	O
both	O
single	O
and	O
ensemble	O
model	O
.	O

Reading	B-RESEARCH_PROBLEM
comprehension	I-RESEARCH_PROBLEM
QA	E-RESEARCH_PROBLEM
tasks	O
have	O
seen	O
a	O
recent	O
surge	O
in	O
popularity	O
,	O
yet	O
most	O
works	O
have	O
focused	O
on	O
fact	O
-	O
finding	O
extractive	O
QA	O
.	O
We	O
instead	O
focus	O
on	O
a	O
more	O
challenging	O
multihop	O
generative	O
task	O
(	O
NarrativeQA	O
)	O
,	O
which	O
requires	O
the	O
model	O
to	O
reason	O
,	O
gather	O
,	O
and	O
synthesize	O
disjoint	O
pieces	O
of	O
information	O
within	O
the	O
context	O
to	O
generate	O
an	O
answer	O
.	O
This	O
type	O
of	O
multi-step	O
reasoning	O
also	O
often	O
requires	O
understanding	O
implicit	O
relations	O
,	O
which	O
humans	O
resolve	O
via	O
external	O
,	O
background	O
commonsense	O
knowledge	O
.	O
We	O
first	O
present	O
a	O
strong	O
generative	O
baseline	O
that	O
uses	O
a	O
multi-attention	O
mechanism	O
to	O
perform	O
multiple	O
hops	O
of	O
reasoning	O
and	O
a	O
pointer	O
-	O
generator	O
decoder	O
to	O
synthesize	O
the	O
answer	O
.	O
This	O
model	O
performs	O
substantially	O
better	O
than	O
previous	O
generative	O
models	O
,	O
and	O
is	O
competitive	O
with	O
current	O
state	O
-	O
of	O
-	O
theart	O
span	O
prediction	O
models	O
.	O
We	O
next	O
introduce	O
a	O
novel	O
system	O
for	O
selecting	O
grounded	O
multi-hop	O
relational	O
commonsense	O
information	O
from	O
Concept	O
Net	O
via	O
a	O
pointwise	O
mutual	O
information	O
and	O
term	O
-	O
frequency	O
based	O
scoring	O
function	O
.	O
Finally	O
,	O
we	O
effectively	O
use	O
this	O
extracted	O
commonsense	O
information	O
to	O
fill	O
in	O
gaps	O
of	O
reasoning	O
between	O
context	O
hops	O
,	O
using	O
a	O
selectively	O
-	O
gated	O
attention	O
mechanism	O
.	O

Machine	O
reading	O
comprehension	O
(	O
MRC	S-RESEARCH_PROBLEM
)	O
on	O
real	O
web	O
data	O
usually	O
requires	O
the	O
machine	O
to	O
answer	O
a	O
question	O
by	O
analyzing	O
multiple	O
passages	O
retrieved	O
by	O
search	O
engine	O
.	O
Compared	O
with	O
MRC	S-RESEARCH_PROBLEM
on	O
a	O
single	O
passage	O
,	O
multi-passage	O
MRC	S-RESEARCH_PROBLEM
is	O
more	O
challenging	O
,	O
since	O
we	O
are	O
likely	O
to	O
get	O
multiple	O
confusing	O
answer	O
candidates	O
from	O
different	O
passages	O
.	O
To	O
address	O
this	O
problem	O
,	O
we	O
propose	O
an	O
end	O
-	O
to	O
-	O
end	O
neural	O
model	O
that	O
enables	O
those	O
answer	O
candidates	O
from	O
different	O
passages	O
to	O
verify	O
each	O
other	O
based	O
on	O
their	O
content	O
representations	O
.	O
Specifically	O
,	O
we	O
jointly	O
train	O
three	O
modules	O
that	O
can	O
predict	O
the	O
final	O
answer	O
based	O
on	O
three	O
factors	O
:	O
the	O
answer	O
boundary	O
,	O
the	O
answer	O
content	O
and	O
the	O
cross	O
-	O
passage	O
answer	O
verification	O
.	O
The	O
experimental	O
results	O
show	O
that	O
our	O
method	O
outperforms	O
the	O
baseline	O
by	O
a	O
large	O
margin	O
and	O
achieves	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
the	O
English	O
MS	O
-	O
MARCO	O
dataset	O
and	O
the	O
Chinese	O
DuReader	O
dataset	O
,	O
both	O
of	O
which	O
are	O
designed	O
for	O
MRC	S-RESEARCH_PROBLEM
in	O
real	O
-	O
world	O
settings	O
.	O

Understanding	B-RESEARCH_PROBLEM
unstructured	I-RESEARCH_PROBLEM
text	E-RESEARCH_PROBLEM
is	O
a	O
major	O
goal	O
within	O
natural	O
language	O
processing	O
.	O
Comprehension	O
tests	O
pose	O
questions	O
based	O
on	O
short	O
text	O
passages	O
to	O
evaluate	O
such	O
understanding	O
.	O
In	O
this	O
work	O
,	O
we	O
investigate	O
machine	O
comprehension	O
on	O
the	O
challenging	O
MCTest	O
benchmark	O
.	O
Partly	O
because	O
of	O
its	O
limited	O
size	O
,	O
prior	O
work	O
on	O
MCTest	O
has	O
focused	O
mainly	O
on	O
engineering	O
better	O
features	O
.	O
We	O
tackle	O
the	O
dataset	O
with	O
a	O
neural	O
approach	O
,	O
harnessing	O
simple	O
neural	O
networks	O
arranged	O
in	O
a	O
parallel	O
hierarchy	O
.	O
The	O
parallel	O
hierarchy	O
enables	O
our	O
model	O
to	O
compare	O
the	O
passage	O
,	O
question	O
,	O
and	O
answer	O
from	O
a	O
variety	O
of	O
trainable	O
perspectives	O
,	O
as	O
opposed	O
to	O
using	O
a	O
manually	O
designed	O
,	O
rigid	O
feature	O
set	O
.	O
Perspectives	O
range	O
from	O
the	O
word	O
level	O
to	O
sentence	O
fragments	O
to	O
sequences	O
of	O
sentences	O
;	O
the	O
networks	O
operate	O
only	O
on	O
word	O
-	O
embedding	O
representations	O
of	O
text	O
.	O

The	O
RepEval	O
2017	O
Shared	O
Task	O
aims	O
to	O
evaluate	O
natural	O
language	O
understanding	O
models	O
for	O
sentence	O
representation	O
,	O
in	O
which	O
a	O
sentence	O
is	O
represented	O
as	O
a	O
fixedlength	O
vector	O
with	O
neural	O
networks	O
and	O
the	O
quality	O
of	O
the	O
representation	O
is	O
tested	O
with	O
a	O
natural	O
language	O
inference	O
task	O
.	O
This	O
paper	O
describes	O
our	O
system	O
(	O
alpha	O
)	O
that	O
is	O
ranked	O
among	O
the	O
top	O
in	O
the	O
Shared	O
Task	O
,	O
on	O
both	O
the	O
in	O
-	O
domain	O
test	O
set	O
(	O
obtaining	O
a	O
74.9	O
%	O
accuracy	O
)	O
and	O
on	O
the	O
crossdomain	O
test	O
set	O
(	O
also	O
attaining	O
a	O
74.9	O
%	O
accuracy	O
)	O
,	O
demonstrating	O
that	O
the	O
model	O
generalizes	O
well	O
to	O
the	O
cross	O
-domain	O
data	O
.	O
Our	O
model	O
is	O
equipped	O
with	O
intra-sentence	O
gated	O
-	O
attention	O
composition	O
which	O
helps	O
achieve	O
a	O
better	O
performance	O
.	O
In	O
addition	O
to	O
submitting	O
our	O
model	O
to	O
the	O
Shared	O
Task	O
,	O
we	O
have	O
also	O
tested	O
it	O
on	O
the	O
Stanford	O
Natural	B-RESEARCH_PROBLEM
Language	I-RESEARCH_PROBLEM
Inference	E-RESEARCH_PROBLEM
(	O
ST2	O
)	O
dataset	O
.	O
We	O
obtain	O
an	O
accuracy	O
of	O
85.5	O
%	O
,	O
which	O
is	O
the	O
best	O
reported	O
result	O
on	O
ST1	O
when	O
cross	O
-	O
sentence	O
attention	O
is	O
not	O
allowed	O
,	O
the	O
same	O
condition	O
enforced	O
in	O
RepEval	O
2017	O
.	O

Machine	O
Comprehension	O
(	O
MC	S-RESEARCH_PROBLEM
)	O
is	O
a	O
challenging	O
task	O
in	O
Natural	O
Language	O
Processing	O
field	O
,	O
which	O
aims	O
to	O
guide	O
the	O
machine	O
to	O
comprehend	O
a	O
passage	O
and	O
answer	O
the	O
given	O
question	O
.	O
Many	O
existing	O
approaches	O
on	O
MC	S-RESEARCH_PROBLEM
task	O
are	O
suffering	O
the	O
inefficiency	O
in	O
some	O
bottlenecks	O
,	O
such	O
as	O
insufficient	O
lexical	O
understanding	O
,	O
complex	O
question	O
-	O
passage	O
interaction	O
,	O
incorrect	O
answer	O
extraction	O
and	O
soon	O
.	O
In	O
this	O
paper	O
,	O
we	O
address	O
these	O
problems	O
from	O
the	O
viewpoint	O
of	O
how	O
humans	O
deal	O
with	O
reading	O
tests	O
in	O
a	O
scientific	O
way	O
.	O
Specifically	O
,	O
we	O
first	O
propose	O
a	O
novel	O
lexical	O
gating	O
mechanism	O
to	O
dynamically	O
combine	O
the	O
words	O
and	O
characters	O
representations	O
.	O
We	O
then	O
guide	O
the	O
machines	O
to	O
read	O
in	O
an	O
interactive	O
way	O
with	O
attention	O
mechanism	O
and	O
memory	O
network	O
.	O
Finally	O
we	O
add	O
a	O
checking	O
layer	O
to	O
refine	O
the	O
answer	O
for	O
insurance	O
.	O
The	O
extensive	O
experiments	O
on	O
two	O
popular	O
datasets	O
SQuAD	O
and	O
Trivia	O
QA	O
show	O
that	O
our	O
method	O
exceeds	O
considerable	O
performance	O
than	O
most	O
stateof	O
-	O
the	O
-	O
art	O
solutions	O
at	O
the	O
time	O
of	O
submission	O
.	O

Negation	O
is	O
an	O
important	O
characteristic	O
of	O
language	O
,	O
and	O
a	O
major	O
component	O
of	O
information	O
extraction	O
from	O
text	O
.	O
This	O
subtask	O
is	O
of	O
considerable	O
importance	O
to	O
the	O
biomedical	O
domain	O
.	O
Over	O
the	O
years	O
,	O
multiple	O
approaches	O
have	O
been	O
explored	O
to	O
address	O
this	O
problem	O
:	O
simple	O
rule	O
-	O
based	O
systems	O
,	O
Machine	O
Learning	O
classifiers	O
,	O
Conditional	O
Random	O
Field	O
Models	O
,	O
CNNs	O
and	O
more	O
recently	O
BiLSTMs	O
.	O
In	O
this	O
paper	O
,	O
we	O
look	O
at	O
applying	O
Transfer	O
Learning	O
to	O
this	O
problem	O
.	O
First	O
,	O
we	O
extensively	O
review	O
previous	O
literature	O
addressing	O
Negation	B-RESEARCH_PROBLEM
Detection	I-RESEARCH_PROBLEM
and	I-RESEARCH_PROBLEM
Scope	I-RESEARCH_PROBLEM
Resolution	E-RESEARCH_PROBLEM
across	O
the	O
3	O
datasets	O
that	O
have	O
gained	O
popularity	O
over	O
the	O
years	O
:	O
BioScope	O
Corpus	O
,	O
the	O
Sherlock	O
dataset	O
,	O
and	O
the	O
SFU	O
Review	O
Corpus	O
.	O
We	O
then	O
explore	O
the	O
decision	O
choices	O
involved	O
with	O
using	O
BERT	O
,	O
a	O
popular	O
transfer	O
learning	O
model	O
,	O
for	O
this	O
task	O
,	O
and	O
report	O
a	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
for	O
scope	O
resolution	O
across	O
all	O
3	O
datasets	O
.	O
Our	O
model	O
,	O
referred	O
to	O
as	O
NegBERT	O
,	O
achieves	O
a	O
token	O
level	O
F	O
1	O
score	O
on	O
scope	O
resolution	O
of	O
92.36	O
on	O
the	O
Sherlock	O
dataset	O
,	O
95.68	O
on	O
the	O
BioScope	O
Abstracts	O
,	O
91.24	O
on	O
the	O
BioScope	O
Full	O
Papers	O
,	O
90.95	O
on	O
the	O
SFU	O
dataset	O
,	O
outperforming	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
by	O
a	O
significant	O
margin	O
.	O

Paraphrase	O
generation	O
is	O
an	O
important	O
problem	O
in	O
NLP	O
,	O
especially	O
in	O
question	O
answering	O
,	O
information	O
retrieval	O
,	O
information	O
extraction	O
,	O
conversation	O
systems	O
,	O
to	O
name	O
a	O
few	O
.	O
In	O
this	O
paper	O
,	O
we	O
address	O
the	O
problem	O
of	O
generating	B-RESEARCH_PROBLEM
paraphrases	I-RESEARCH_PROBLEM
automatically	E-RESEARCH_PROBLEM
.	O
Our	O
proposed	O
method	O
is	O
based	O
on	O
a	O
combination	O
of	O
deep	O
generative	O
models	O
(	O
VAE	O
)	O
with	O
sequence	O
-	O
to	O
-	O
sequence	O
models	O
(	O
LSTM	O
)	O
to	O
generate	O
paraphrases	O
,	O
given	O
an	O
input	O
sentence	O
.	O
Traditional	O
VAEs	O
when	O
combined	O
with	O
recurrent	O
neural	O
networks	O
can	O
generate	O
free	O
text	O
but	O
they	O
are	O
not	O
suitable	O
for	O
paraphrase	O
generation	O
for	O
a	O
given	O
sentence	O
.	O
We	O
address	O
this	O
problem	O
by	O
conditioning	O
the	O
both	O
,	O
encoder	O
and	O
decoder	O
sides	O
of	O
VAE	O
,	O
on	O
the	O
original	O
sentence	O
,	O
so	O
that	O
it	O
can	O
generate	O
the	O
given	O
sentence	O
's	O
paraphrases	O
.	O
Unlike	O
most	O
existing	O
models	O
,	O
our	O
model	O
is	O
simple	O
,	O
modular	O
and	O
can	O
generate	O
multiple	O
paraphrases	O
,	O
for	O
a	O
given	O
sentence	O
.	O
Quantitative	O
evaluation	O
of	O
the	O
proposed	O
method	O
on	O
a	O
benchmark	O
paraphrase	O
dataset	O
demonstrates	O
its	O
efficacy	O
,	O
and	O
its	O
performance	O
improvement	O
over	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
by	O
a	O
significant	O
margin	O
,	O
whereas	O
qualitative	O
human	O
evaluation	O
indicate	O
that	O
the	O
generated	O
paraphrases	O
are	O
well	O
-	O
formed	O
,	O
grammatically	O
correct	O
,	O
and	O
are	O
relevant	O
to	O
the	O
input	O
sentence	O
.	O

Adversarial	O
training	O
(	O
AT	O
)	O
1	O
is	O
a	O
powerful	O
regularization	O
method	O
for	O
neural	O
networks	O
,	O
aiming	O
to	O
achieve	O
robustness	O
to	O
input	O
perturbations	O
.	O
Yet	O
,	O
the	O
specific	O
effects	O
of	O
the	O
robustness	O
obtained	O
from	O
AT	O
are	O
still	O
unclear	O
in	O
the	O
context	O
of	O
natural	O
language	O
processing	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
and	O
analyze	O
a	O
neural	B-RESEARCH_PROBLEM
POS	I-RESEARCH_PROBLEM
tagging	E-RESEARCH_PROBLEM
model	O
that	O
exploits	O
AT	O
.	O
In	O
our	O
experiments	O
on	O
the	O
Penn	O
Treebank	O
WSJ	O
corpus	O
and	O
the	O
Universal	O
Dependencies	O
(	O
UD	O
)	O
dataset	O
(	O
27	O
languages	O
)	O
,	O
we	O
find	O
that	O
AT	O
not	O
only	O
improves	O
the	O
over	O
all	O
tagging	O
accuracy	O
,	O
but	O
also	O
1	O
)	O
prevents	O
over	O
-	O
fitting	O
well	O
in	O
low	O
resource	O
languages	O
and	O
2	O
)	O
boosts	O
tagging	O
accuracy	O
for	O
rare	O
/	O
unseen	O
words	O
.	O
We	O
also	O
demonstrate	O
that	O
3	O
)	O
the	O
improved	O
tagging	O
performance	O
by	O
AT	O
contributes	O
to	O
the	O
downstream	O
task	O
of	O
dependency	O
parsing	O
,	O
and	O
that	O
4	O
)	O
AT	O
helps	O
the	O
model	O
to	O
learn	O
cleaner	O
word	O
representations	O
.	O
5	O
)	O
The	O
proposed	O
AT	O
model	O
is	O
generally	O
effective	O
in	O
different	O
sequence	O
labeling	O
tasks	O
.	O
These	O
positive	O
results	O
motivate	O
further	O
use	O
of	O
AT	O
for	O
natural	O
language	O
tasks	O
.	O

State	O
-	O
of	O
-	O
the	O
-	O
art	O
sequence	B-RESEARCH_PROBLEM
labeling	E-RESEARCH_PROBLEM
systems	O
traditionally	O
require	O
large	O
amounts	O
of	O
taskspecific	O
knowledge	O
in	O
the	O
form	O
of	O
handcrafted	O
features	O
and	O
data	O
pre-processing	O
.	O
In	O
this	O
paper	O
,	O
we	O
introduce	O
a	O
novel	O
neutral	O
network	O
architecture	O
that	O
benefits	O
from	O
both	O
word	O
-	O
and	O
character	O
-	O
level	O
representations	O
automatically	O
,	O
by	O
using	O
combination	O
of	O
bidirectional	O
LSTM	O
,	O
CNN	O
and	O
CRF	O
.	O
Our	O
system	O
is	O
truly	O
end	O
-	O
to	O
-	O
end	O
,	O
requiring	O
no	O
feature	O
engineering	O
or	O
data	O
preprocessing	O
,	O
thus	O
making	O
it	O
applicable	O
to	O
a	O
wide	O
range	O
of	O
sequence	B-RESEARCH_PROBLEM
labeling	E-RESEARCH_PROBLEM
tasks	O
.	O
We	O
evaluate	O
our	O
system	O
on	O
two	O
data	O
sets	O
for	O
two	O
sequence	B-RESEARCH_PROBLEM
labeling	E-RESEARCH_PROBLEM
tasks	O
-	O
Penn	O
Treebank	O
WSJ	O
corpus	O
for	O
part	O
-	O
of	O
-	O
speech	O
(	O
POS	O
)	O
tagging	O
and	O
CoNLL	O
2003	O
corpus	O
for	O
named	O
entity	O
recognition	O
(	O
NER	O
)	O
.	O
We	O
obtain	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
both	O
datasets	O
-	O
97.55	O
%	O
accuracy	O
for	O
POS	O
tagging	O
and	O
91.21	O
%	O
F1	O
for	O
NER	O
.	O

CRF	O
has	O
been	O
used	O
as	O
a	O
powerful	O
model	O
for	O
statistical	B-RESEARCH_PROBLEM
sequence	I-RESEARCH_PROBLEM
labeling	E-RESEARCH_PROBLEM
.	O
For	O
neural	O
sequence	O
labeling	O
,	O
however	O
,	O
BiLSTM	O
-	O
CRF	O
does	O
not	O
always	O
lead	O
to	O
better	O
results	O
compared	O
with	O
BiLSTM	O
-	O
softmax	O
local	O
classification	O
.	O
This	O
can	O
be	O
because	O
the	O
simple	O
Markov	O
label	O
transition	O
model	O
of	O
CRF	O
does	O
not	O
give	O
much	O
information	O
gain	O
over	O
strong	O
neural	O
encoding	O
.	O
For	O
better	O
representing	O
label	O
sequences	O
,	O
we	O
investigate	O
a	O
hierarchically	O
-	O
refined	O
label	O
attention	O
network	O
,	O
which	O
explicitly	O
leverages	O
label	O
embeddings	O
and	O
captures	O
potential	O
long	O
-	O
term	O
label	O
dependency	O
by	O
giving	O
each	O
word	O
incrementally	O
refined	O
label	O
distributions	O
with	O
hierarchical	O
attention	O
.	O
Results	O
on	O
POS	O
tagging	O
,	O
NER	O
and	O
CCG	O
supertagging	O
show	O
that	O
the	O
proposed	O
model	O
not	O
only	O
improves	O
the	O
over	O
all	O
tagging	O
accuracy	O
with	O
similar	O
number	O
of	O
parameters	O
,	O
but	O
also	O
significantly	O
speeds	O
up	O
the	O
training	O
and	O
testing	O
compared	O
to	O
BiLSTM	O
-	O
CRF	O
.	O

Bidirectional	O
long	O
short	O
-	O
term	O
memory	O
(	O
bi	O
-	O
LSTM	O
)	O
networks	O
have	O
recently	O
proven	O
successful	O
for	O
various	O
NLP	O
sequence	O
modeling	O
tasks	O
,	O
but	O
little	O
is	O
known	O
about	O
their	O
reliance	O
to	O
input	O
representations	O
,	O
target	O
languages	O
,	O
data	O
set	O
size	O
,	O
and	O
label	O
noise	O
.	O
We	O
address	O
these	O
issues	O
and	O
evaluate	O
bi	O
-	O
LSTMs	O
with	O
word	O
,	O
character	O
,	O
and	O
unicode	O
byte	O
embeddings	O
for	O
POS	B-RESEARCH_PROBLEM
tagging	E-RESEARCH_PROBLEM
.	O
We	O
compare	O
bi	O
-	O
LSTMs	O
to	O
traditional	O
POS	O
taggers	O
across	O
languages	O
and	O
data	O
sizes	O
.	O
We	O
also	O
present	O
a	O
novel	O
bi	O
-	O
LSTM	O
model	O
,	O
which	O
combines	O
the	O
POS	B-RESEARCH_PROBLEM
tagging	E-RESEARCH_PROBLEM
loss	O
function	O
with	O
an	O
auxiliary	O
loss	O
function	O
that	O
accounts	O
for	O
rare	O
words	O
.	O
The	O
model	O
obtains	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
across	O
22	O
languages	O
,	O
and	O
works	O
especially	O
well	O
for	O
morphologically	O
complex	O
languages	O
.	O
Our	O
analysis	O
suggests	O
that	O
bi	O
-	O
LSTMs	O
are	O
less	O
sensitive	O
to	O
training	O
data	O
size	O
and	O
label	O
corruptions	O
(	O
at	O
small	O
noise	O
levels	O
)	O
than	O
previously	O
assumed	O
.	O

Recently	O
,	O
neural	O
models	O
pretrained	O
on	O
a	O
language	O
modeling	O
task	O
,	O
such	O
as	O
ELMo	O
(	O
Peters	O
et	O
al.	O
,	O
2017	O
)	O
,	O
OpenAI	O
GPT	O
(	O
Radford	O
et	O
al.	O
,	O
2018	O
)	O
,	O
and	O
BERT	O
(	O
Devlin	O
et	O
al.	O
,	O
2018	O
)	O
,	O
have	O
achieved	O
impressive	O
results	O
on	O
various	O
natural	O
language	O
processing	O
tasks	O
such	O
as	O
question	O
-	O
answering	O
and	O
natural	O
language	O
inference	O
.	O
In	O
this	O
paper	O
,	O
we	O
describe	O
a	O
simple	O
re-implementation	O
of	O
BERT	O
for	O
query	B-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
based	I-RESEARCH_PROBLEM
passage	I-RESEARCH_PROBLEM
re-ranking	E-RESEARCH_PROBLEM
.	O
Our	O
system	O
is	O
the	O
state	O
of	O
the	O
art	O
on	O
the	O
TREC	O
-	O
CAR	O
dataset	O
and	O
the	O
top	O
entry	O
in	O
the	O
leaderboard	O
of	O
the	O
MS	O
MARCO	O
passage	O
retrieval	O
task	O
,	O
outperforming	O
the	O
previous	O
state	O
of	O
the	O
art	O
by	O
27	O
%	O
(	O
relative	O
)	O
in	O
MRR@	O
10	O
.	O
The	O
code	O
to	O
reproduce	O
our	O
results	O
is	O
available	O
at	O
https://github.com/nyu-dl/	O
dl4marco-bert	O
INTRODUCTION	O
We	O
have	O
seen	O
rapid	O
progress	O
in	O
machine	O
reading	O
compression	O
in	O
recent	O
years	O
with	O
the	O
introduction	O
of	O
large	O
-	O
scale	O
datasets	O
,	O
such	O
as	O
SQuAD	O
,	O
MS	O
MARCO	O
,	O
SearchQA	O
,	O
TriviaQA	O
,	O
and	O
QUASAR	O
-	O
T	O
,	O
and	O
the	O
broad	O
adoption	O
of	O
neural	O
models	O
,	O
such	O
as	O
BiDAF	O
,	O
DrQA	O
,	O
DocumentQA	O
,	O
and	O
QAnet	O
.	O

We	O
address	O
the	O
problem	O
of	O
phrase	B-RESEARCH_PROBLEM
grounding	E-RESEARCH_PROBLEM
by	O
learning	O
a	O
multi	O
-	O
level	O
common	O
semantic	O
space	O
shared	O
by	O
the	O
textual	O
and	O
visual	O
modalities	O
.	O
We	O
exploit	O
multiple	O
levels	O
of	O
feature	O
maps	O
of	O
a	O
Deep	O
Convolutional	O
Neural	O
Network	O
,	O
as	O
well	O
as	O
contextualized	O
word	O
and	O
sentence	O
embeddings	O
extracted	O
from	O
a	O
character	O
-	O
based	O
language	O
model	O
.	O
Following	O
dedicated	O
non-linear	O
mappings	O
for	O
visual	O
features	O
at	O
each	O
level	O
,	O
word	O
,	O
and	O
sentence	O
embeddings	O
,	O
we	O
obtain	O
multiple	O
instantiations	O
of	O
our	O
common	O
semantic	O
space	O
in	O
which	O
comparisons	O
between	O
any	O
target	O
text	O
and	O
the	O
visual	O
content	O
is	O
performed	O
with	O
cosine	O
similarity	O
.	O
We	O
guide	O
the	O
model	O
by	O
a	O
multi	O
-	O
level	O
multimodal	O
attention	O
mechanism	O
which	O
outputs	O
attended	O
visual	O
features	O
at	O
each	O
level	O
.	O
The	O
best	O
level	O
is	O
chosen	O
to	O
be	O
compared	O
with	O
text	O
content	O
for	O
maximizing	O
the	O
pertinence	O
scores	O
of	O
image	O
-	O
sentence	O
pairs	O
of	O
the	O
ground	O
truth	O
.	O
Experiments	O
conducted	O
on	O
three	O
publicly	O
available	O
datasets	O
show	O
significant	O
performance	O
gains	O
(	O
20	O
%	O
-	O
60	O
%	O
relative	O
)	O
over	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
in	O
phrase	O
localization	O
and	O
set	O
a	O
new	O
performance	O
record	O
on	O
those	O
datasets	O
.	O
We	O
provide	O
a	O
detailed	O
ablation	O
study	O
to	O
show	O
the	O
contribution	O
of	O
each	O
element	O
of	O
our	O
approach	O
and	O
release	O
our	O
code	O
on	O
GitHub	O
1	O
.	O

In	O
this	O
paper	O
we	O
introduce	O
a	O
new	O
natural	O
language	O
processing	O
dataset	O
and	O
benchmark	O
for	O
predicting	B-RESEARCH_PROBLEM
prosodic	I-RESEARCH_PROBLEM
prominence	I-RESEARCH_PROBLEM
from	I-RESEARCH_PROBLEM
written	I-RESEARCH_PROBLEM
text	E-RESEARCH_PROBLEM
.	O
To	O
our	O
knowledge	O
this	O
will	O
be	O
the	O
largest	O
publicly	O
available	O
dataset	O
with	O
prosodic	O
labels	O
.	O
We	O
describe	O
the	O
dataset	O
construction	O
and	O
the	O
resulting	O
benchmark	O
dataset	O
in	O
detail	O
and	O
train	O
a	O
number	O
of	O
different	O
models	O
ranging	O
from	O
feature	O
-	O
based	O
classifiers	O
to	O
neural	O
network	O
systems	O
for	O
the	O
prediction	O
of	O
discretized	O
prosodic	O
prominence	O
.	O
We	O
show	O
that	O
pre-trained	O
contextualized	O
word	O
representations	O
from	O
BERT	O
outperform	O
the	O
other	O
models	O
even	O
with	O
less	O
than	O
10	O
%	O
of	O
the	O
training	O
data	O
.	O
Finally	O
we	O
discuss	O
the	O
dataset	O
in	O
light	O
of	O
the	O
results	O
and	O
point	O
to	O
future	O
research	O
and	O
plans	O
for	O
further	O
improving	O
both	O
the	O
dataset	O
and	O
methods	O
of	O
predicting	O
prosodic	O
prominence	O
from	O
text	O
.	O
The	O
dataset	O
and	O
the	O
code	O
for	O
the	O
models	O
are	O
publicly	O
available	O
.	O

The	O
most	O
approaches	O
to	O
Knowledge	B-RESEARCH_PROBLEM
Base	I-RESEARCH_PROBLEM
Question	I-RESEARCH_PROBLEM
Answering	E-RESEARCH_PROBLEM
are	O
based	O
on	O
semantic	O
parsing	O
.	O
In	O
this	O
paper	O
,	O
we	O
address	O
the	O
problem	O
of	O
learning	O
vector	O
representations	O
for	O
complex	O
semantic	O
parses	O
that	O
consist	O
of	O
multiple	O
entities	O
and	O
relations	O
.	O
Previous	O
work	O
largely	O
focused	O
on	O
selecting	O
the	O
correct	O
semantic	O
relations	O
for	O
a	O
question	O
and	O
disregarded	O
the	O
structure	O
of	O
the	O
semantic	O
parse	O
:	O
the	O
connections	O
between	O
entities	O
and	O
the	O
directions	O
of	O
the	O
relations	O
.	O
We	O
propose	O
to	O
use	O
Gated	O
Graph	O
Neural	O
Networks	O
to	O
encode	O
the	O
graph	O
structure	O
of	O
the	O
semantic	O
parse	O
.	O
We	O
show	O
on	O
two	O
data	O
sets	O
that	O
the	O
graph	O
networks	O
outperform	O
all	O
baseline	O
models	O
that	O
do	O
not	O
explicitly	O
model	O
the	O
structure	O
.	O
The	O
error	O
analysis	O
confirms	O
that	O
our	O
approach	O
can	O
successfully	O
process	O
complex	O
semantic	O
parses	O
.	O

Machine	O
comprehension	O
(	O
MC	S-RESEARCH_PROBLEM
)	O
,	O
answering	O
a	O
query	O
about	O
a	O
given	O
context	O
paragraph	O
,	O
requires	O
modeling	O
complex	O
interactions	O
between	O
the	O
context	O
and	O
the	O
query	O
.	O
Recently	O
,	O
attention	O
mechanisms	O
have	O
been	O
successfully	O
extended	O
to	O
MC	S-RESEARCH_PROBLEM
.	O
Typically	O
these	O
methods	O
use	O
attention	O
to	O
focus	O
on	O
a	O
small	O
portion	O
of	O
the	O
context	O
and	O
summarize	O
it	O
with	O
a	O
fixed	O
-	O
size	O
vector	O
,	O
couple	O
attentions	O
temporally	O
,	O
and	O
/	O
or	O
often	O
form	O
a	O
uni-directional	O
attention	O
.	O
In	O
this	O
paper	O
we	O
introduce	O
the	O
Bi-	O
Directional	O
Attention	O
Flow	O
(	O
BIDAF	O
)	O
network	O
,	O
a	O
multi-stage	O
hierarchical	O
process	O
that	O
represents	O
the	O
context	O
at	O
different	O
levels	O
of	O
granularity	O
and	O
uses	O
bidirectional	O
attention	O
flow	O
mechanism	O
to	O
obtain	O
a	O
query	O
-	O
aware	O
context	O
representation	O
without	O
early	O
summarization	O
.	O
Our	O
experimental	O
evaluations	O
show	O
that	O
our	O
model	O
achieves	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
in	O
Stanford	O
Question	O
Answering	O
Dataset	O
(	O
SQuAD	O
)	O
and	O
CNN	O
/	O
DailyMail	O
cloze	O
test	O
.	O
INTRODUCTION	O
The	O
tasks	O
of	O
machine	O
comprehension	O
(	O
MC	S-RESEARCH_PROBLEM
)	O
and	O
question	O
answering	O
(	O
QA	O
)	O
have	O
gained	O
significant	O
popularity	O
over	O
the	O
past	O
few	O
years	O
within	O
the	O
natural	O
language	O
processing	O
and	O
computer	O
vision	O
communities	O
.	O

Sequence	O
encoders	O
are	O
crucial	O
components	O
in	O
many	O
neural	O
architectures	O
for	O
learning	O
to	O
read	O
and	O
comprehend	O
.	O
This	O
paper	O
presents	O
a	O
new	O
compositional	O
encoder	O
for	O
reading	O
comprehension	O
(	O
RC	S-RESEARCH_PROBLEM
)	O
.	O
Our	O
proposed	O
encoder	O
is	O
not	O
only	O
aimed	O
at	O
being	O
fast	O
but	O
also	O
expressive	O
.	O
Specifically	O
,	O
the	O
key	O
novelty	O
behind	O
our	O
encoder	O
is	O
that	O
it	O
explicitly	O
models	O
across	O
multiple	O
granularities	O
using	O
a	O
new	O
dilated	O
composition	O
mechanism	O
.	O
In	O
our	O
approach	O
,	O
gating	O
functions	O
are	O
learned	O
by	O
modeling	O
relationships	O
and	O
reasoning	O
over	O
multi-granular	O
sequence	O
information	O
,	O
enabling	O
compositional	O
learning	O
that	O
is	O
aware	O
of	O
both	O
long	O
and	O
short	O
term	O
information	O
.	O
We	O
conduct	O
experiments	O
on	O
three	O
RC	S-RESEARCH_PROBLEM
datasets	O
,	O
showing	O
that	O
our	O
proposed	O
encoder	O
demonstrates	O
very	O
promising	O
results	O
both	O
as	O
a	O
standalone	O
encoder	O
as	O
well	O
as	O
a	O
complementary	O
building	O
block	O
.	O
Empirical	O
results	O
show	O
that	O
simple	O
Bi-Attentive	O
architectures	O
augmented	O
with	O
our	O
proposed	O
encoder	O
not	O
only	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
/	O
highly	O
competitive	O
results	O
but	O
is	O
also	O
considerably	O
faster	O
than	O
other	O
published	O
works	O
.	O

We	O
propose	O
DECAPROP	O
(	O
Densely	O
Connected	O
Attention	O
Propagation	O
)	O
,	O
a	O
new	O
densely	O
connected	O
neural	O
architecture	O
for	O
reading	O
comprehension	O
(	O
RC	S-RESEARCH_PROBLEM
)	O
.	O
There	O
are	O
two	O
distinct	O
characteristics	O
of	O
our	O
model	O
.	O
Firstly	O
,	O
our	O
model	O
densely	O
connects	O
all	O
pairwise	O
layers	O
of	O
the	O
network	O
,	O
modeling	O
relationships	O
between	O
passage	O
and	O
query	O
across	O
all	O
hierarchical	O
levels	O
.	O
Secondly	O
,	O
the	O
dense	O
connectors	O
in	O
our	O
network	O
are	O
learned	O
via	O
attention	O
instead	O
of	O
standard	O
residual	O
skip	O
-	O
connectors	O
.	O
To	O
this	O
end	O
,	O
we	O
propose	O
novel	O
Bidirectional	O
Attention	O
Connectors	O
(	O
BAC	O
)	O
for	O
efficiently	O
forging	O
connections	O
throughout	O
the	O
network	O
.	O
We	O
conduct	O
extensive	O
experiments	O
on	O
four	O
challenging	O
RC	S-RESEARCH_PROBLEM
benchmarks	O
.	O
Our	O
proposed	O
approach	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
all	O
four	O
,	O
outperforming	O
existing	O
baselines	O
by	O
up	O
to	O
2.6	O
%	O
?	O

A	O
popular	O
recent	O
approach	O
to	O
answering	O
open	O
-	O
domain	O
questions	O
is	O
to	O
first	O
search	O
for	O
question	O
-	O
related	O
passages	O
and	O
then	O
apply	O
reading	O
comprehension	O
models	O
to	O
extract	O
answers	O
.	O
Existing	O
methods	O
usually	O
extract	O
answers	O
from	O
single	O
passages	O
independently	O
.	O
But	O
some	O
questions	O
require	O
a	O
combination	O
of	O
evidence	O
from	O
across	O
different	O
sources	O
to	O
answer	O
correctly	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
two	O
models	O
which	O
make	O
use	O
of	O
multiple	O
passages	O
to	O
generate	O
their	O
answers	O
.	O
Both	O
use	O
an	O
answerreranking	O
approach	O
which	O
reorders	O
the	O
answer	O
candidates	O
generated	O
by	O
an	O
existing	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
QA	O
model	O
.	O
We	O
propose	O
two	O
methods	O
,	O
namely	O
,	O
strengthbased	O
re-ranking	O
and	O
coverage	O
-	O
based	O
re-ranking	O
,	O
to	O
make	O
use	O
of	O
the	O
aggregated	O
evidence	O
from	O
different	O
passages	O
to	O
better	O
determine	O
the	O
answer	O
.	O
Our	O
models	O
have	O
achieved	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
three	O
public	O
open	B-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
domain	I-RESEARCH_PROBLEM
QA	E-RESEARCH_PROBLEM
datasets	O
:	O

Automatic	B-RESEARCH_PROBLEM
question	I-RESEARCH_PROBLEM
generation	E-RESEARCH_PROBLEM
aims	O
to	O
generate	O
questions	O
from	O
a	O
text	O
passage	O
where	O
the	O
generated	O
questions	O
can	O
be	O
answered	O
by	O
certain	O
sub-	O
spans	O
of	O
the	O
given	O
passage	O
.	O
Traditional	O
methods	O
mainly	O
use	O
rigid	O
heuristic	O
rules	O
to	O
transform	O
a	O
sentence	O
into	O
related	O
questions	O
.	O
In	O
this	O
work	O
,	O
we	O
propose	O
to	O
apply	O
the	O
neural	O
encoderdecoder	O
model	O
to	O
generate	O
meaningful	O
and	O
diverse	O
questions	O
from	O
natural	O
language	O
sentences	O
.	O
The	O
encoder	O
reads	O
the	O
input	O
text	O
and	O
the	O
answer	O
position	O
,	O
to	O
produce	O
an	O
answer	O
-	O
aware	O
input	O
representation	O
,	O
which	O
is	O
fed	O
to	O
the	O
decoder	O
to	O
generate	O
an	O
answer	O
focused	O
question	O
.	O
We	O
conduct	O
a	O
preliminary	O
study	O
on	O
neural	O
question	B-RESEARCH_PROBLEM
generation	E-RESEARCH_PROBLEM
from	O
text	O
with	O
the	O
SQuAD	O
dataset	O
,	O
and	O
the	O
experiment	O
results	O
show	O
that	O
our	O
method	O
can	O
produce	O
fluent	O
and	O
diverse	O
questions	O
.	O

Generating	B-RESEARCH_PROBLEM
natural	I-RESEARCH_PROBLEM
questions	I-RESEARCH_PROBLEM
from	I-RESEARCH_PROBLEM
an	I-RESEARCH_PROBLEM
image	E-RESEARCH_PROBLEM
is	O
a	O
semantic	O
task	O
that	O
requires	O
using	O
visual	O
and	O
language	O
modality	O
to	O
learn	O
multimodal	O
representations	O
.	O
Images	O
can	O
have	O
multiple	O
visual	O
and	O
language	O
contexts	O
thatare	O
relevant	O
for	O
generating	O
questions	O
namely	O
places	O
,	O
captions	O
,	O
and	O
tags	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
the	O
use	O
of	O
exemplars	O
for	O
obtaining	O
the	O
relevant	O
context	O
.	O
We	O
obtain	O
this	O
by	O
using	O
a	O
Multimodal	O
Differential	O
Network	O
to	O
produce	O
natural	O
and	O
engaging	O
questions	O
.	O
The	O
generated	O
questions	O
show	O
a	O
remarkable	O
similarity	O
to	O
the	O
natural	O
questions	O
as	O
validated	O
by	O
a	O
human	O
study	O
.	O
Further	O
,	O
we	O
observe	O
that	O
the	O
proposed	O
approach	O
substantially	O
improves	O
over	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
benchmarks	O
on	O
the	O
quantitative	O
metrics	O
(	O
BLEU	O
,	O
METEOR	O
,	O
ROUGE	O
,	O
and	O
CIDEr	O
)	O
.	O

In	O
this	O
paper	O
,	O
we	O
describe	O
our	O
team	O
's	O
effort	O
on	O
the	O
semantic	B-RESEARCH_PROBLEM
text	I-RESEARCH_PROBLEM
question	I-RESEARCH_PROBLEM
similarity	E-RESEARCH_PROBLEM
task	O
of	O
NSURL	O
2019	O
.	O
Our	O
top	O
performing	O
system	O
utilizes	O
several	O
innovative	O
data	O
augmentation	O
techniques	O
to	O
enlarge	O
the	O
training	O
data	O
.	O
Then	O
,	O
it	O
takes	O
ELMo	O
pre-trained	O
contextual	O
embeddings	O
of	O
the	O
data	O
and	O
feeds	O
them	O
into	O
an	O
ON	O
-	O
LSTM	O
network	O
with	O
self	O
-	O
attention	O
.	O
This	O
results	O
in	O
sequence	O
representation	O
vectors	O
that	O
are	O
used	O
to	O
predict	O
the	O
relation	O
between	O
the	O
question	O
pairs	O
.	O
The	O
model	O
is	O
ranked	O
in	O
the	O
1st	O
place	O
with	O
96.499	O
F1score	O
(	O
same	O
as	O
the	O
second	O
place	O
F1-score	O
)	O
and	O
the	O
2nd	O
place	O
with	O
94.848	O
F1-	O
score	O
(	O
differs	O
by	O
1.076	O
F1-score	O
from	O
the	O
first	O
place	O
)	O
on	O
the	O
public	O
and	O
private	O
leaderboards	O
,	O
respectively	O
.	O

Relation	B-RESEARCH_PROBLEM
Extraction	E-RESEARCH_PROBLEM
is	O
the	O
task	O
of	O
identifying	O
entity	O
mention	O
spans	O
in	O
raw	O
text	O
and	O
then	O
identifying	O
relations	O
between	O
pairs	O
of	O
the	O
entity	O
mentions	O
.	O
Recent	O
approaches	O
for	O
this	O
spanlevel	O
task	O
have	O
been	O
token	O
-	O
level	O
models	O
which	O
have	O
inherent	O
limitations	O
.	O
They	O
can	O
not	O
easily	O
define	O
and	O
implement	O
span	O
-	O
level	O
features	O
,	O
can	O
not	O
model	O
overlapping	O
entity	O
mentions	O
and	O
have	O
cascading	O
errors	O
due	O
to	O
the	O
use	O
of	O
sequential	O
decoding	O
.	O
To	O
address	O
these	O
concerns	O
,	O
we	O
present	O
a	O
model	O
which	O
directly	O
models	O
all	O
possible	O
spans	O
and	O
performs	O
joint	O
entity	O
mention	O
detection	O
and	O
relation	O
extraction	O
.	O
We	O
report	O
a	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
of	O
62.83	O
F	O
1	O
(	O
prev	O
best	O
was	O
60.49	O
)	O
on	O
the	O
ACE2005	O
dataset	O
.	O

Distantly	O
-	O
supervised	O
Relation	O
Extraction	O
(	O
RE	S-RESEARCH_PROBLEM
)	O
methods	O
train	O
an	O
extractor	O
by	O
automatically	O
aligning	O
relation	O
instances	O
in	O
a	O
Knowledge	O
Base	O
(	O
KB	O
)	O
with	O
unstructured	O
text	O
.	O
In	O
addition	O
to	O
relation	O
instances	O
,	O
KBs	O
often	O
contain	O
other	O
relevant	O
side	O
information	O
,	O
such	O
as	O
aliases	O
of	O
relations	O
(	O
e.g.	O
,	O
founded	O
and	O
co-founded	O
are	O
aliases	O
for	O
the	O
relation	O
founder	O
OfCompany	O
)	O
.	O
RE	S-RESEARCH_PROBLEM
models	O
usually	O
ignore	O
such	O
readily	O
available	O
side	O
information	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
,	O
a	O
distantly	O
-	O
supervised	O
neural	O
relation	O
extraction	O
method	O
which	O
utilizes	O
additional	O
side	O
information	O
from	O
KBs	O
for	O
improved	O
relation	O
extraction	O
.	O
It	O
uses	O
entity	O
type	O
and	O
relation	O
alias	O
information	O
for	O
imposing	O
soft	O
constraints	O
while	O
predicting	O
relations	O
.	O
RE	S-RESEARCH_PROBLEM
-	O
SIDE	O
employs	O
Graph	O
Convolution	O
Networks	O
(	O
GCN	O
)	O
to	O
encode	O
syntactic	O
information	O
from	O
text	O
and	O
improves	O
performance	O
even	O
when	O
limited	O
side	O
information	O
is	O
available	O
.	O
Through	O
extensive	O
experiments	O
on	O
benchmark	O
datasets	O
,	O
we	O
demonstrate	O
's	O
effectiveness	O
.	O

General	O
purpose	O
relation	O
extractors	O
,	O
which	O
can	O
model	O
arbitrary	O
relations	O
,	O
are	O
a	O
core	O
aspiration	O
in	O
information	O
extraction	O
.	O
Efforts	O
have	O
been	O
made	O
to	O
build	O
general	O
purpose	O
extractors	O
that	O
represent	O
relations	O
with	O
their	O
surface	O
forms	O
,	O
or	O
which	O
jointly	O
embed	O
surface	O
forms	O
with	O
relations	O
from	O
an	O
existing	O
knowledge	O
graph	O
.	O
However	O
,	O
both	O
of	O
these	O
approaches	O
are	O
limited	O
in	O
their	O
ability	O
to	O
generalize	O
.	O
In	O
this	O
paper	O
,	O
we	O
build	O
on	O
extensions	O
of	O
Harris	O
'	O
distributional	O
hypothesis	O
to	O
relations	O
,	O
as	O
well	O
as	O
recent	O
advances	O
in	O
learning	O
text	O
representations	O
(	O
specifically	O
,	O
BERT	O
)	O
,	O
to	O
build	O
task	O
agnostic	O
relation	O
representations	O
solely	O
from	O
entity	O
-	O
linked	O
text	O
.	O
We	O
show	O
that	O
these	O
representations	O
significantly	O
outperform	O
previous	O
work	O
on	O
exemplar	O
based	O
relation	B-RESEARCH_PROBLEM
extraction	E-RESEARCH_PROBLEM
(	O
FewRel	O
)	O
even	O
without	O
using	O
any	O
of	O
that	O
task	O
's	O
training	O
data	O
.	O
We	O
also	O
show	O
that	O
models	O
initialized	O
with	O
our	O
task	O
agnostic	O
representations	O
,	O
and	O
then	O
tuned	O
on	O
supervised	O
relation	B-RESEARCH_PROBLEM
extraction	E-RESEARCH_PROBLEM
datasets	O
,	O
significantly	O
outperform	O
the	O
previous	O
methods	O
on	O
Se	O
-	O
m	O
Eval	O
2010	O
Task	O
8	O
,	O
KBP37	O
,	O
and	O
TACRED	O
.	O

The	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
solutions	O
for	O
extracting	B-RESEARCH_PROBLEM
multiple	I-RESEARCH_PROBLEM
entity	I-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
relations	E-RESEARCH_PROBLEM
from	O
an	O
input	O
paragraph	O
always	O
require	O
a	O
multiple	O
-	O
pass	O
encoding	O
on	O
the	O
input	O
.	O
This	O
paper	O
proposes	O
a	O
new	O
solution	O
that	O
can	O
complete	O
the	O
multiple	B-RESEARCH_PROBLEM
entityrelations	I-RESEARCH_PROBLEM
extraction	E-RESEARCH_PROBLEM
task	O
with	O
only	O
one	O
-	O
pass	O
encoding	O
on	O
the	O
input	O
corpus	O
,	O
and	O
achieve	O
a	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
accuracy	O
performance	O
,	O
as	O
demonstrated	O
in	O
the	O
ACE	O
2005	O
benchmark	O
.	O
Our	O
solution	O
is	O
built	O
on	O
top	O
of	O
the	O
pre-trained	O
self	O
-	O
attentive	O
models	O
(	O
Transformer	O
)	O
.	O
Since	O
our	O
method	O
uses	O
a	O
single	O
-	O
pass	O
to	O
compute	O
all	O
relations	O
at	O
once	O
,	O
it	O
scales	O
to	O
larger	O
datasets	O
easily	O
;	O
which	O
makes	O
it	O
more	O
usable	O
in	O
real	O
-	O
world	O
applications	O
.	O
1	O

Organized	O
relational	O
knowledge	O
in	O
the	O
form	O
of	O
"	O
knowledge	O
graphs	O
"	O
is	O
important	O
for	O
many	O
applications	O
.	O
However	O
,	O
the	O
ability	O
to	O
populate	B-RESEARCH_PROBLEM
knowledge	I-RESEARCH_PROBLEM
bases	I-RESEARCH_PROBLEM
with	I-RESEARCH_PROBLEM
facts	E-RESEARCH_PROBLEM
automatically	O
extracted	O
from	O
documents	O
has	O
improved	O
frustratingly	O
slowly	O
.	O
This	O
paper	O
simultaneously	O
addresses	O
two	O
issues	O
that	O
have	O
held	O
back	O
prior	O
work	O
.	O
We	O
first	O
propose	O
an	O
effective	O
new	O
model	O
,	O
which	O
combines	O
an	O
LSTM	O
sequence	O
model	O
with	O
a	O
form	O
of	O
entity	O
position	O
-	O
aware	O
attention	O
that	O
is	O
better	O
suited	O
to	O
relation	B-RESEARCH_PROBLEM
extraction	E-RESEARCH_PROBLEM
.	O
Then	O
we	O
build	O
TACRED	O
,	O
a	O
large	O
(	O
119,474	O
examples	O
)	O
supervised	O
relation	B-RESEARCH_PROBLEM
extraction	E-RESEARCH_PROBLEM
dataset	O
,	O
obtained	O
via	O
crowdsourcing	O
and	O
targeted	O
towards	O
TAC	O
KBP	O
relations	O
.	O
The	O
combination	O
of	O
better	O
supervised	O
data	O
and	O
a	O
more	O
appropriate	O
high	O
-	O
capacity	O
model	O
enables	O
much	O
better	O
relation	B-RESEARCH_PROBLEM
extraction	E-RESEARCH_PROBLEM
performance	O
.	O
When	O
the	O
model	O
trained	O
on	O
this	O
new	O
dataset	O
replaces	O
the	O
previous	O
relation	B-RESEARCH_PROBLEM
extraction	E-RESEARCH_PROBLEM
component	O
of	O
the	O
best	O
TAC	O
KBP	O
2015	O
slot	O
filling	O
system	O
,	O
its	O
F	O
1	O
score	O
increases	O
markedly	O
from	O
22.2	O
%	O
to	O
26.7	O
%	O
.	O

We	O
demonstrate	O
that	O
for	O
sentence	B-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
level	I-RESEARCH_PROBLEM
relation	I-RESEARCH_PROBLEM
extraction	E-RESEARCH_PROBLEM
it	O
is	O
beneficial	O
to	O
consider	O
other	O
relations	O
in	O
the	O
sentential	O
context	O
while	O
predicting	O
the	O
target	O
relation	O
.	O
Our	O
architecture	O
uses	O
an	O
LSTM	O
-	O
based	O
encoder	O
to	O
jointly	O
learn	O
representations	O
for	O
all	O
relations	O
in	O
a	O
single	O
sentence	O
.	O
We	O
combine	O
the	O
context	O
representations	O
with	O
an	O
attention	O
mechanism	O
to	O
make	O
the	O
final	O
prediction	O
.	O
We	O
use	O
the	O
Wikidata	O
knowledge	O
base	O
to	O
construct	O
a	O
dataset	O
of	O
multiple	O
relations	O
per	O
sentence	O
and	O
to	O
evaluate	O
our	O
approach	O
.	O
Compared	O
to	O
a	O
baseline	O
system	O
,	O
our	O
method	O
results	O
in	O
an	O
average	O
error	O
reduction	O
of	O
24	O
%	O
on	O
a	O
held	O
-	O
out	O
set	O
of	O
relations	O
.	O
The	O
code	O
and	O
the	O
dataset	O
to	O
replicate	O
the	O
experiments	O
are	O
made	O
available	O
at	O
https://github.com/ukplab.	O
1	O
Unique	O
IDs	O
in	O
Wikidata	O
have	O
a	O
Q-prefix	O
for	O
entities	O
and	O
a	O
P-prefix	O
for	O
relations	O
.	O

Two	O
problems	O
arise	O
when	O
using	O
distant	O
supervision	O
for	O
relation	B-RESEARCH_PROBLEM
extraction	E-RESEARCH_PROBLEM
.	O
First	O
,	O
in	O
this	O
method	O
,	O
an	O
already	O
existing	O
knowledge	O
base	O
is	O
heuristically	O
aligned	O
to	O
texts	O
,	O
and	O
the	O
alignment	O
results	O
are	O
treated	O
as	O
labeled	O
data	O
.	O
However	O
,	O
the	O
heuristic	O
alignment	O
can	O
fail	O
,	O
resulting	O
in	O
wrong	O
label	O
problem	O
.	O
In	O
addition	O
,	O
in	O
previous	O
approaches	O
,	O
statistical	O
models	O
have	O
typically	O
been	O
applied	O
toad	O
hoc	O
features	O
.	O
The	O
noise	O
that	O
originates	O
from	O
the	O
feature	O
extraction	O
process	O
can	O
cause	O
poor	O
performance	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
novel	O
model	O
dubbed	O
the	O
Piecewise	O
Convolutional	O
Neural	O
Networks	O
(	O
PCNNs	O
)	O
with	O
multi-instance	O
learning	O
to	O
address	O
these	O
two	O
problems	O
.	O
To	O
solve	O
the	O
first	O
problem	O
,	O
distant	B-RESEARCH_PROBLEM
supervised	I-RESEARCH_PROBLEM
relation	I-RESEARCH_PROBLEM
extraction	E-RESEARCH_PROBLEM
is	O
treated	O
as	O
a	O
multi-instance	O
problem	O
in	O
which	O
the	O
uncertainty	O
of	O
instance	O
labels	O
is	O
taken	O
into	O
account	O
.	O

We	O
introduce	O
the	O
Self	O
-	O
Annotated	O
Reddit	O
Corpus	O
(	O
SARC	O
)	O
,	O
a	O
large	O
corpus	O
for	O
sarcasm	O
research	O
and	O
for	O
training	O
and	O
evaluating	O
systems	O
for	O
sarcasm	B-RESEARCH_PROBLEM
detection	E-RESEARCH_PROBLEM
.	O
The	O
corpus	O
has	O
1.3	O
million	O
sarcastic	O
statements	O
-	O
10	O
times	O
more	O
than	O
any	O
previous	O
dataset	O
-	O
and	O
many	O
times	O
more	O
instances	O
of	O
non-sarcastic	O
statements	O
,	O
allowing	O
for	O
learning	O
in	O
both	O
balanced	O
and	O
unbalanced	O
label	O
regimes	O
.	O
Each	O
statement	O
is	O
furthermore	O
self	O
-	O
annotated	O
-	O
sarcasm	O
is	O
labeled	O
by	O
the	O
author	O
,	O
not	O
an	O
independent	O
annotator	O
-	O
and	O
provided	O
with	O
user	O
,	O
topic	O
,	O
and	O
conversation	O
context	O
.	O
We	O
evaluate	O
the	O
corpus	O
for	O
accuracy	O
,	O
construct	O
benchmarks	O
for	O
sarcasm	B-RESEARCH_PROBLEM
detection	E-RESEARCH_PROBLEM
,	O
and	O
evaluate	O
baseline	O
methods	O
.	O

The	O
literature	O
in	O
automated	B-RESEARCH_PROBLEM
sarcasm	I-RESEARCH_PROBLEM
detection	E-RESEARCH_PROBLEM
has	O
mainly	O
focused	O
on	O
lexical	O
,	O
syntactic	O
and	O
semantic	O
-	O
level	O
analysis	O
of	O
text	O
.	O
However	O
,	O
a	O
sarcastic	O
sentence	O
can	O
be	O
expressed	O
with	O
contextual	O
presumptions	O
,	O
background	O
and	O
commonsense	O
knowledge	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
CASCADE	O
(	O
a	O
ContextuAl	O
SarCasm	O
DEtector	O
)	O
that	O
adopts	O
a	O
hybrid	O
approach	O
of	O
both	O
content	O
and	O
context	O
-	O
driven	O
modeling	O
for	O
sarcasm	B-RESEARCH_PROBLEM
detection	E-RESEARCH_PROBLEM
in	O
online	O
social	O
media	O
discussions	O
.	O
For	O
the	O
latter	O
,	O
CASCADE	O
aims	O
at	O
extracting	O
contextual	O
information	O
from	O
the	O
discourse	O
of	O
a	O
discussion	O
thread	O
.	O
Also	O
,	O
since	O
the	O
sarcastic	O
nature	O
and	O
form	O
of	O
expression	O
can	O
vary	O
from	O
person	O
to	O
person	O
,	O
CASCADE	O
utilizes	O
user	O
embeddings	O
that	O
encode	O
stylometric	O
and	O
personality	O
features	O
of	O
the	O
users	O
.	O
When	O
used	O
along	O
with	O
content	O
-	O
based	O
feature	O
extractors	O
such	O
as	O
Convolutional	O
Neural	O
Networks	O
(	O
CNNs	O
)	O
,	O
we	O
see	O
a	O
significant	O
boost	O
in	O
the	O
classification	O
performance	O
on	O
a	O
large	O
Reddit	O
corpus	O
.	O

Semantic	B-RESEARCH_PROBLEM
parsing	E-RESEARCH_PROBLEM
aims	O
at	O
mapping	O
natural	O
language	O
utterances	O
into	O
structured	O
meaning	O
representations	O
.	O
In	O
this	O
work	O
,	O
we	O
propose	O
a	O
structure	O
-	O
aware	O
neural	O
architecture	O
which	O
decomposes	O
the	O
semantic	O
parsing	O
process	O
into	O
two	O
stages	O
.	O
Given	O
an	O
input	O
utterance	O
,	O
we	O
first	O
generate	O
a	O
rough	O
sketch	O
of	O
its	O
meaning	O
,	O
where	O
low	O
-	O
level	O
information	O
(	O
such	O
as	O
variable	O
names	O
and	O
arguments	O
)	O
is	O
glossed	O
over	O
.	O
Then	O
,	O
we	O
fill	O
in	O
missing	O
details	O
by	O
taking	O
into	O
account	O
the	O
natural	O
language	O
input	O
and	O
the	O
sketch	O
itself	O
.	O
Experimental	O
results	O
on	O
four	O
datasets	O
characteristic	O
of	O
different	O
domains	O
and	O
meaning	O
representations	O
show	O
that	O
our	O
approach	O
consistently	O
improves	O
performance	O
,	O
achieving	O
competitive	O
results	O
despite	O
the	O
use	O
of	O
relatively	O
simple	O
decoders	O
.	O

Recent	O
BIO	O
-	O
tagging	O
-	O
based	O
neural	O
semantic	O
role	O
labeling	O
models	O
are	O
very	O
high	O
performing	O
,	O
but	O
assume	O
gold	O
predicates	O
as	O
part	O
of	O
the	O
input	O
and	O
can	O
not	O
incorporate	O
span	O
-	O
level	O
features	O
.	O
We	O
propose	O
an	O
endto	O
-	O
end	O
approach	O
for	O
jointly	O
predicting	O
all	O
predicates	O
,	O
arguments	O
spans	O
,	O
and	O
the	O
relations	O
between	O
them	O
.	O
The	O
model	O
makes	O
independent	O
decisions	O
about	O
what	O
relationship	O
,	O
if	O
any	O
,	O
holds	O
between	O
every	O
possible	O
word	O
-	O
span	O
pair	O
,	O
and	O
learns	O
contextualized	O
span	O
representations	O
that	O
provide	O
rich	O
,	O
shared	O
input	O
features	O
for	O
each	O
decision	O
.	O
Experiments	O
demonstrate	O
that	O
this	O
approach	O
sets	O
a	O
new	O
state	O
of	O
the	O
art	O
on	O
PropBank	O
SRL	S-RESEARCH_PROBLEM
without	O
gold	O
predicates	O
.	O
1	O

Current	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
semantic	O
role	O
labeling	O
(	O
SRL	S-RESEARCH_PROBLEM
)	O
uses	O
a	O
deep	O
neural	O
network	O
with	O
no	O
explicit	O
linguistic	O
features	O
.	O
However	O
,	O
prior	O
work	O
has	O
shown	O
that	O
gold	O
syntax	O
trees	O
can	O
dramatically	O
improve	O
SRL	S-RESEARCH_PROBLEM
decoding	O
,	O
suggesting	O
the	O
possibility	O
of	O
increased	O
accuracy	O
from	O
explicit	O
modeling	O
of	O
syntax	O
.	O
In	O
this	O
work	O
,	O
we	O
present	O
linguistically	O
-	O
informed	O
self	O
-	O
attention	O
(	O
LISA	O
)	O
:	O
a	O
neural	O
network	O
model	O
that	O
combines	O
multi-head	O
self	O
-	O
attention	O
with	O
multi-task	O
learning	O
across	O
dependency	O
parsing	O
,	O
part	O
-	O
ofspeech	O
tagging	O
,	O
predicate	O
detection	O
and	O
SRL	S-RESEARCH_PROBLEM
.	O
Unlike	O
previous	O
models	O
which	O
require	O
significant	O
pre-processing	O
to	O
prepare	O
linguistic	O
features	O
,	O
LISA	O
can	O
incorporate	O
syntax	O
using	O
merely	O
raw	O
tokens	O
as	O
input	O
,	O
encoding	O
the	O
sequence	O
only	O
once	O
to	O
simultaneously	O
perform	O
parsing	O
,	O
predicate	O
detection	O
and	O
role	O
labeling	O
for	O
all	O
predicates	O
.	O
Syntax	O
is	O
incorporated	O
by	O
training	O
one	O
attention	O
head	O
to	O
attend	O
to	O
syntactic	O
parents	O
for	O
each	O
token	O
.	O
Moreover	O
,	O
if	O
a	O
high	O
-	O
quality	O
syntactic	O
parse	O
is	O
already	O
available	O
,	O
it	O
can	O
be	O
beneficially	O
injected	O
at	O
test	O
time	O
without	O
re-training	O
our	O
SRL	S-RESEARCH_PROBLEM
model	O
.	O
In	O
experiments	O
on	O
CoNLL	O
-	O
2005	O
SRL	S-RESEARCH_PROBLEM
,	O
LISA	O
achieves	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
for	O
a	O
model	O
using	O
predicted	O
predicates	O
and	O
standard	O
word	O
embeddings	O
,	O
attaining	O
2.5	O
F1	O
absolute	O
higher	O
than	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
on	O
newswire	O
and	O
more	O
than	O
3.5	O
F1	O
on	O
outof	O
-	O
domain	O
data	O
,	O
nearly	O
10	O
%	O
reduction	O
in	O
error	O
.	O

We	O
introduce	O
a	O
new	O
deep	O
learning	O
model	O
for	O
semantic	O
role	O
labeling	O
(	O
SRL	S-RESEARCH_PROBLEM
)	O
that	O
significantly	O
improves	O
the	O
state	O
of	O
the	O
art	O
,	O
along	O
with	O
detailed	O
analyses	O
to	O
reveal	O
its	O
strengths	O
and	O
limitations	O
.	O
We	O
use	O
a	O
deep	O
highway	O
BiLSTM	O
architecture	O
with	O
constrained	O
decoding	O
,	O
while	O
observing	O
a	O
number	O
of	O
recent	O
best	O
practices	O
for	O
initialization	O
and	O
regularization	O
.	O
Our	O
8	O
-	O
layer	O
ensemble	O
model	O
achieves	O
83.2	O
F1	O
on	O
the	O
CoNLL	O
2005	O
test	O
set	O
and	O
83.4	O
F1	O
on	O
CoNLL	O
2012	O
,	O
roughly	O
a	O
10	O
%	O
relative	O
error	O
reduction	O
over	O
the	O
previous	O
state	O
of	O
the	O
art	O
.	O
Extensive	O
empirical	O
analysis	O
of	O
these	O
gains	O
show	O
that	O
(	O
1	O
)	O
deep	O
models	O
excel	O
at	O
recovering	O
long	O
-	O
distance	O
dependencies	O
but	O
can	O
still	O
make	O
surprisingly	O
obvious	O
errors	O
,	O
and	O
(	O
2	O
)	O
that	O
there	O
is	O
still	O
room	O
for	O
syntactic	O
parsers	O
to	O
improve	O
these	O
results	O
.	O

Semantic	B-RESEARCH_PROBLEM
Role	I-RESEARCH_PROBLEM
Labeling	E-RESEARCH_PROBLEM
(	O
SRL	S-RESEARCH_PROBLEM
)	O
is	O
believed	O
to	O
be	O
a	O
crucial	O
step	O
towards	O
natural	O
language	O
understanding	O
and	O
has	O
been	O
widely	O
studied	O
.	O
Recent	O
years	O
,	O
end	O
-	O
to	O
-	O
end	O
SRL	S-RESEARCH_PROBLEM
with	O
recurrent	O
neural	O
networks	O
(	O
RNN	O
)	O
has	O
gained	O
increasing	O
attention	O
.	O
However	O
,	O
it	O
remains	O
a	O
major	O
challenge	O
for	O
RNNs	O
to	O
handle	O
structural	O
information	O
and	O
long	O
range	O
dependencies	O
.	O
In	O
this	O
paper	O
,	O
we	O
present	O
a	O
simple	O
and	O
effective	O
architecture	O
for	O
SRL	S-RESEARCH_PROBLEM
which	O
aims	O
to	O
address	O
these	O
problems	O
.	O
Our	O
model	O
is	O
based	O
on	O
self	O
-	O
attention	O
which	O
can	O
directly	O
capture	O
the	O
relationships	O
between	O
two	O
tokens	O
regardless	O
of	O
their	O
distance	O
.	O
Our	O
single	O
model	O
achieves	O
F1	O
=	O
83.4	O
on	O
the	O
CoNLL	O
-	O
2005	O
shared	O
task	O
dataset	O
and	O
F1	O
=	O
82.7	O
on	O
the	O
CoNLL	O
-	O
2012	O
shared	O
task	O
dataset	O
,	O
which	O
outperforms	O
the	O
previous	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
by	O
1.8	O
and	O
1.0	O
F1	O
score	O
respectively	O
.	O
Besides	O
,	O
our	O
model	O
is	O
computationally	O
efficient	O
,	O
and	O
the	O
parsing	O
speed	O
is	O
50	O
K	O
tokens	O
per	O
second	O
on	O
a	O
single	O
Titan	O
X	O
GPU	O
.	O

We	O
present	O
a	O
simple	O
and	O
accurate	O
span	O
-	O
based	O
model	O
for	O
semantic	O
role	O
labeling	O
(	O
SRL	S-RESEARCH_PROBLEM
)	O
.	O
Our	O
model	O
directly	O
takes	O
into	O
account	O
all	O
possible	O
argument	O
spans	O
and	O
scores	O
them	O
for	O
each	O
label	O
.	O
At	O
decoding	O
time	O
,	O
we	O
greedily	O
select	O
higher	O
scoring	O
labeled	O
spans	O
.	O
One	O
advantage	O
of	O
our	O
model	O
is	O
to	O
allow	O
us	O
to	O
design	O
and	O
use	O
spanlevel	O
features	O
,	O
thatare	O
difficult	O
to	O
use	O
in	O
tokenbased	O
BIO	O
tagging	O
approaches	O
.	O
Experimental	O
results	O
demonstrate	O
that	O
our	O
ensemble	O
model	O
achieves	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
,	O
87.4	O
F1	O
and	O
87.0	O
F1	O
on	O
the	O
CoNLL	O
-	O
2005	O
and	O
2012	O
datasets	O
,	O
respectively	O
.	O

Prevalent	O
models	O
based	O
on	O
artificial	O
neural	O
network	O
(	O
ANN	O
)	O
for	O
sentence	O
classification	O
often	O
classify	O
sentences	O
in	O
isolation	O
without	O
considering	O
the	O
context	O
in	O
which	O
sentences	O
appear	O
.	O
This	O
hampers	O
the	O
traditional	O
sentence	O
classification	O
approaches	O
to	O
the	O
problem	O
of	O
sequential	B-RESEARCH_PROBLEM
sentence	I-RESEARCH_PROBLEM
classification	E-RESEARCH_PROBLEM
,	O
where	O
structured	O
prediction	O
is	O
needed	O
for	O
better	O
over	O
all	O
classification	O
performance	O
.	O
In	O
this	O
work	O
,	O
we	O
present	O
a	O
hierarchical	O
sequential	O
labeling	O
network	O
to	O
make	O
use	O
of	O
the	O
contextual	O
information	O
within	O
surrounding	O
sentences	O
to	O
help	O
classify	O
the	O
current	O
sentence	O
.	O
Our	O
model	O
outperforms	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
by	O
2	O
%	O
-	O
3	O
%	O
on	O
two	O
benchmarking	O
datasets	O
for	O
sequential	B-RESEARCH_PROBLEM
sentence	I-RESEARCH_PROBLEM
classification	E-RESEARCH_PROBLEM
in	O
medical	O
scientific	O
abstracts	O
.	O

We	O
present	O
an	O
LSTM	O
approach	O
to	O
deletion	B-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
based	I-RESEARCH_PROBLEM
sentence	I-RESEARCH_PROBLEM
compression	E-RESEARCH_PROBLEM
where	O
the	O
task	O
is	O
to	O
translate	O
a	O
sentence	O
into	O
a	O
sequence	O
of	O
zeros	O
and	O
ones	O
,	O
corresponding	O
to	O
token	O
deletion	O
decisions	O
.	O
We	O
demonstrate	O
that	O
even	O
the	O
most	O
basic	O
version	O
of	O
the	O
system	O
,	O
which	O
is	O
given	O
no	O
syntactic	O
information	O
(	O
no	O
PoS	O
or	O
NE	O
tags	O
,	O
or	O
dependencies	O
)	O
or	O
desired	O
compression	O
length	O
,	O
performs	O
surprisingly	O
well	O
:	O
around	O
30	O
%	O
of	O
the	O
compressions	O
from	O
a	O
large	O
test	O
set	O
could	O
be	O
regenerated	O
.	O
We	O
compare	O
the	O
LSTM	O
system	O
with	O
a	O
competitive	O
baseline	O
which	O
is	O
trained	O
on	O
the	O
same	O
amount	O
of	O
data	O
but	O
is	O
additionally	O
provided	O
with	O
all	O
kinds	O
of	O
linguistic	O
features	O
.	O
In	O
an	O
experiment	O
with	O
human	O
raters	O
the	O
LSTMbased	O
model	O
outperforms	O
the	O
baseline	O
achieving	O
4.5	O
in	O
readability	O
and	O
3.8	O
in	O
informativeness	O
.	O

We	O
show	O
how	O
eye	O
-	O
tracking	O
corpora	O
can	O
be	O
used	O
to	O
improve	O
sentence	B-RESEARCH_PROBLEM
compression	E-RESEARCH_PROBLEM
models	O
,	O
presenting	O
a	O
novel	O
multi-task	O
learning	O
algorithm	O
based	O
on	O
multi	O
-	O
layer	O
LSTMs	O
.	O
We	O
obtain	O
performance	O
competitive	O
with	O
or	O
better	O
than	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
approaches	O
.	O
Readers	O
fixate	O
longer	O
at	O
rare	O
words	O
,	O
words	O
that	O
are	O
semantically	O
ambiguous	O
,	O
and	O
words	O
that	O
are	O
mor	O
-	O

We	O
herein	O
present	O
a	O
language	O
-	O
modelbased	O
evaluator	O
for	O
deletion	B-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
based	I-RESEARCH_PROBLEM
sentence	I-RESEARCH_PROBLEM
compression	E-RESEARCH_PROBLEM
,	O
and	O
viewed	O
this	O
task	O
as	O
a	O
series	O
of	O
deletion	O
-	O
and	O
-	O
evaluation	O
operations	O
using	O
the	O
evaluator	O
.	O
More	O
specifically	O
,	O
the	O
evaluator	O
is	O
a	O
syntactic	O
neural	O
language	O
model	O
that	O
is	O
first	O
built	O
by	O
learning	O
the	O
syntactic	O
and	O
structural	O
collocation	O
among	O
words	O
.	O
Subsequently	O
,	O
a	O
series	O
of	O
trial	O
-	O
and	O
-	O
error	O
deletion	O
operations	O
are	O
conducted	O
on	O
the	O
source	O
sentences	O
via	O
a	O
reinforcement	O
learning	O
framework	O
to	O
obtain	O
the	O
best	O
target	O
compression	O
.	O
An	O
empirical	O
study	O
shows	O
that	O
the	O
proposed	O
model	O
can	O
effectively	O
generate	O
more	O
readable	O
compression	O
,	O
comparable	O
or	O
superior	O
to	O
several	O
strong	O
baselines	O
.	O
Furthermore	O
,	O
we	O
introduce	O
a	O
200	O
-	O
sentence	O
test	O
set	O
for	O
a	O
largescale	O
dataset	O
,	O
setting	O
a	O
new	O
baseline	O
for	O
the	O
future	O
research	O
.	O

Emotion	B-RESEARCH_PROBLEM
recognition	I-RESEARCH_PROBLEM
in	I-RESEARCH_PROBLEM
conversations	E-RESEARCH_PROBLEM
is	O
crucial	O
for	O
the	O
development	O
of	O
empathetic	O
machines	O
.	O
Present	O
methods	O
mostly	O
ignore	O
the	O
role	O
of	O
inter-speaker	O
dependency	O
relations	O
while	O
classifying	O
emotions	O
in	O
conversations	O
.	O
In	O
this	O
paper	O
,	O
we	O
address	O
recognizing	O
utterance	O
-	O
level	O
emotions	O
in	O
dyadic	O
conversational	O
videos	O
.	O
We	O
propose	O
a	O
deep	O
neural	O
framework	O
,	O
termed	O
conversational	O
memory	O
network	O
,	O
which	O
leverages	O
contextual	O
information	O
from	O
the	O
conversation	O
history	O
.	O
The	O
framework	O
takes	O
a	O
multimodal	O
approach	O
comprising	O
audio	O
,	O
visual	O
and	O
textual	O
features	O
with	O
gated	O
recurrent	O
units	O
to	O
model	O
past	O
utterances	O
of	O
each	O
speaker	O
into	O
memories	O
.	O
Such	O
memories	O
are	O
then	O
merged	O
using	O
attention	O
-	O
based	O
hops	O
to	O
capture	O
inter-speaker	O
dependencies	O
.	O
Experiments	O
show	O
an	O
accuracy	O
improvement	O
of	O
3?4	O
%	O
over	O
the	O
state	O
of	O
the	O
art	O
.	O

In	O
aspect	O
-	O
level	O
sentiment	O
classification	O
(	O
ASC	S-RESEARCH_PROBLEM
)	O
,	O
it	O
is	O
prevalent	O
to	O
equip	O
dominant	O
neural	O
models	O
with	O
attention	O
mechanisms	O
,	O
for	O
the	O
sake	O
of	O
acquiring	O
the	O
importance	O
of	O
each	O
context	O
word	O
on	O
the	O
given	O
aspect	O
.	O
However	O
,	O
such	O
a	O
mechanism	O
tends	O
to	O
excessively	O
focus	O
on	O
a	O
few	O
frequent	O
words	O
with	O
sentiment	O
polarities	O
,	O
while	O
ignoring	O
infrequent	O
ones	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
progressive	O
self	O
-	O
supervised	O
attention	O
learning	O
approach	O
for	O
neural	B-RESEARCH_PROBLEM
ASC	E-RESEARCH_PROBLEM
models	O
,	O
which	O
automatically	O
mines	O
useful	O
attention	O
supervision	O
information	O
from	O
a	O
training	O
corpus	O
to	O
refine	O
attention	O
mechanisms	O
.	O
Specifically	O
,	O
we	O
iteratively	O
conduct	O
sentiment	O
predictions	O
on	O
all	O
training	O
instances	O
.	O
Particularly	O
,	O
at	O
each	O
iteration	O
,	O
the	O
context	O
word	O
with	O
the	O
maximum	O
attention	O
weight	O
is	O
extracted	O
as	O
the	O
one	O
with	O
active	O
/	O
misleading	O
influence	O
on	O
the	O
correct	O
/	O
incorrect	O
prediction	O
of	O
every	O
instance	O
,	O
and	O
then	O
the	O
word	O
itself	O
is	O
masked	O
for	O
subsequent	O
iterations	O
.	O
Finally	O
,	O
we	O
augment	O
the	O
conventional	O
training	O
objective	O
with	O
a	O
regularization	O
term	O
,	O
which	O
enables	O
ASC	S-RESEARCH_PROBLEM
models	O
to	O
continue	O
equally	O
focusing	O
on	O
the	O
extracted	O
active	O
context	O
words	O
while	O
decreasing	O
weights	O
of	O
those	O
misleading	O
ones	O
.	O
Experimental	O
results	O
on	O
multiple	O
datasets	O
show	O
that	O
our	O
proposed	O
approach	O
yields	O
better	O
attention	O
mechanisms	O
,	O
leading	O
to	O
substantial	O
improvements	O
over	O
the	O
two	O
stateof	O
-	O
the	O
-	O
art	O
neural	B-RESEARCH_PROBLEM
ASC	E-RESEARCH_PROBLEM
models	O
.	O

Question	O
-	O
answering	O
plays	O
an	O
important	O
role	O
in	O
e-commerce	O
as	O
it	O
allows	O
potential	O
customers	O
to	O
actively	O
seek	O
crucial	O
information	O
about	O
products	O
or	O
services	O
to	O
help	O
their	O
purchase	O
decision	O
making	O
.	O
Inspired	O
by	O
the	O
recent	O
success	O
of	O
machine	O
reading	O
comprehension	O
(	O
MRC	O
)	O
on	O
formal	O
documents	O
,	O
this	O
paper	O
explores	O
the	O
potential	O
of	O
turning	O
customer	O
reviews	O
into	O
a	O
large	O
source	O
of	O
knowledge	O
that	O
can	O
be	O
exploited	O
to	O
answer	O
user	O
questions	O
.	O
We	O
call	O
this	O
problem	O
Review	O
Reading	O
Comprehension	O
(	O
RRC	O
)	O
.	O
To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
no	O
existing	O
work	O
has	O
been	O
done	O
on	O
RRC	O
.	O
In	O
this	O
work	O
,	O
we	O
first	O
build	O
an	O
RRC	O
dataset	O
called	O
ReviewRC	O
based	O
on	O
a	O
popular	O
benchmark	O
for	O
aspectbased	O
sentiment	O
analysis	O
.	O
Since	O
ReviewRC	O
has	O
limited	O
training	O
examples	O
for	O
RRC	O
(	O
and	O
also	O
for	O
aspect	O
-	O
based	O
sentiment	O
analysis	O
)	O
,	O
we	O
then	O
explore	O
a	O
novel	O
post	O
-	O
training	O
approach	O
on	O
the	O
popular	O
language	O
model	O
BERT	O
to	O
enhance	O
the	O
performance	O
of	O
fine	O
-	O
tuning	O
of	O
BERT	O
for	O
RRC	O
.	O
To	O
show	O
the	O
generality	O
of	O
the	O
approach	O
,	O
the	O
proposed	O
post	O
-	O
training	O
is	O
also	O
applied	O
to	O
some	O
other	O
review	O
-	O
based	O
tasks	O
such	O
as	O
aspect	B-RESEARCH_PROBLEM
extraction	E-RESEARCH_PROBLEM
and	O
aspect	B-RESEARCH_PROBLEM
sentiment	I-RESEARCH_PROBLEM
classification	E-RESEARCH_PROBLEM
in	O
aspect	O
-	O
based	O
sentiment	O
analysis	O
.	O

Semantic	O
word	O
spaces	O
have	O
been	O
very	O
useful	O
but	O
can	O
not	O
express	O
the	O
meaning	O
of	O
longer	O
phrases	O
in	O
a	O
principled	O
way	O
.	O
Further	O
progress	O
towards	O
understanding	O
compositionality	O
in	O
tasks	O
such	O
as	O
sentiment	O
detection	O
requires	O
richer	B-RESEARCH_PROBLEM
supervised	I-RESEARCH_PROBLEM
training	I-RESEARCH_PROBLEM
and	I-RESEARCH_PROBLEM
evaluation	I-RESEARCH_PROBLEM
resources	E-RESEARCH_PROBLEM
and	O
more	O
powerful	O
models	O
of	O
composition	O
.	O
To	O
remedy	O
this	O
,	O
we	O
introduce	O
a	O
Sentiment	B-RESEARCH_PROBLEM
Treebank	E-RESEARCH_PROBLEM
.	O
It	O
includes	O
fine	O
grained	O
sentiment	O
labels	O
for	O
215,154	O
phrases	O
in	O
the	O
parse	O
trees	O
of	O
11,855	O
sentences	O
and	O
presents	O
new	O
challenges	O
for	O
sentiment	O
compositionality	O
.	O
To	O
address	O
them	O
,	O
we	O
introduce	O
the	O
Recursive	O
Neural	O
Tensor	O
Network	O
.	O
When	O
trained	O
on	O
the	O
new	O
treebank	O
,	O
this	O
model	O
outperforms	O
all	O
previous	O
methods	O
on	O
several	O
metrics	O
.	O
It	O
pushes	O
the	O
state	O
of	O
the	O
art	O
in	O
single	O
sentence	O
positive	O
/	O
negative	O
classification	O
from	O
80	O
%	O
up	O
to	O
85.4	O
%	O
.	O

Aspect	O
sentiment	O
classification	O
(	O
ASC	S-RESEARCH_PROBLEM
)	O
is	O
a	O
fundamental	O
task	O
in	O
sentiment	B-RESEARCH_PROBLEM
analysis	E-RESEARCH_PROBLEM
.	O
Given	O
an	O
aspect	O
/	O
target	O
and	O
a	O
sentence	O
,	O
the	O
task	O
classifies	O
the	O
sentiment	O
polarity	O
expressed	O
on	O
the	O
target	O
in	O
the	O
sentence	O
.	O
Memory	O
networks	O
(	O
MNs	O
)	O
have	O
been	O
used	O
for	O
this	O
task	O
recently	O
and	O
have	O
achieved	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
.	O
In	O
MNs	O
,	O
attention	O
mechanism	O
plays	O
a	O
crucial	O
role	O
in	O
detecting	O
the	O
sentiment	O
context	O
for	O
the	O
given	O
target	O
.	O
However	O
,	O
we	O
found	O
an	O
important	O
problem	O
with	O
the	O
current	O
MNs	O
in	O
performing	O
the	O
ASC	S-RESEARCH_PROBLEM
task	O
.	O
Simply	O
improving	O
the	O
attention	O
mechanism	O
will	O
not	O
solve	O
it	O
.	O
The	O
problem	O
is	O
referred	O
to	O
as	O
target	O
-	O
sensitive	O
sentiment	O
,	O
which	O
means	O
that	O
the	O
sentiment	O
polarity	O
of	O
the	O
(	O
detected	O
)	O
context	O
is	O
dependent	O
on	O
the	O
given	O
target	O
and	O
it	O
can	O
not	O
be	O
inferred	O
from	O
the	O
context	O
alone	O
.	O

Because	O
of	O
their	O
superior	O
ability	O
to	O
preserve	O
sequence	O
information	O
overtime	O
,	O
Long	O
Short	O
-	O
Term	O
Memory	O
(	O
LSTM	O
)	O
networks	O
,	O
a	O
type	O
of	O
recurrent	O
neural	O
network	O
with	O
a	O
more	O
complex	O
computational	O
unit	O
,	O
have	O
obtained	O
strong	O
results	O
on	O
a	O
variety	O
of	O
sequence	O
modeling	O
tasks	O
.	O
The	O
only	O
underlying	O
LSTM	O
structure	O
that	O
has	O
been	O
explored	O
so	O
far	O
is	O
a	O
linear	O
chain	O
.	O
However	O
,	O
natural	O
language	O
exhibits	O
syntactic	O
properties	O
that	O
would	O
naturally	O
combine	O
words	O
to	O
phrases	O
.	O
We	O
introduce	O
the	O
Tree	O
-	O
LSTM	O
,	O
a	O
generalization	O
of	O
LSTMs	O
to	O
tree	O
-	O
structured	O
network	O
topologies	O
.	O
Tree	O
-	O
LSTMs	O
outperform	O
all	O
existing	O
systems	O
and	O
strong	O
LSTM	O
baselines	O
on	O
two	O
tasks	O
:	O
predicting	B-RESEARCH_PROBLEM
the	I-RESEARCH_PROBLEM
semantic	I-RESEARCH_PROBLEM
relatedness	I-RESEARCH_PROBLEM
of	I-RESEARCH_PROBLEM
two	I-RESEARCH_PROBLEM
sentences	E-RESEARCH_PROBLEM
(	O
Sem	O
Eval	O
2014	O
,	O
Task	O
1	O
)	O
and	O
sentiment	B-RESEARCH_PROBLEM
classification	E-RESEARCH_PROBLEM
(	O
Stanford	O
Sentiment	O
Treebank	O
)	O
.	O

Recurrent	O
neural	O
networks	O
have	O
become	O
ubiquitous	O
in	O
computing	B-RESEARCH_PROBLEM
representations	I-RESEARCH_PROBLEM
of	I-RESEARCH_PROBLEM
sequential	I-RESEARCH_PROBLEM
data	E-RESEARCH_PROBLEM
,	O
especially	O
textual	O
data	O
in	O
natural	O
language	O
processing	O
.	O
In	O
particular	O
,	O
Bidirectional	O
LSTMs	O
are	O
at	O
the	O
heart	O
of	O
several	O
neural	O
models	O
achieving	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
in	O
a	O
wide	O
variety	O
of	O
tasks	O
in	O
NLP	O
.	O
However	O
,	O
BiLSTMs	O
are	O
known	O
to	O
suffer	O
from	O
sequential	O
bias	O
the	O
contextual	O
representation	O
of	O
a	O
token	O
is	O
heavily	O
influenced	O
by	O
tokens	O
close	O
to	O
it	O
in	O
a	O
sentence	O
.	O
We	O
propose	O
a	O
general	O
and	O
effective	O
improvement	O
to	O
the	O
BiLSTM	O
model	O
which	O
encodes	O
each	O
suffix	O
and	O
prefix	O
of	O
a	O
sequence	O
of	O
tokens	O
in	O
both	O
forward	O
and	O
reverse	O
directions	O
.	O
We	O
call	O
our	O
model	O
Suffix	O
Bidirectional	O
LSTM	O
or	O
SuBiLSTM	O
.	O
This	O
introduces	O
an	O
alternate	O
bias	O
that	O
favors	O
long	O
range	O
dependencies	O
.	O
We	O
apply	O
SuBiLSTMs	O
to	O
several	O
tasks	O
that	O
require	O
sentence	O
modeling	O
.	O

In	O
this	O
paper	O
we	O
present	O
two	O
deep	O
-	O
learning	O
systems	O
that	O
competed	O
at	O
SemEval	O
-	O
2017	O
Task	O
4	O
"	O
Sentiment	B-RESEARCH_PROBLEM
Analysis	I-RESEARCH_PROBLEM
in	I-RESEARCH_PROBLEM
Twitter	E-RESEARCH_PROBLEM
"	O
.	O
We	O
participated	O
in	O
all	O
subtasks	O
for	O
English	O
tweets	O
,	O
involving	O
message	O
-	O
level	O
and	O
topic	O
-	O
based	O
sentiment	O
polarity	O
classification	O
and	O
quantification	O
.	O
We	O
use	O
Long	O
Short	O
-	O
Term	O
Memory	O
(	O
LSTM	O
)	O
networks	O
augmented	O
with	O
two	O
kinds	O
of	O
attention	O
mechanisms	O
,	O
on	O
top	O
of	O
word	O
embeddings	O
pre-trained	O
on	O
a	O
big	O
collection	O
of	O
Twitter	O
messages	O
.	O
Also	O
,	O
we	O
present	O
a	O
text	O
processing	O
tool	O
suitable	O
for	O
social	O
network	O
messages	O
,	O
which	O
performs	O
tokenization	O
,	O
word	O
normalization	O
,	O
segmentation	O
and	O
spell	O
correction	O
.	O
Moreover	O
,	O
our	O
approach	O
uses	O
no	O
hand	O
-	O
crafted	O
features	O
or	O
sentiment	O
lexicons	O
.	O
We	O
ranked	O
1	O
st	O
(	O
tie	O
)	O
in	O
Subtask	O
A	O
,	O
and	O
achieved	O
very	O
competitive	O
results	O
in	O
the	O
rest	O
of	O
the	O
Subtasks	O
.	O
Both	O
the	O
word	O
embeddings	O
and	O
our	O
text	O
processing	O
tool	O
1	O
are	O
available	O
to	O
the	O
research	O
community	O
.	O

In	O
document	B-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
level	I-RESEARCH_PROBLEM
sentiment	I-RESEARCH_PROBLEM
classification	E-RESEARCH_PROBLEM
,	O
each	O
document	O
must	O
be	O
mapped	O
to	O
a	O
fixed	O
length	O
vector	O
.	O
Document	O
embedding	O
models	O
map	O
each	O
document	O
to	O
a	O
dense	O
,	O
lowdimensional	O
vector	O
in	O
continuous	O
vector	O
space	O
.	O
This	O
paper	O
proposes	O
training	O
document	O
embeddings	O
using	O
cosine	O
similarity	O
instead	O
of	O
dot	O
product	O
.	O
Experiments	O
on	O
the	O
IMDB	O
dataset	O
show	O
that	O
accuracy	O
is	O
improved	O
when	O
using	O
cosine	O
similarity	O
compared	O
to	O
using	O
dot	O
product	O
,	O
while	O
using	O
feature	O
combination	O
with	O
Nave	O
Bayes	O
weighted	O
bag	O
of	O
n-grams	O
achieves	O
a	O
new	O
state	O
of	O
the	O
art	O
accuracy	O
of	O
97.42	O
%	O
.	O
Code	O
to	O
reproduce	O
all	O
experiments	O
is	O
available	O
at	O
https://github.com/tanthongtan/dv-cosine.	O

Aspect	O
-	O
level	O
sentiment	B-RESEARCH_PROBLEM
analysis	E-RESEARCH_PROBLEM
aims	O
to	O
identify	O
the	O
sentiment	O
of	O
a	O
specific	O
target	O
in	O
its	O
context	O
.	O
Previous	O
works	O
have	O
proved	O
that	O
the	O
interactions	O
between	O
aspects	O
and	O
the	O
contexts	O
are	O
important	O
.	O
On	O
this	O
basis	O
,	O
we	O
also	O
propose	O
a	O
succinct	O
hierarchical	O
attention	O
based	O
mechanism	O
to	O
fuse	O
the	O
information	O
of	O
targets	O
and	O
the	O
contextual	O
words	O
.	O
In	O
addition	O
,	O
most	O
existing	O
methods	O
ignore	O
the	O
position	O
information	O
of	O
the	O
aspect	O
when	O
encoding	O
the	O
sentence	O
.	O
In	O
this	O
paper	O
,	O
we	O
argue	O
that	O
the	O
position	O
-	O
aware	O
representations	O
are	O
beneficial	O
to	O
this	O
task	O
.	O
Therefore	O
,	O
we	O
propose	O
a	O
hierarchical	O
attention	O
based	O
position	O
-	O
aware	O
network	O
(	O
HAPN	O
)	O
,	O
which	O
introduces	O
position	O
embeddings	O
to	O
learn	O
the	O
position	O
-	O
aware	O
representations	O
of	O
sentences	O
and	O
further	O
generate	O
the	O
target	O
-	O
specific	O
representations	O
of	O
contextual	O
words	O
.	O
The	O
experimental	O
results	O
on	O
SemEval	O
2014	O
dataset	O
show	O
that	O
our	O
approach	O
outperforms	O
the	O
state	O
-	O
of	O
-	O
theart	O
methods	O
.	O

Aspect	O
based	O
sentiment	O
analysis	O
(	O
ABSA	S-RESEARCH_PROBLEM
)	O
can	O
provide	O
more	O
detailed	O
information	O
than	O
general	O
sentiment	O
analysis	O
,	O
because	O
it	O
aims	O
to	O
predict	O
the	O
sentiment	O
polarities	O
of	O
the	O
given	O
aspects	O
or	O
entities	O
in	O
text	O
.	O
We	O
summarize	O
previous	O
approaches	O
into	O
two	O
subtasks	O
:	O
aspect	O
-	O
category	O
sentiment	O
analysis	O
(	O
ACSA	O
)	O
and	O
aspect	O
-	O
term	O
sentiment	O
analysis	O
(	O
ATSA	O
)	O
.	O
Most	O
previous	O
approaches	O
employ	O
long	O
short	O
-	O
term	O
memory	O
and	O
attention	O
mechanisms	O
to	O
predict	O
the	O
sentiment	O
polarity	O
of	O
the	O
concerned	O
targets	O
,	O
which	O
are	O
often	O
complicated	O
and	O
need	O
more	O
training	O
time	O
.	O
We	O
propose	O
a	O
model	O
based	O
on	O
convolutional	O
neural	O
networks	O
and	O
gating	O
mechanisms	O
,	O
which	O
is	O
more	O
accurate	O
and	O
efficient	O
.	O
First	O
,	O
the	O
novel	O
Gated	O
Tanh	O
-	O
ReLU	O
Units	O
can	O
selectively	O
output	O
the	O
sentiment	O
features	O
according	O
to	O
the	O
given	O
aspect	O
or	O
entity	O
.	O
The	O
architecture	O
is	O
much	O
simpler	O
than	O
attention	O
layer	O
used	O
in	O
the	O
existing	O
models	O
.	O
Second	O
,	O
the	O
computations	O
of	O
our	O
model	O
could	O
be	O
easily	O
parallelized	O
during	O
training	O
,	O
because	O
convolutional	O
layers	O
do	O
not	O
have	O
time	O
dependency	O
as	O
in	O
LSTM	O
layers	O
,	O
and	O
gating	O
units	O
also	O
work	O
independently	O
.	O

Aspect	B-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
level	I-RESEARCH_PROBLEM
sentiment	I-RESEARCH_PROBLEM
classification	E-RESEARCH_PROBLEM
aims	O
to	O
identify	O
the	O
sentiment	O
expressed	O
towards	O
some	O
aspects	O
given	O
context	O
sentences	O
.	O
In	O
this	O
paper	O
,	O
we	O
introduce	O
an	O
attention	O
-	O
over-	O
attention	O
(	O
AOA	O
)	O
neural	O
network	O
for	O
aspect	O
level	O
sentiment	O
classification	O
.	O
Our	O
approach	O
models	O
aspects	O
and	O
sentences	O
in	O
a	O
joint	O
way	O
and	O
explicitly	O
captures	O
the	O
interaction	O
between	O
aspects	O
and	O
context	O
sentences	O
.	O
With	O
the	O
AOA	O
module	O
,	O
our	O
model	O
jointly	O
learns	O
the	O
representations	O
for	O
aspects	O
and	O
sentences	O
,	O
and	O
automatically	O
focuses	O
on	O
the	O
important	O
parts	O
in	O
sentences	O
.	O
Our	O
experiments	O
on	O
laptop	O
and	O
restaurant	O
datasets	O
demonstrate	O
our	O
approach	O
outperforms	O
previous	O
LSTM	O
-	O
based	O
architectures	O
.	O

Messages	O
in	O
human	O
conversations	O
inherently	O
convey	O
emotions	O
.	O
The	O
task	O
of	O
detecting	B-RESEARCH_PROBLEM
emotions	I-RESEARCH_PROBLEM
in	I-RESEARCH_PROBLEM
textual	I-RESEARCH_PROBLEM
conversations	E-RESEARCH_PROBLEM
leads	O
to	O
a	O
wide	O
range	O
of	O
applications	O
such	O
as	O
opinion	O
mining	O
in	O
social	O
networks	O
.	O
However	O
,	O
enabling	O
machines	O
to	O
analyze	O
emotions	O
in	O
conversations	O
is	O
challenging	O
,	O
partly	O
because	O
humans	O
often	O
rely	O
on	O
the	O
context	O
and	O
commonsense	O
knowledge	O
to	O
express	O
emotions	O
.	O
In	O
this	O
paper	O
,	O
we	O
address	O
these	O
challenges	O
by	O
proposing	O
a	O
Knowledge	O
-	O
Enriched	O
Transformer	O
(	O
KET	O
)	O
,	O
where	O
contextual	O
utterances	O
are	O
interpreted	O
using	O
hierarchical	O
self	O
-	O
attention	O
and	O
external	O
commonsense	O
knowledge	O
is	O
dynamically	O
leveraged	O
using	O
a	O
context	O
-	O
aware	O
affective	O
graph	O
attention	O
mechanism	O
.	O
Experiments	O
on	O
multiple	O
textual	O
conversation	O
datasets	O
demonstrate	O
that	O
both	O
context	O
and	O
commonsense	O
knowledge	O
are	O
consistently	O
beneficial	O
to	O
the	O
emotion	O
detection	O
performance	O
.	O
In	O
addition	O
,	O
the	O
experimental	O
results	O
show	O
that	O
our	O
KET	O
model	O
outperforms	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
on	O
most	O
of	O
the	O
tested	O
datasets	O
in	O
F1	O
score	O
.	O

We	O
introduce	O
a	O
novel	O
parameterized	O
convolutional	O
neural	O
network	O
for	O
aspect	O
level	O
sentiment	B-RESEARCH_PROBLEM
classification	E-RESEARCH_PROBLEM
.	O
Using	O
parameterized	O
filters	O
and	O
parameterized	O
gates	O
,	O
we	O
incorporate	O
aspect	O
information	O
into	O
convolutional	O
neural	O
networks	O
(	O
CNN	O
)	O
.	O
Experiments	O
demonstrate	O
that	O
our	O
parameterized	O
filters	O
and	O
parameterized	O
gates	O
effectively	O
capture	O
the	O
aspectspecific	O
features	O
,	O
and	O
our	O
CNN	O
-	O
based	O
models	O
achieve	O
excellent	O
results	O
on	O
SemEval	O
2014	O
datasets	O
.	O

Aspect	O
-	O
level	O
sentiment	B-RESEARCH_PROBLEM
classification	E-RESEARCH_PROBLEM
aims	O
at	O
identifying	O
the	O
sentiment	O
polarity	O
of	O
specific	O
target	O
in	O
its	O
context	O
.	O
Previous	O
approaches	O
have	O
realized	O
the	O
importance	O
of	O
targets	O
in	O
sentiment	B-RESEARCH_PROBLEM
classification	E-RESEARCH_PROBLEM
and	O
developed	O
various	O
methods	O
with	O
the	O
goal	O
of	O
precisely	O
modeling	O
their	O
contexts	O
via	O
generating	O
target	O
-	O
specific	O
representations	O
.	O
However	O
,	O
these	O
studies	O
always	O
ignore	O
the	O
separate	O
modeling	O
of	O
targets	O
.	O
In	O
this	O
paper	O
,	O
we	O
argue	O
that	O
both	O
targets	O
and	O
contexts	O
deserve	O
special	O
treatment	O
and	O
need	O
to	O
be	O
learned	O
their	O
own	O
representations	O
via	O
interactive	O
learning	O
.	O
Then	O
,	O
we	O
propose	O
the	O
interactive	O
attention	O
networks	O
(	O
IAN	O
)	O
to	O
interactively	O
learn	O
attentions	O
in	O
the	O
contexts	O
and	O
targets	O
,	O
and	O
generate	O
the	O
representations	O
for	O
targets	O
and	O
contexts	O
separately	O
.	O
With	O
this	O
design	O
,	O
the	O
IAN	O
model	O
can	O
well	O
represent	O
a	O
target	O
and	O
its	O
collocative	O
context	O
,	O
which	O
is	O
helpful	O
to	O
sentiment	B-RESEARCH_PROBLEM
classification	E-RESEARCH_PROBLEM
.	O
Experimental	O
results	O
on	O
Se	O
-m	O
Eval	O
2014	O
Datasets	O
demonstrate	O
the	O
effectiveness	O
of	O
our	O
model	O
.	O

We	O
introduce	O
a	O
class	O
of	O
convolutional	O
neural	O
networks	O
(	O
CNNs	O
)	O
that	O
utilize	O
recurrent	O
neural	O
networks	O
(	O
RNNs	O
)	O
as	O
convolution	O
filters	O
.	O
A	O
convolution	O
filter	O
is	O
typically	O
implemented	O
as	O
a	O
linear	O
affine	O
transformation	O
followed	O
by	O
a	O
nonlinear	O
function	O
,	O
which	O
fails	O
to	O
account	O
for	O
language	O
compositionality	O
.	O
As	O
a	O
result	O
,	O
it	O
limits	O
the	O
use	O
of	O
high	O
-	O
order	O
filters	O
thatare	O
often	O
warranted	O
for	O
natural	O
language	O
processing	O
tasks	O
.	O
In	O
this	O
work	O
,	O
we	O
model	B-RESEARCH_PROBLEM
convolution	I-RESEARCH_PROBLEM
filters	I-RESEARCH_PROBLEM
with	I-RESEARCH_PROBLEM
RNNs	E-RESEARCH_PROBLEM
that	O
naturally	O
capture	O
compositionality	O
and	O
long	O
-	O
term	O
dependencies	O
in	O
language	O
.	O
We	O
show	O
that	O
simple	O
CNN	O
architectures	O
equipped	O
with	O
recurrent	O
neural	O
filters	O
(	O
RNFs	O
)	O
achieve	O
results	O
thatare	O
on	O
par	O
with	O
the	O
best	O
published	O
ones	O
on	O
the	O
Stanford	O
Sentiment	O
Treebank	O
and	O
two	O
answer	O
sentence	O
selection	O
datasets	O
.	O
1	O

Sentiment	B-RESEARCH_PROBLEM
analysis	E-RESEARCH_PROBLEM
(	O
SA	S-RESEARCH_PROBLEM
)	O
is	O
one	O
of	O
the	O
most	O
useful	O
natural	O
language	O
processing	O
applications	O
.	O
Literature	O
is	O
flooding	O
with	O
many	O
papers	O
and	O
systems	O
addressing	O
this	O
task	O
,	O
but	O
most	O
of	O
the	O
work	O
is	O
focused	O
on	O
English	O
.	O
In	O
this	O
paper	O
,	O
we	O
present	O
"	O
Mazajak	O
"	O
,	O
an	O
online	O
system	O
for	O
Arabic	O
SA	S-RESEARCH_PROBLEM
.	O
The	O
system	O
is	O
based	O
on	O
a	O
deep	O
learning	O
model	O
,	O
which	O
achieves	O
state	O
-	O
of	O
-	O
theart	O
results	O
on	O
many	O
Arabic	O
dialect	O
datasets	O
including	O
SemEval	O
2017	O
and	O
ASTD	O
.	O
The	O
availability	O
of	O
such	O
system	O
should	O
assist	O
various	O
applications	O
and	O
research	O
that	O
rely	O
on	O
sentiment	O
analysis	O
as	O
a	O
tool	O
.	O

Sentiment	B-RESEARCH_PROBLEM
analysis	E-RESEARCH_PROBLEM
has	O
immense	O
implications	O
in	O
modern	O
businesses	O
through	O
user-feedback	O
mining	O
.	O
Large	O
product	O
-	O
based	O
enterprises	O
like	O
Samsung	O
and	O
Apple	O
make	O
crucial	O
business	O
decisions	O
based	O
on	O
the	O
large	O
quantity	O
of	O
user	O
reviews	O
and	O
suggestions	O
available	O
in	O
different	O
e-commerce	O
websites	O
and	O
social	O
media	O
platforms	O
like	O
Amazon	O
and	O
Facebook	O
.	O
Sentiment	B-RESEARCH_PROBLEM
analysis	E-RESEARCH_PROBLEM
caters	O
to	O
these	O
needs	O
by	O
summarizing	O
user	O
sentiment	O
behind	O
a	O
particular	O
object	O
.	O
In	O
this	O
paper	O
,	O
we	O
present	O
a	O
novel	O
approach	O
of	O
incorporating	O
the	O
neighboring	O
aspects	O
related	O
information	O
into	O
the	O
sentiment	O
classification	O
of	O
the	O
target	O
aspect	O
using	O
memory	O
networks	O
.	O
Our	O
method	O
outperforms	O
the	O
state	O
of	O
the	O
art	O
by	O
1.6	O
%	O
on	O
average	O
in	O
two	O
distinct	O
domains	O
.	O

We	O
explore	O
the	O
properties	O
of	O
byte	O
-	O
level	O
recurrent	O
language	O
models	O
.	O
When	O
given	O
sufficient	O
amounts	O
of	O
capacity	O
,	O
training	O
data	O
,	O
and	O
compute	O
time	O
,	O
the	O
representations	O
learned	O
by	O
these	O
models	O
include	O
disentangled	O
features	O
corresponding	O
to	O
high	O
-	O
level	O
concepts	O
.	O
Specifically	O
,	O
we	O
find	O
a	O
single	O
unit	O
which	O
performs	O
sentiment	B-RESEARCH_PROBLEM
analysis	E-RESEARCH_PROBLEM
.	O
These	O
representations	O
,	O
learned	O
in	O
an	O
unsupervised	O
manner	O
,	O
achieve	O
state	O
of	O
the	O
art	O
on	O
the	O
binary	O
subset	O
of	O
the	O
Stanford	O
Sentiment	O
Treebank	O
.	O
They	O
are	O
also	O
very	O
data	O
efficient	O
.	O
When	O
using	O
only	O
a	O
handful	O
of	O
labeled	O
examples	O
,	O
our	O
approach	O
matches	O
the	O
performance	O
of	O
strong	O
baselines	O
trained	O
on	O
full	O
datasets	O
.	O
We	O
also	O
demonstrate	O
the	O
sentiment	O
unit	O
has	O
a	O
direct	O
influence	O
on	O
the	O
generative	O
process	O
of	O
the	O
model	O
.	O

In	O
this	O
paper	O
,	O
we	O
introduce	O
the	O
task	O
of	O
targeted	B-RESEARCH_PROBLEM
aspect	I-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
based	I-RESEARCH_PROBLEM
sentiment	I-RESEARCH_PROBLEM
analysis	E-RESEARCH_PROBLEM
.	O
The	O
goal	O
is	O
to	O
extract	O
fine	O
-	O
grained	O
information	O
with	O
respect	O
to	O
entities	O
mentioned	O
in	O
user	O
comments	O
.	O
This	O
work	O
extends	O
both	O
aspect	O
-	O
based	O
sentiment	O
analysis	O
that	O
assumes	O
a	O
single	O
entity	O
per	O
document	O
and	O
targeted	O
sentiment	O
analysis	O
that	O
assumes	O
a	O
single	O
sentiment	O
towards	O
a	O
target	O
entity	O
.	O
In	O
particular	O
,	O
we	O
identify	O
the	O
sentiment	O
towards	O
each	O
aspect	O
of	O
one	O
or	O
more	O
entities	O
.	O
As	O
a	O
testbed	O
for	O
this	O
task	O
,	O
we	O
introduce	O
the	O
SentiHood	O
dataset	O
,	O
extracted	O
from	O
a	O
question	O
answering	O
(	O
QA	O
)	O
platform	O
where	O
urban	O
neighbourhoods	O
are	O
discussed	O
by	O
users	O
.	O
In	O
this	O
context	O
units	O
of	O
text	O
often	O
mention	O
several	O
aspects	O
of	O
one	O
or	O
more	O
neighbourhoods	O
.	O
This	O
is	O
the	O
first	O
time	O
that	O
a	O
generic	O
social	O
media	O
platform	O
in	O
this	O
case	O
a	O
QA	O
platform	O
,	O
is	O
used	O
for	O
fine	O
-	O
grained	O
opinion	O
mining	O
.	O

Emotion	B-RESEARCH_PROBLEM
recognition	I-RESEARCH_PROBLEM
in	I-RESEARCH_PROBLEM
conversations	E-RESEARCH_PROBLEM
is	O
crucial	O
for	O
building	O
empathetic	O
machines	O
.	O
Current	O
work	O
in	O
this	O
domain	O
do	O
not	O
explicitly	O
consider	O
the	O
inter-personal	O
influences	O
that	O
thrive	O
in	O
the	O
emotional	O
dynamics	O
of	O
dialogues	O
.	O
To	O
this	O
end	O
,	O
we	O
propose	O
Interactive	O
COnversational	O
memory	O
Network	O
(	O
ICON	O
)	O
,	O
a	O
multimodal	O
emotion	O
detection	O
framework	O
that	O
extracts	O
multimodal	O
features	O
from	O
conversational	O
videos	O
and	O
hierarchically	O
models	O
the	O
selfand	O
interspeaker	O
emotional	O
influences	O
into	O
global	O
memories	O
.	O
Such	O
memories	O
generate	O
contextual	O
summaries	O
which	O
aid	O
in	O
predicting	O
the	O
emotional	O
orientation	O
of	O
utterance	O
-	O
videos	O
.	O
Our	O
model	O
outperforms	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
networks	O
on	O
multiple	O
classification	O
and	O
regression	O
tasks	O
in	O
two	O
benchmark	O
datasets	O
.	O
1	O

Aspect	O
-	O
level	O
sentiment	O
classification	O
is	O
a	O
finegrained	O
task	O
in	O
sentiment	B-RESEARCH_PROBLEM
analysis	E-RESEARCH_PROBLEM
.	O
Since	O
it	O
provides	O
more	O
complete	O
and	O
in	O
-	O
depth	O
results	O
,	O
aspect	O
-	O
level	O
sentiment	B-RESEARCH_PROBLEM
analysis	E-RESEARCH_PROBLEM
has	O
received	O
much	O
attention	O
these	O
years	O
.	O
In	O
this	O
paper	O
,	O
we	O
reveal	O
that	O
the	O
sentiment	O
polarity	O
of	O
a	O
sentence	O
is	O
not	O
only	O
determined	O
by	O
the	O
content	O
but	O
is	O
also	O
highly	O
related	O
to	O
the	O
concerned	O
aspect	O
.	O
For	O
instance	O
,	O
"	O
The	O
appetizers	O
are	O
ok	O
,	O
but	O
the	O
service	O
is	O
slow	O
.	O
"	O
,	O
for	O
aspect	O
taste	O
,	O
the	O
polarity	O
is	O
positive	O
while	O
for	O
service	O
,	O
the	O
polarity	O
is	O
negative	O
.	O
Therefore	O
,	O
it	O
is	O
worthwhile	O
to	O
explore	O
the	O
connection	O
between	O
an	O
aspect	O
and	O
the	O
content	O
of	O
a	O
sentence	O
.	O
To	O
this	O
end	O
,	O
we	O
propose	O
an	O
Attention	O
-	O
based	O
Long	O
Short	O
-	O
Term	O
Memory	O
Network	O
for	O
aspect	O
-	O
level	O
sentiment	O
classification	O
.	O
The	O
attention	O
mechanism	O
can	O
concentrate	O
on	O
different	O
parts	O
of	O
a	O
sentence	O
when	O
different	O
aspects	O
are	O
taken	O
as	O
input	O
.	O

We	O
propose	O
a	O
novel	O
multi-grained	O
attention	O
network	O
(	O
MGAN	O
)	O
model	O
for	O
aspect	B-RESEARCH_PROBLEM
level	I-RESEARCH_PROBLEM
sentiment	I-RESEARCH_PROBLEM
classification	E-RESEARCH_PROBLEM
.	O
Existing	O
approaches	O
mostly	O
adopt	O
coarse	O
-	O
grained	O
attention	O
mechanism	O
,	O
which	O
may	O
bring	O
information	O
loss	O
if	O
the	O
aspect	O
has	O
multiple	O
words	O
or	O
larger	O
context	O
.	O
We	O
propose	O
a	O
fine	O
-	O
grained	O
attention	O
mechanism	O
,	O
which	O
can	O
capture	O
the	O
word	O
-	O
level	O
interaction	O
between	O
aspect	O
and	O
context	O
.	O
And	O
then	O
we	O
leverage	O
the	O
fine	O
-	O
grained	O
and	O
coarsegrained	O
attention	O
mechanisms	O
to	O
compose	O
the	O
MGAN	O
framework	O
.	O
Moreover	O
,	O
unlike	O
previous	O
works	O
which	O
train	O
each	O
aspect	O
with	O
its	O
context	O
separately	O
,	O
we	O
design	O
an	O
aspect	O
alignment	O
loss	O
to	O
depict	O
the	O
aspect	O
-	O
level	O
interactions	O
among	O
the	O
aspects	O
that	O
have	O
the	O
same	O
context	O
.	O
We	O
evaluate	O
the	O
proposed	O
approach	O
on	O
three	O
datasets	O
:	O
laptop	O
and	O
restaurant	O
are	O
from	O
SemEval	O
2014	O
,	O
and	O
the	O
last	O
one	O
is	O
atwitter	O
dataset	O
.	O
Experimental	O
results	O
show	O
that	O
the	O
multi-grained	O
attention	O
network	O
consistently	O
outperforms	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
on	O
all	O
three	O
datasets	O
.	O

Aspect	O
-	O
based	O
sentiment	O
analysis	O
(	O
ABT3	O
)	O
,	O
which	O
aims	O
to	O
identify	O
fine	O
-	O
grained	O
opinion	O
polarity	O
towards	O
a	O
specific	O
aspect	O
,	O
is	O
a	O
challenging	O
subtask	O
of	O
sentiment	O
analysis	O
(	O
SA	O
)	O
.	O
In	O
this	O
paper	O
,	O
we	O
construct	O
an	O
auxiliary	O
sentence	O
from	O
the	O
aspect	O
and	O
convert	O
ABSA	S-RESEARCH_PROBLEM
to	O
a	O
sentence	O
-	O
pair	O
classification	O
task	O
,	O
such	O
as	O
question	O
answering	O
(	O
QA	O
)	O
and	O
natural	O
language	O
inference	O
(	O
NLI	O
)	O
.	O
We	O
fine	O
-	O
tune	O
the	O
pre-trained	O
model	O
from	O
BERT	O
and	O
achieve	O
new	O
state	O
-	O
of	O
the	O
-	O
art	O
results	O
on	O
SentiHood	O
and	O
SemEval	O
-	O
2014	O
Task	O
4	O
datasets	O
.	O

Deep	O
learning	O
techniques	O
have	O
achieved	O
success	O
in	O
aspect	O
-	O
based	O
sentiment	B-RESEARCH_PROBLEM
analysis	E-RESEARCH_PROBLEM
in	O
recent	O
years	O
.	O
However	O
,	O
there	O
are	O
two	O
important	O
issues	O
that	O
still	O
remain	O
to	O
be	O
further	O
studied	O
,	O
i.e.	O
,	O
1	O
)	O
how	O
to	O
efficiently	O
represent	O
the	O
target	O
especially	O
when	O
the	O
target	O
contains	O
multiple	O
words	O
;	O
2	O
)	O
how	O
to	O
utilize	O
the	O
interaction	O
between	O
target	O
and	O
left	O
/	O
right	O
contexts	O
to	O
capture	O
the	O
most	O
important	O
words	O
in	O
them	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
an	O
approach	O
,	O
called	O
left	O
-	O
centerright	O
separated	O
neural	O
network	O
with	O
rotatory	O
attention	O
(	O
LCR	O
-	O
Rot	O
)	O
,	O
to	O
better	O
address	O
the	O
two	O
problems	O
.	O
Our	O
approach	O
has	O
two	O
characteristics	O
:	O
1	O
)	O
it	O
has	O
three	O
separated	O
LSTMs	O
,	O
i.e.	O
,	O
left	O
,	O
center	O
and	O
right	O
LSTMs	O
,	O
corresponding	O
to	O
three	O
parts	O
of	O
a	O
review	O
(	O
left	O
context	O
,	O
target	O
phrase	O
and	O
right	O
context	O
)	O
;	O
2	O
)	O
it	O
has	O
a	O
rotatory	O
attention	O
mechanism	O
which	O
models	O
the	O
relation	O
between	O
target	O
and	O
left	O
/	O
right	O
contexts	O
.	O
The	O
target2context	O
attention	O
is	O
used	O
to	O
capture	O
the	O
most	O
indicative	O
sentiment	O
words	O
in	O
left	O
/	O
right	O
contexts	O
.	O

Multi-modal	O
sentiment	B-RESEARCH_PROBLEM
analysis	E-RESEARCH_PROBLEM
offers	O
various	O
challenges	O
,	O
one	O
being	O
the	O
effective	O
combination	O
of	O
different	O
input	O
modalities	O
,	O
namely	O
text	O
,	O
visual	O
and	O
acoustic	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
recurrent	O
neural	O
network	O
based	O
multi-modal	O
attention	O
framework	O
that	O
leverages	O
the	O
contextual	O
information	O
for	O
utterance	O
-	O
level	O
sentiment	O
prediction	O
.	O
The	O
proposed	O
approach	O
applies	O
attention	O
on	O
multi-modal	O
multi-utterance	O
representations	O
and	O
tries	O
to	O
learn	O
the	O
contributing	O
features	O
amongst	O
them	O
.	O
We	O
evaluate	O
our	O
proposed	O
approach	O
on	O
two	O
multi-modal	O
sentiment	B-RESEARCH_PROBLEM
analysis	E-RESEARCH_PROBLEM
benchmark	O
datasets	O
,	O
viz.	O
CMU	O
Multi-modal	O
Opinion	O
-	O
level	O
Sentiment	O
Intensity	O
(	O
CMU	O
-	O
MOSI	O
)	O
corpus	O
and	O
the	O
recently	O
released	O
CMU	O
Multi-modal	O
Opinion	O
Sentiment	O
and	O
Emotion	O
Intensity	O
(	O
CMU	O
-	O
MOSEI	O
)	O
corpus	O
.	O
Evaluation	O
results	O
show	O
the	O
effectiveness	O
of	O
our	O
proposed	O
approach	O
with	O
the	O
accuracies	O
of	O
82.31	O
%	O
and	O
79.	O
80	O
%	O
for	O
the	O
MOSI	O
and	O
MO	O
-	O
SEI	O
datasets	O
,	O
respectively	O
.	O
These	O
are	O
approximately	O
2	O
and	O
1	O
points	O
performance	O
improvement	O
over	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
for	O
the	O
datasets	O
.	O

The	O
neuroscience	O
study	O
[	O
1	O
]	O
has	O
revealed	O
the	O
discrepancy	O
of	O
emotion	O
expression	O
between	O
left	O
and	O
right	O
hemispheres	O
of	O
human	O
brain	O
.	O
Inspired	O
by	O
this	O
study	O
,	O
in	O
this	O
paper	O
,	O
we	O
propose	O
a	O
novel	O
bi-hemispheric	O
discrepancy	O
model	O
(	O
BiHDM	O
)	O
to	O
learn	O
the	O
asymmetric	O
differences	O
between	O
two	O
hemispheres	O
for	O
electroencephalograph	O
(	O
EEG	O
)	O
emotion	B-RESEARCH_PROBLEM
recognition	E-RESEARCH_PROBLEM
.	O
Concretely	O
,	O
we	O
first	O
employ	O
four	O
directed	O
recurrent	O
neural	O
networks	O
(	O
RNNs	O
)	O
based	O
on	O
two	O
spatial	O
orientations	O
to	O
traverse	O
electrode	O
signals	O
on	O
two	O
separate	O
brain	O
regions	O
,	O
which	O
enables	O
the	O
model	O
to	O
obtain	O
the	O
deep	O
representations	O
of	O
all	O
the	O
EEG	O
electrodes	O
'	O
signals	O
while	O
keeping	O
the	O
intrinsic	O
spatial	O
dependence	O
.	O
Then	O
we	O
design	O
a	O
pairwise	O
subnetwork	O
to	O
capture	O
the	O
discrepancy	O
information	O
between	O
two	O
hemispheres	O
and	O
extract	O
higher	O
-	O
level	O
features	O
for	O
final	O
classification	O
.	O
Besides	O
,	O
in	O
order	O
to	O
reduce	O
the	O
domain	O
shift	O
between	O
training	O
and	O
testing	O
data	O
,	O
we	O
use	O
a	O
domain	O
discriminator	O
that	O
adversarially	O
induces	O
the	O
over	O
all	O
feature	O
learning	O
module	O
to	O
generate	O
emotion	O
-	O
related	O
but	O
domain	O
-	O
invariant	O
feature	O
,	O
which	O
can	O
further	O
promote	O
EEG	O
emotion	B-RESEARCH_PROBLEM
recognition	E-RESEARCH_PROBLEM
.	O
We	O
conduct	O
experiments	O
on	O
three	O
public	O
EEG	O
emotional	O
datasets	O
,	O
and	O
the	O
experiments	O
show	O
that	O
the	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
can	O
be	O
achieved	O
.	O
I.	O
INTRODUCTION	O

Sentiment	B-RESEARCH_PROBLEM
classification	E-RESEARCH_PROBLEM
is	O
an	O
important	O
process	O
in	O
understanding	O
people	O
's	O
perception	O
towards	O
a	O
product	O
,	O
service	O
,	O
or	O
topic	O
.	O
Many	O
natural	O
language	O
processing	O
models	O
have	O
been	O
proposed	O
to	O
solve	O
the	O
sentiment	O
classification	O
problem	O
.	O
However	O
,	O
most	O
of	O
them	O
have	O
focused	O
on	O
binary	O
sentiment	O
classification	O
.	O
In	O
this	O
paper	O
,	O
we	O
use	O
a	O
promising	O
deep	O
learning	O
model	O
called	O
BERT	O
to	O
solve	O
the	O
fine	O
-	O
grained	O
sentiment	O
classification	O
task	O
.	O
Experiments	O
show	O
that	O
our	O
model	O
outperforms	O
other	O
popular	O
models	O
for	O
this	O
task	O
without	O
sophisticated	O
architecture	O
.	O
We	O
also	O
demonstrate	O
the	O
effectiveness	O
of	O
transfer	O
learning	O
in	O
natural	O
language	O
processing	O
in	O
the	O
process	O
.	O
I.	O
INTRODUCTION	O

Emotion	B-RESEARCH_PROBLEM
recognition	E-RESEARCH_PROBLEM
has	O
become	O
an	O
important	O
field	O
of	O
research	O
in	O
human	O
computer	O
interactions	O
and	O
there	O
is	O
a	O
growing	O
need	O
for	O
automatic	O
emotion	O
recognition	O
systems	O
.	O
One	O
of	O
the	O
directions	O
the	O
research	O
is	O
heading	O
is	O
the	O
use	O
of	O
neural	O
networks	O
which	O
are	O
adept	O
at	O
estimating	O
complex	O
functions	O
that	O
depend	O
on	O
a	O
large	O
number	O
and	O
diverse	O
source	O
of	O
input	O
data	O
.	O
In	O
this	O
paper	O
we	O
attempt	O
to	O
exploit	O
this	O
effectiveness	O
of	O
neural	O
networks	O
to	O
enable	O
us	O
to	O
perform	O
multimodal	O
emotion	O
recognition	O
on	O
IEMOCAP	O
dataset	O
using	O
data	O
from	O
speech	O
,	O
text	O
,	O
and	O
motions	O
captured	O
from	O
face	O
expressions	O
,	O
rotation	O
and	O
hand	O
movements	O
.	O
Our	O
approach	O
first	O
identifies	O
best	O
individual	O
architectures	O
for	O
classification	O
on	O
each	O
modality	O
and	O
performs	O
fusion	O
only	O
at	O
the	O
final	O
layer	O
which	O
allows	O
for	O
a	O
more	O
robust	O
and	O
accurate	O
emotion	O
detection	O
.	O
INTRODUCTION	O
Emotion	O
is	O
a	O
psycho-physiological	O
process	O
that	O
can	O
be	O
triggered	O
by	O
conscious	O
and	O
/	O
or	O
unconscious	O
perception	O
of	O
objects	O
and	O
situations	O
,	O
associated	O
with	O
multitude	O
of	O
factors	O
such	O
as	O
mood	O
,	O
temperament	O
,	O
personality	O
,	O
disposition	O
,	O
and	O
motivation	O
.	O
Emotions	O
are	O
very	O
important	O
in	O
human	O
decision	O
handling	O
,	O
interaction	O
and	O
cognitive	O
process	O
.	O

Identifying	B-RESEARCH_PROBLEM
emotion	I-RESEARCH_PROBLEM
from	I-RESEARCH_PROBLEM
speech	E-RESEARCH_PROBLEM
is	O
a	O
nontrivial	O
task	O
pertaining	O
to	O
the	O
ambiguous	O
definition	O
of	O
emotion	O
itself	O
.	O
In	O
this	O
work	O
,	O
we	O
adopt	O
a	O
featureengineering	O
based	O
approach	O
to	O
tackle	O
the	O
task	O
of	O
speech	O
emotion	O
recognition	O
.	O
Formalizing	O
our	O
problem	O
as	O
a	O
multi-class	O
classification	O
problem	O
,	O
we	O
compare	O
the	O
performance	O
of	O
two	O
categories	O
of	O
models	O
.	O
For	O
both	O
,	O
we	O
extract	O
eight	O
hand	O
-	O
crafted	O
features	O
from	O
the	O
audio	O
signal	O
.	O
In	O
the	O
first	O
approach	O
,	O
the	O
extracted	O
features	O
are	O
used	O
to	O
train	O
six	O
traditional	O
machine	O
learning	O
classifiers	O
,	O
whereas	O
the	O
second	O
approach	O
is	O
based	O
on	O
deep	O
learning	O
wherein	O
a	O
baseline	O
feed	O
-	O
forward	O
neural	O
network	O
and	O
an	O
LSTM	O
-	O
based	O
classifier	O
are	O
trained	O
over	O
the	O
same	O
features	O
.	O
In	O
order	O
to	O
resolve	O
ambiguity	O
in	O
communication	O
,	O
we	O
also	O
include	O
features	O
from	O
the	O
text	O
domain	O
.	O
We	O
report	O
accuracy	O
,	O
f-	O
score	O
,	O
precision	O
and	O
recall	O
for	O
the	O
different	O
experiment	O
settings	O
we	O
evaluated	O
our	O
models	O
in	O
.	O

Proposing	O
a	O
model	O
for	O
the	O
joint	O
task	O
of	O
aspect	B-RESEARCH_PROBLEM
term	I-RESEARCH_PROBLEM
extraction	E-RESEARCH_PROBLEM
and	O
aspect	O
polarity	O
classification	O
.	O
The	O
model	O
proposed	O
is	O
Chinese	O
language	O
-	O
oriented	O
and	O
applicable	O
to	O
the	O
English	O
language	O
,	O
with	O
the	O
ability	O
to	O
handle	O
both	O
Chinese	O
and	O
English	O
reviews	O
.	O
The	O
model	O
also	O
integrates	O
the	O
domain	O
-	O
adapted	O
BERT	O
model	O
for	O
enhancement	O
.	O
The	O
model	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
seven	O
ABSA	O
datasets	O
.	O
Aspect	B-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
based	I-RESEARCH_PROBLEM
sentiment	I-RESEARCH_PROBLEM
analysis	E-RESEARCH_PROBLEM
(	O
ABSA	O
)	O
task	O
is	O
a	O
multi	O
-	O
grained	O
task	O
of	O
natural	O
language	O
processing	O
and	O
consists	O
of	O
two	O
subtasks	O
:	O
aspect	B-RESEARCH_PROBLEM
term	I-RESEARCH_PROBLEM
extraction	E-RESEARCH_PROBLEM
(	O
ATE	O
)	O
and	O
aspect	O
polarity	O
classification	O
(	O
APC	S-RESEARCH_PROBLEM
)	O
.	O
Most	O
of	O
the	O
existing	O
work	O
focuses	O
on	O
the	O
subtask	O
of	O
aspect	O
term	O
polarity	O
inferring	O
and	O
ignores	O
the	O
significance	O
of	O
aspect	B-RESEARCH_PROBLEM
term	I-RESEARCH_PROBLEM
extraction	E-RESEARCH_PROBLEM
.	O
Besides	O
,	O
the	O
existing	O
researches	O
do	O
not	O
pay	O
attention	O
to	O
the	O
research	O
of	O
the	O
Chinese	O
-	O
oriented	O
ABSA	O
task	O
.	O

Inspired	O
by	O
recent	O
successes	O
of	O
deep	O
learning	O
in	O
computer	O
vision	O
,	O
we	O
propose	O
a	O
novel	O
application	O
of	O
deep	O
convolutional	O
neural	O
networks	O
to	O
facial	B-RESEARCH_PROBLEM
expression	I-RESEARCH_PROBLEM
recognition	E-RESEARCH_PROBLEM
,	O
in	O
particular	O
smile	O
recognition	O
.	O
A	O
smile	O
recognition	O
test	O
accuracy	O
of	O
99.	O
45	O
%	O
is	O
achieved	O
for	O
the	O
Denver	O
Intensity	O
of	O
Spontaneous	O
Facial	O
Action	O
(	O
DISFA	O
)	O
database	O
,	O
significantly	O
outperforming	O
existing	O
approaches	O
based	O
on	O
hand	O
-	O
crafted	O
features	O
with	O
accuracies	O
ranging	O
from	O
65	O
.	O
55	O
%	O
to	O
79.67	O
%	O
.	O
The	O
novelty	O
of	O
this	O
approach	O
includes	O
a	O
comprehensive	O
model	O
selection	O
of	O
the	O
architecture	O
parameters	O
,	O
allowing	O
to	O
find	O
an	O
appropriate	O
architecture	O
for	O
each	O
expression	O
such	O
as	O
smile	O
.	O
This	O
is	O
feasible	O
because	O
all	O
experiments	O
were	O
run	O
on	O
a	O
Tesla	O
K40c	O
GPU	O
,	O
allowing	O
a	O
speedup	O
of	O
factor	O
10	O
over	O
traditional	O
computations	O
on	O
a	O
CPU	O
.	O

We	O
present	O
CATENA	O
,	O
a	O
sieve	O
-	O
based	O
system	O
to	O
perform	O
temporal	B-RESEARCH_PROBLEM
and	I-RESEARCH_PROBLEM
causal	I-RESEARCH_PROBLEM
relation	I-RESEARCH_PROBLEM
extraction	I-RESEARCH_PROBLEM
and	I-RESEARCH_PROBLEM
classification	E-RESEARCH_PROBLEM
from	O
English	O
texts	O
,	O
exploiting	O
the	O
interaction	O
between	O
the	O
temporal	O
and	O
the	O
causal	O
model	O
.	O
We	O
evaluate	O
the	O
performance	O
of	O
each	O
sieve	O
,	O
showing	O
that	O
the	O
rule	O
-	O
based	O
,	O
the	O
machinelearned	O
and	O
the	O
reasoning	O
components	O
all	O
contribute	O
to	O
achieving	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
TempEval	O
-	O
3	O
and	O
TimeBank	O
-	O
Dense	O
data	O
.	O
Although	O
causal	O
relations	O
are	O
much	O
sparser	O
than	O
temporal	O
ones	O
,	O
the	O
architecture	O
and	O
the	O
selected	O
features	O
are	O
mostly	O
suitable	O
to	O
serve	O
both	O
tasks	O
.	O
The	O
effects	O
of	O
the	O
interaction	O
between	O
the	O
temporal	O
and	O
the	O
causal	O
components	O
,	O
although	O
limited	O
,	O
yield	O
promising	O
results	O
and	O
confirm	O
the	O
tight	O
connection	O
between	O
the	O
temporal	O
and	O
the	O
causal	O
dimension	O
of	O
texts	O
.	O
narrative	O
:	O
System	O
architecture	O
of	O
CATENA	O
The	O
problem	O
of	O
detecting	O
causality	O
between	O
events	O
is	O
as	O
challenging	O
as	O
recognizing	O
their	O
temporal	O
order	O
,	O
but	O
less	O
analyzed	O
from	O
an	O
NLP	O
perspective	O
.	O

Identifying	B-RESEARCH_PROBLEM
temporal	I-RESEARCH_PROBLEM
relations	I-RESEARCH_PROBLEM
between	I-RESEARCH_PROBLEM
events	E-RESEARCH_PROBLEM
is	O
an	O
essential	O
step	O
towards	O
natural	O
language	O
understanding	O
.	O
However	O
,	O
the	O
temporal	O
relation	O
between	O
two	O
events	O
in	O
a	O
story	O
depends	O
on	O
,	O
and	O
is	O
often	O
dictated	O
by	O
,	O
relations	O
among	O
other	O
events	O
.	O
Consequently	O
,	O
effectively	O
identifying	O
temporal	O
relations	O
between	O
events	O
is	O
a	O
challenging	O
problem	O
even	O
for	O
human	O
annotators	O
.	O
This	O
paper	O
suggests	O
that	O
it	O
is	O
important	O
to	O
take	O
these	O
dependencies	O
into	O
account	O
while	O
learning	O
to	O
identify	O
these	O
relations	O
and	O
proposes	O
a	O
structured	O
learning	O
approach	O
to	O
address	O
this	O
challenge	O
.	O
As	O
a	O
byproduct	O
,	O
this	O
provides	O
a	O
new	O
perspective	O
on	O
handling	O
missing	O
relations	O
,	O
a	O
known	O
issue	O
that	O
hurts	O
existing	O
methods	O
.	O
As	O
we	O
show	O
,	O
the	O
proposed	O
approach	O
results	O
in	O
significant	O
improvements	O
on	O
the	O
two	O
commonly	O
used	O
data	O
sets	O
for	O
this	O
problem	O
.	O

As	O
a	O
new	O
way	O
of	O
training	O
generative	O
models	O
,	O
Generative	O
Adversarial	O
Net	O
(	O
GAN	O
)	O
that	O
uses	O
a	O
discriminative	O
model	O
to	O
guide	O
the	O
training	O
of	O
the	O
generative	O
model	O
has	O
enjoyed	O
considerable	O
success	O
in	O
generating	B-RESEARCH_PROBLEM
real	I-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
valued	I-RESEARCH_PROBLEM
data	E-RESEARCH_PROBLEM
.	O
However	O
,	O
it	O
has	O
limitations	O
when	O
the	O
goal	O
is	O
for	O
generating	B-RESEARCH_PROBLEM
sequences	I-RESEARCH_PROBLEM
of	I-RESEARCH_PROBLEM
discrete	I-RESEARCH_PROBLEM
tokens	E-RESEARCH_PROBLEM
.	O
A	O
major	O
reason	O
lies	O
in	O
that	O
the	O
discrete	O
outputs	O
from	O
the	O
generative	O
model	O
make	O
it	O
difficult	O
to	O
pass	O
the	O
gradient	O
update	O
from	O
the	O
discriminative	O
model	O
to	O
the	O
generative	O
model	O
.	O
Also	O
,	O
the	O
discriminative	O
model	O
can	O
only	O
assess	O
a	O
complete	O
sequence	O
,	O
while	O
for	O
a	O
partially	O
generated	O
sequence	O
,	O
it	O
is	O
nontrivial	O
to	O
balance	O
its	O
current	O
score	O
and	O
the	O
future	O
one	O
once	O
the	O
entire	O
sequence	O
has	O
been	O
generated	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
sequence	O
generation	O
framework	O
,	O
called	O
SeqGAN	O
,	O
to	O
solve	O
the	O
problems	O
.	O
Modeling	O
the	O
data	O
generator	O
as	O
a	O
stochastic	O
policy	O
in	O
reinforcement	O
learning	O
(	O
RL	O
)	O
,	O
SeqGAN	O
bypasses	O
the	O
generator	O
differentiation	O
problem	O
by	O
directly	O
performing	O
gradient	O
policy	O
update	O
.	O
The	O
RL	O
reward	O
signal	O
comes	O
from	O
the	O
GAN	O
discriminator	O
judged	O
on	O
a	O
complete	O
sequence	O
,	O
and	O
is	O
passed	O
back	O
to	O
the	O
intermediate	O
state	O
-	O
action	O
steps	O
using	O
Monte	O
Carlo	O
search	O
.	O

Automatically	B-RESEARCH_PROBLEM
generating	I-RESEARCH_PROBLEM
coherent	I-RESEARCH_PROBLEM
and	I-RESEARCH_PROBLEM
semantically	I-RESEARCH_PROBLEM
meaningful	I-RESEARCH_PROBLEM
text	E-RESEARCH_PROBLEM
has	O
many	O
applications	O
in	O
machine	O
translation	O
,	O
dialogue	O
systems	O
,	O
image	O
captioning	O
,	O
etc	O
.	O
Recently	O
,	O
by	O
combining	O
with	O
policy	O
gradient	O
,	O
Generative	O
Adversarial	O
Nets	O
(	O
GAN	O
)	O
that	O
use	O
a	O
discriminative	O
model	O
to	O
guide	O
the	O
training	O
of	O
the	O
generative	O
model	O
as	O
a	O
reinforcement	O
learning	O
policy	O
has	O
shown	O
promising	O
results	O
in	O
text	O
generation	O
.	O
However	O
,	O
the	O
scalar	O
guiding	O
signal	O
is	O
only	O
available	O
after	O
the	O
entire	O
text	O
has	O
been	O
generated	O
and	O
lacks	O
intermediate	O
information	O
about	O
text	O
structure	O
during	O
the	O
generative	O
process	O
.	O
As	O
such	O
,	O
it	O
limits	O
its	O
success	O
when	O
the	O
length	O
of	O
the	O
generated	O
text	O
samples	O
is	O
long	O
(	O
more	O
than	O
20	O
words	O
)	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
new	O
framework	O
,	O
called	O
LeakGAN	O
,	O
to	O
address	O
the	O
problem	O
for	O
long	O
text	O
generation	O
.	O
We	O
allow	O
the	O
discriminative	O
net	O
to	O
leak	O
its	O
own	O
high	O
-	O
level	O
extracted	O
features	O
to	O
the	O
generative	O
net	O
to	O
further	O
help	O
the	O
guidance	O
.	O
The	O
generator	O
incorporates	O
such	O
informative	O
signals	O
into	O
all	O
generation	O
steps	O
through	O
an	O
additional	O
MANAGER	O
module	O
,	O
which	O
takes	O
the	O
extracted	O
features	O
of	O
current	O
generated	O
words	O
and	O
outputs	O
a	O
latent	O
vector	O
to	O
guide	O
the	O
WORKER	O
module	O
for	O
next	O
-	O
word	O
generation	O
.	O

Recent	O
work	O
on	O
generative	B-RESEARCH_PROBLEM
text	I-RESEARCH_PROBLEM
modeling	E-RESEARCH_PROBLEM
has	O
found	O
that	O
variational	O
autoencoders	O
(	O
VAE	O
)	O
with	O
LSTM	O
decoders	O
perform	O
worse	O
than	O
simpler	O
LSTM	O
language	O
models	O
(	O
Bowman	O
et	O
al.	O
,	O
2015	O
)	O
.	O
This	O
negative	O
result	O
is	O
so	O
far	O
poorly	O
understood	O
,	O
but	O
has	O
been	O
attributed	O
to	O
the	O
propensity	O
of	O
LSTM	O
decoders	O
to	O
ignore	O
conditioning	O
information	O
from	O
the	O
encoder	O
.	O
In	O
this	O
paper	O
,	O
we	O
experiment	O
with	O
a	O
new	O
type	O
of	O
decoder	O
for	O
VAE	O
:	O
a	O
dilated	O
CNN	O
.	O
By	O
changing	O
the	O
decoder	O
's	O
dilation	O
architecture	O
,	O
we	O
control	O
the	O
size	O
of	O
context	O
from	O
previously	O
generated	O
words	O
.	O
In	O
experiments	O
,	O
we	O
find	O
that	O
there	O
is	O
a	O
trade	O
-	O
off	O
between	O
contextual	O
capacity	O
of	O
the	O
decoder	O
and	O
effective	O
use	O
of	O
encoding	O
information	O
.	O
We	O
show	O
that	O
when	O
carefully	O
managed	O
,	O
VAEs	O
can	O
outperform	O
LSTM	O
language	O
models	O
.	O
We	O
demonstrate	O
perplexity	O
gains	O
on	O
two	O
datasets	O
,	O
representing	O
the	O
first	O
positive	O
language	O
modeling	O
result	O
with	O
VAE	O
.	O

In	O
neural	B-RESEARCH_PROBLEM
abstractive	I-RESEARCH_PROBLEM
summarization	E-RESEARCH_PROBLEM
field	O
,	O
conventional	O
sequence	O
-	O
to	O
-	O
sequence	O
based	O
models	O
often	O
suffer	O
from	O
summarizing	O
the	O
wrong	O
aspect	O
of	O
the	O
document	O
with	O
respect	O
to	O
the	O
main	O
aspect	O
.	O
To	O
tackle	O
this	O
problem	O
,	O
we	O
propose	O
the	O
task	O
of	O
reader	B-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
aware	I-RESEARCH_PROBLEM
abstractive	I-RESEARCH_PROBLEM
summary	I-RESEARCH_PROBLEM
generation	E-RESEARCH_PROBLEM
,	O
which	O
utilizes	O
the	O
reader	O
comments	O
to	O
help	O
the	O
model	O
produce	O
better	O
summary	O
about	O
the	O
main	O
aspect	O
.	O
Unlike	O
traditional	O
abstractive	B-RESEARCH_PROBLEM
summarization	E-RESEARCH_PROBLEM
task	O
,	O
reader	O
-	O
aware	O
summarization	O
confronts	O
two	O
main	O
challenges	O
:	O
(	O
1	O
)	O
Comments	O
are	O
informal	O
and	O
noisy	O
;	O
(	O
2	O
)	O
jointly	O
modeling	O
the	O
news	O
document	O
and	O
the	O
reader	O
comments	O
is	O
challenging	O
.	O
To	O
tackle	O
the	O
above	O
challenges	O
,	O
we	O
design	O
an	O
adversarial	O
learning	O
model	O
named	O
reader	O
-	O
aware	O
summary	O
generator	O
(	O
RASG	O
)	O
,	O
which	O
consists	O
of	O
four	O
components	O
:	O
(	O
1	O
)	O
a	O
sequence	O
-	O
to	O
-	O
sequence	O
based	O
summary	O
generator	O
;	O
(	O
2	O
)	O
a	O
reader	O
attention	O
module	O
capturing	O
the	O
reader	O
focused	O
aspects	O
;	O

Generating	B-RESEARCH_PROBLEM
diverse	I-RESEARCH_PROBLEM
sequences	E-RESEARCH_PROBLEM
is	O
important	O
in	O
many	O
NLP	O
applications	O
such	O
as	O
question	O
generation	O
or	O
summarization	O
that	O
exhibit	O
semantically	O
one	O
-	O
to	O
-	O
many	O
relationships	O
between	O
source	O
and	O
the	O
target	O
sequences	O
.	O
We	O
present	O
a	O
method	O
to	O
explicitly	O
separate	O
diversification	O
from	O
generation	O
using	O
a	O
general	O
plug	O
-	O
and	O
-	O
play	O
module	O
(	O
called	O
SELECTOR	O
)	O
that	O
wraps	O
around	O
and	O
guides	O
an	O
existing	O
encoder	O
-	O
decoder	O
model	O
.	O
The	O
diversification	O
stage	O
uses	O
a	O
mixture	O
of	O
experts	O
to	O
sample	O
different	O
binary	O
masks	O
on	O
the	O
source	O
sequence	O
for	O
diverse	O
content	O
selection	O
.	O
The	O
generation	O
stage	O
uses	O
a	O
standard	O
encoder	O
-	O
decoder	O
model	O
given	O
each	O
selected	O
content	O
from	O
the	O
source	O
sequence	O
.	O
Due	O
to	O
the	O
non-differentiable	O
nature	O
of	O
discrete	O
sampling	O
and	O
the	O
lack	O
of	O
ground	O
truth	O
labels	O
for	O
binary	O
mask	O
,	O
we	O
leverage	O
a	O
proxy	O
for	O
ground	O
-	O
truth	O
mask	O
and	O
adopt	O
stochastic	O
hard	O
-	O
EM	O
for	O
training	O
.	O
In	O
question	O
generation	O
(	O
SQuAD	O
)	O
and	O
abstractive	O
summarization	O
(	O
CNN	O
-	O
DM	O
)	O
,	O
our	O
method	O
demonstrates	O
significant	O
improvements	O
in	O
accuracy	O
,	O
diversity	O
and	O
training	O
efficiency	O
,	O
including	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
top	O
-	O
1	O
accuracy	O
in	O
both	O
datasets	O
,	O
6	O
%	O
gain	O
in	O
top	O
-	O
5	O
accuracy	O
,	O
and	O
3.7	O
times	O
faster	O
training	O
over	O
a	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
model	O
.	O
Our	O
code	O
is	O
publicly	O
available	O
at	O
https://github.com/	O
clovaai/FocusSeq2Seq.	O

An	O
accurate	O
abstractive	O
summary	O
of	O
a	O
document	O
should	O
contain	O
all	O
its	O
salient	O
information	O
and	O
should	O
be	O
logically	O
entailed	O
by	O
the	O
input	O
document	O
.	O
We	O
improve	O
these	O
important	O
aspects	O
of	O
abstractive	B-RESEARCH_PROBLEM
summarization	E-RESEARCH_PROBLEM
via	O
multi-task	O
learning	O
with	O
the	O
auxiliary	O
tasks	O
of	O
question	O
generation	O
and	O
entailment	O
generation	O
,	O
where	O
the	O
former	O
teaches	O
the	O
summarization	O
model	O
how	O
to	O
look	O
for	O
salient	O
questioning	O
-	O
worthy	O
details	O
,	O
and	O
the	O
latter	O
teaches	O
the	O
model	O
how	O
to	O
rewrite	O
a	O
summary	O
which	O
is	O
a	O
directed	O
-	O
logical	O
subset	O
of	O
the	O
input	O
document	O
.	O
We	O
also	O
propose	O
novel	O
multitask	O
architectures	O
with	O
high	O
-	O
level	O
(	O
semantic	O
)	O
layer	O
-	O
specific	O
sharing	O
across	O
multiple	O
encoder	O
and	O
decoder	O
layers	O
of	O
the	O
three	O
tasks	O
,	O
as	O
well	O
as	O
soft	O
-	O
sharing	O
mechanisms	O
(	O
and	O
show	O
performance	O
ablations	O
and	O
analysis	O
examples	O
of	O
each	O
contribution	O
)	O
.	O
Overall	O
,	O
we	O
achieve	O
statistically	O
significant	O
improvements	O
over	O
the	O
state	O
-	O
of	O
the	O
-	O
art	O
on	O
both	O
the	O
CNN	O
/	O
DailyMail	O
and	O
Gigaword	O
datasets	O
,	O
as	O
well	O
as	O
on	O
the	O
DUC	O
-	O
2002	O
transfer	O
setup	O
.	O
We	O
also	O
present	O
several	O
quantitative	O
and	O
qualitative	O
analysis	O
studies	O
of	O
our	O
model	O
's	O
learned	O
saliency	O
and	O
entailment	O
skills	O
.	O

In	O
neural	B-RESEARCH_PROBLEM
abstractive	I-RESEARCH_PROBLEM
summarization	E-RESEARCH_PROBLEM
,	O
the	O
conventional	O
sequence	O
-	O
to	O
-	O
sequence	O
(	O
seq2seq	O
)	O
model	O
often	O
suffers	O
from	O
repetition	O
and	O
semantic	O
irrelevance	O
.	O
To	O
tackle	O
the	O
problem	O
,	O
we	O
propose	O
a	O
global	O
encoding	O
framework	O
,	O
which	O
controls	O
the	O
information	O
flow	O
from	O
the	O
encoder	O
to	O
the	O
decoder	O
based	O
on	O
the	O
global	O
information	O
of	O
the	O
source	O
context	O
.	O
It	O
consists	O
of	O
a	O
convolutional	O
gated	O
unit	O
to	O
perform	O
global	O
encoding	O
to	O
improve	O
the	O
representations	O
of	O
the	O
source	O
-	O
side	O
information	O
.	O
Evaluations	O
on	O
the	O
LCSTS	O
and	O
the	O
English	O
Gigaword	O
both	O
demonstrate	O
that	O
our	O
model	O
outperforms	O
the	O
baseline	O
models	O
,	O
and	O
the	O
analysis	O
shows	O
that	O
our	O
model	O
is	O
capable	O
of	O
generating	O
summary	O
of	O
higher	O
quality	O
and	O
reducing	O
repetition	O
1	O
.	O

We	O
propose	O
a	O
selective	O
encoding	O
model	O
to	O
extend	O
the	O
sequence	O
-	O
to	O
-	O
sequence	O
framework	O
for	O
abstractive	O
sentence	B-RESEARCH_PROBLEM
summarization	E-RESEARCH_PROBLEM
.	O
It	O
consists	O
of	O
a	O
sentence	O
encoder	O
,	O
a	O
selective	O
gate	O
network	O
,	O
and	O
an	O
attention	O
equipped	O
decoder	O
.	O
The	O
sentence	O
encoder	O
and	O
decoder	O
are	O
built	O
with	O
recurrent	O
neural	O
networks	O
.	O
The	O
selective	O
gate	O
network	O
constructs	O
a	O
second	O
level	O
sentence	O
representation	O
by	O
controlling	O
the	O
information	O
flow	O
from	O
encoder	O
to	O
decoder	O
.	O
The	O
second	O
level	O
representation	O
is	O
tailored	O
for	O
sentence	B-RESEARCH_PROBLEM
summarization	E-RESEARCH_PROBLEM
task	O
,	O
which	O
leads	O
to	O
better	O
performance	O
.	O
We	O
evaluate	O
our	O
model	O
on	O
the	O
English	O
Gigaword	O
,	O
DUC	O
2004	O
and	O
MSR	O
abstractive	O
sentence	B-RESEARCH_PROBLEM
summarization	E-RESEARCH_PROBLEM
datasets	O
.	O
The	O
experimental	O
results	O
show	O
that	O
the	O
proposed	O
selective	O
encoding	O
model	O
outperforms	O
the	O
state	O
-	O
of	O
the	O
-	O
art	O
baseline	O
models	O
.	O

In	O
this	O
paper	O
,	O
we	O
investigate	O
the	O
sentence	B-RESEARCH_PROBLEM
summarization	E-RESEARCH_PROBLEM
task	O
that	O
produces	O
a	O
summary	O
from	O
a	O
source	O
sentence	O
.	O
Neural	O
sequence	O
-	O
to	O
-	O
sequence	O
models	O
have	O
gained	O
considerable	O
success	O
for	O
this	O
task	O
,	O
while	O
most	O
existing	O
approaches	O
only	O
focus	O
on	O
improving	O
word	O
overlap	O
between	O
the	O
generated	O
summary	O
and	O
the	O
reference	O
,	O
which	O
ignore	O
the	O
correctness	O
,	O
i.e.	O
,	O
the	O
summary	O
should	O
not	O
contain	O
error	O
messages	O
with	O
respect	O
to	O
the	O
source	O
sentence	O
.	O
We	O
argue	O
that	O
correctness	O
is	O
an	O
essential	O
requirement	O
for	O
summarization	O
systems	O
.	O
Considering	O
a	O
correct	O
summary	O
is	O
semantically	O
entailed	O
by	O
the	O
source	O
sentence	O
,	O
we	O
incorporate	O
entailment	O
knowledge	O
into	O
abstractive	O
summarization	O
models	O
.	O
We	O
propose	O
an	O
entailment	O
-	O
aware	O
encoder	O
under	O
multi-task	O
framework	O
(	O
i.e.	O
,	O
summarization	O
generation	O
and	O
entailment	O
recognition	O
)	O
and	O
an	O
entailment	O
-	O
aware	O
decoder	O
by	O
entailment	O
Reward	O
Augmented	O
Maximum	O
Likelihood	O
(	O
RAML	O
)	O
training	O
.	O
Experimental	O
results	O
demonstrate	O
that	O
our	O
models	O
significantly	O
outperform	O
baselines	O
from	O
the	O
aspects	O
of	O
informativeness	O
and	O
correctness	O
.	O
This	O
work	O
is	O
licensed	O
under	O
a	O
Creative	O
Commons	O
Attribution	O
4.0	O
International	O
License	O
.	O

Seq2seq	O
learning	O
has	O
produced	O
promising	O
results	O
on	O
summarization	S-RESEARCH_PROBLEM
.	O
However	O
,	O
in	O
many	O
cases	O
,	O
system	O
summaries	O
still	O
struggle	O
to	O
keep	O
the	O
meaning	O
of	O
the	O
original	O
intact	O
.	O
They	O
may	O
miss	O
out	O
important	O
words	O
or	O
relations	O
that	O
play	O
critical	O
roles	O
in	O
the	O
syntactic	O
structure	O
of	O
source	O
sentences	O
.	O
In	O
this	O
paper	O
,	O
we	O
present	O
structure	O
-	O
infused	O
copy	O
mechanisms	O
to	O
facilitate	O
copying	O
important	O
words	O
and	O
relations	O
from	O
the	O
source	O
sentence	O
to	O
summary	O
sentence	O
.	O
The	O
approach	O
naturally	O
combines	O
source	O
dependency	O
structure	O
with	O
the	O
copy	O
mechanism	O
of	O
an	O
abstractive	O
sentence	O
summarizer	O
.	O
Experimental	O
results	O
demonstrate	O
the	O
effectiveness	O
of	O
incorporating	O
source	O
-	O
side	O
syntactic	O
information	O
in	O
the	O
system	O
,	O
and	O
our	O
proposed	O
approach	O
compares	O
favorably	O
to	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
methods	O
.	O

Abstractive	B-RESEARCH_PROBLEM
Sentence	I-RESEARCH_PROBLEM
Summarization	E-RESEARCH_PROBLEM
generates	O
a	O
shorter	O
version	O
of	O
a	O
given	O
sentence	O
while	O
attempting	O
to	O
preserve	O
its	O
meaning	O
.	O
We	O
introduce	O
a	O
conditional	O
recurrent	O
neural	O
network	O
(	O
RNN	O
)	O
which	O
generates	O
a	O
summary	O
of	O
an	O
input	O
sentence	O
.	O
The	O
conditioning	O
is	O
provided	O
by	O
a	O
novel	O
convolutional	O
attention	O
-	O
based	O
encoder	O
which	O
ensures	O
that	O
the	O
decoder	O
focuses	O
on	O
the	O
appropriate	O
input	O
words	O
at	O
each	O
step	O
of	O
generation	O
.	O
Our	O
model	O
relies	O
only	O
on	O
learned	O
features	O
and	O
is	O
easy	O
to	O
train	O
in	O
an	O
end	O
-	O
to	O
-	O
end	O
fashion	O
on	O
large	O
data	O
sets	O
.	O
Our	O
experiments	O
show	O
that	O
the	O
model	O
significantly	O
outperforms	O
the	O
recently	O
proposed	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
method	O
on	O
the	O
Gigaword	O
corpus	O
while	O
performing	O
competitively	O
on	O
the	O
DUC	O
-	O
2004	O
shared	O
task	O
.	O

Majority	O
of	O
the	O
text	O
modelling	O
techniques	O
yield	O
only	O
point	O
-	O
estimates	O
of	O
document	O
embeddings	O
and	O
lack	O
in	O
capturing	O
the	O
uncertainty	O
of	O
the	O
estimates	O
.	O
These	O
uncertainties	O
give	O
a	O
notion	O
of	O
how	O
well	O
the	O
embeddings	O
represent	O
a	O
document	O
.	O
We	O
present	O
Bayesian	O
subspace	O
multinomial	O
model	O
(	O
Bayesian	O
SMM	O
)	O
,	O
a	O
generative	O
log	O
-	O
linear	O
model	O
that	O
learns	O
to	O
represent	O
documents	O
in	O
the	O
form	O
of	O
Gaussian	O
distributions	O
,	O
thereby	O
encoding	O
the	O
uncertainty	O
in	O
its	O
covariance	O
.	O
Additionally	O
,	O
in	O
the	O
proposed	O
Bayesian	O
SMM	O
,	O
we	O
address	O
a	O
commonly	O
encountered	O
problem	O
of	O
intractability	O
that	O
appears	O
during	O
variational	O
inference	O
in	O
mixed	O
-	O
logit	O
models	O
.	O
We	O
also	O
present	O
a	O
generative	O
Gaussian	O
linear	O
classifier	O
for	O
topic	B-RESEARCH_PROBLEM
identification	E-RESEARCH_PROBLEM
that	O
exploits	O
the	O
uncertainty	O
in	O
document	O
embeddings	O
.	O
Our	O
intrinsic	O
evaluation	O
using	O
perplexity	O
measure	O
shows	O
that	O
the	O
proposed	O
Bayesian	O
SMM	O
fits	O
the	O
data	O
better	O
as	O
compared	O
to	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
neural	O
variational	O
document	O
model	O
on	O
(	O
Fisher	O
)	O
speech	O
and	O
(	O
20	O
Newsgroups	O
)	O
text	O
corpora	O
.	O
Our	O
topic	B-RESEARCH_PROBLEM
identification	E-RESEARCH_PROBLEM
experiments	O
show	O
that	O
the	O
proposed	O
systems	O
are	O
robust	O
to	O
over-fitting	O
on	O
unseen	O
test	O
data	O
.	O

