We	O
present	O
a	O
novel	O
neural	O
network	O
for	O
processing	O
sequences	O
.	O
The	O
ByteNet	O
is	O
a	O
one-dimensional	O
convolutional	O
neural	O
network	O
that	O
is	O
composed	O
of	O
two	O
parts	O
,	O
one	O
to	O
encode	O
the	O
source	O
sequence	O
and	O
the	O
other	O
to	O
decode	O
the	O
target	O
sequence	O
.	O
The	O
two	O
network	O
parts	O
are	O
connected	O
by	O
stacking	O
the	O
decoder	O
on	O
top	O
of	O
the	O
encoder	O
and	O
preserving	O
the	O
temporal	O
resolution	O
of	O
the	O
sequences	O
.	O
To	O
address	O
the	O
differing	O
lengths	O
of	O
the	O
source	O
and	O
the	O
target	O
,	O
we	O
introduce	O
an	O
efficient	O
mechanism	O
by	O
which	O
the	O
decoder	O
is	O
dynamically	O
unfolded	O
over	O
the	O
representation	O
of	O
the	O
encoder	O
.	O
The	O
ByteNet	O
uses	O
dilation	O
in	O
the	O
convolutional	O
layers	O
to	O
increase	O
its	O
receptive	O
field	O
.	O
The	O
resulting	O
network	O
has	O
two	O
core	O
properties	O
:	O
it	O
runs	O
in	O
time	O
that	O
is	O
linear	O
in	O
the	O
length	O
of	O
the	O
sequences	O
and	O
it	O
sidesteps	O
the	O
need	O
for	O
excessive	O
memorization	O
.	O
The	O
ByteNet	O
decoder	O
attains	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
character	B-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
level	I-RESEARCH_PROBLEM
language	I-RESEARCH_PROBLEM
modelling	E-RESEARCH_PROBLEM
and	O
outperforms	O
the	O
previous	O
best	O
results	O
obtained	O
with	O
recurrent	O
networks	O
.	O

The	O
dominant	O
sequence	O
transduction	O
models	O
are	O
based	O
on	O
complex	O
recurrent	O
or	O
convolutional	O
neural	O
networks	O
that	O
include	O
an	O
encoder	O
and	O
a	O
decoder	O
.	O
The	O
best	O
performing	O
models	O
also	O
connect	O
the	O
encoder	O
and	O
decoder	O
through	O
an	O
attention	O
mechanism	O
.	O
We	O
propose	O
a	O
new	O
simple	O
network	O
architecture	O
,	O
the	O
Transformer	O
,	O
based	O
solely	O
on	O
attention	B-RESEARCH_PROBLEM
mechanisms	E-RESEARCH_PROBLEM
,	O
dispensing	O
with	O
recurrence	O
and	O
convolutions	O
entirely	O
.	O
Experiments	O
on	O
two	O
machine	O
translation	O
tasks	O
show	O
these	O
models	O
to	O
be	O
superior	O
in	O
quality	O
while	O
being	O
more	O
parallelizable	O
and	O
requiring	O
significantly	O
less	O
time	O
to	O
train	O
.	O
Our	O
model	O
achieves	O
28.4	O
BLEU	O
on	O
the	O
WMT	O
2014	O
Englishto	O
-	O
German	O
translation	O
task	O
,	O
improving	O
over	O
the	O
existing	O
best	O
results	O
,	O
including	O
ensembles	O
,	O
by	O
over	O
2	O
BLEU	O
.	O
On	O
the	O
WMT	O
2014	O
English	O
-	O
to	O
-	O
French	O
translation	O
task	O
,	O
our	O
model	O
establishes	O
a	O
new	O
single	O
-	O
model	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
BLEU	O
score	O
of	O
41.8	O
after	O
training	O
for	O
3.5	O
days	O
on	O
eight	O
GPUs	O
,	O
a	O
small	O
fraction	O
of	O
the	O
training	O
costs	O
of	O
the	O
best	O
models	O
from	O
the	O
literature	O
.	O
We	O
show	O
that	O
the	O
Transformer	O
generalizes	O
well	O
to	O
other	O
tasks	O
by	O
applying	O
it	O
successfully	O
to	O
English	O
constituency	O
parsing	O
both	O
with	O
large	O
and	O
limited	O
training	O
data	O
.	O

Neural	B-RESEARCH_PROBLEM
machine	I-RESEARCH_PROBLEM
translation	E-RESEARCH_PROBLEM
(	O
NT12	O
)	O
aims	O
at	O
solving	O
machine	B-RESEARCH_PROBLEM
translation	E-RESEARCH_PROBLEM
(	O
MT	S-RESEARCH_PROBLEM
)	O
problems	O
using	O
neural	O
networks	O
and	O
has	O
exhibited	O
promising	O
results	O
in	O
recent	O
years	O
.	O
However	O
,	O
most	O
of	O
the	O
existing	O
NMT	S-RESEARCH_PROBLEM
models	O
are	O
shallow	O
and	O
there	O
is	O
still	O
a	O
performance	O
gap	O
between	O
a	O
single	O
NMT	S-RESEARCH_PROBLEM
model	O
and	O
the	O
best	O
conventional	O
MT	S-RESEARCH_PROBLEM
system	O
.	O
In	O
this	O
work	O
,	O
we	O
introduce	O
a	O
new	O
type	O
of	O
linear	O
connections	O
,	O
named	O
fastforward	O
connections	O
,	O
based	O
on	O
deep	O
Long	O
Short	O
-	O
Term	O
Memory	O
(	O
LSTM	O
)	O
networks	O
,	O
and	O
an	O
interleaved	O
bi-directional	O
architecture	O
for	O
stacking	O
the	O
LSTM	O
layers	O
.	O
Fast	O
-	O
forward	O
connections	O
play	O
an	O
essential	O
role	O
in	O
propagating	O
the	O
gradients	O
and	O
building	O
a	O
deep	O
topology	O
of	O
depth	O
16	O
.	O
On	O
the	O
WMT	O
'	O
14	O
Englishto	O
-	O
French	O
task	O
,	O
we	O
achieve	O
BLEU	O
=	O
37.7	O
with	O
a	O
single	O
attention	O
model	O
,	O
which	O
outperforms	O
the	O
corresponding	O
single	O
shallow	O
model	O
by	O
6.2	O
BLEU	O
points	O
.	O
This	O
is	O
the	O
first	O
time	O
that	O
a	O
single	O
NMT	S-RESEARCH_PROBLEM
model	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
and	O
outperforms	O
the	O
best	O
conventional	O
model	O
by	O
0.7	O
BLEU	O
points	O
.	O
We	O
can	O
still	O
achieve	O
BLEU	O
=	O
36.3	O
even	O
without	O
using	O
an	O
attention	O
mechanism	O
.	O

Unsupervised	B-RESEARCH_PROBLEM
neural	I-RESEARCH_PROBLEM
machine	I-RESEARCH_PROBLEM
translation	E-RESEARCH_PROBLEM
(	O
NMT	S-RESEARCH_PROBLEM
)	O
is	O
a	O
recently	O
proposed	O
approach	O
for	O
machine	B-RESEARCH_PROBLEM
translation	E-RESEARCH_PROBLEM
which	O
aims	O
to	O
train	O
the	O
model	O
without	O
using	O
any	O
labeled	O
data	O
.	O
The	O
models	O
proposed	O
for	O
unsupervised	B-RESEARCH_PROBLEM
NMT	E-RESEARCH_PROBLEM
often	O
use	O
only	O
one	O
shared	O
encoder	O
to	O
map	O
the	O
pairs	O
of	O
sentences	O
from	O
different	O
languages	O
to	O
a	O
shared	O
-	O
latent	O
space	O
,	O
which	O
is	O
weak	O
in	O
keeping	O
the	O
unique	O
and	O
internal	O
characteristics	O
of	O
each	O
language	O
,	O
such	O
as	O
the	O
style	O
,	O
terminology	O
,	O
and	O
sentence	O
structure	O
.	O
To	O
address	O
this	O
issue	O
,	O
we	O
introduce	O
an	O
extension	O
by	O
utilizing	O
two	O
independent	O
encoders	O
but	O
sharing	O
some	O
partial	O
weights	O
which	O
are	O
responsible	O
for	O
extracting	O
high	O
-	O
level	O
representations	O
of	O
the	O
input	O
sentences	O
.	O
Besides	O
,	O
two	O
different	O
generative	O
adversarial	O
networks	O
(	O
GANs	O
)	O
,	O
namely	O
the	O
local	O
GAN	O
and	O
global	O
GAN	O
,	O
are	O
proposed	O
to	O
enhance	O
the	O
cross	O
-	O
language	O
translation	O
.	O
With	O
this	O
new	O
approach	O
,	O
we	O
achieve	O
significant	O
improvements	O
on	O
English	O
-	O
German	O
,	O
English	O
-	O
French	O
and	O
Chinese	O
-	O
to	O
-	O
English	O
translation	O
tasks	O
.	O

The	O
paper	O
describes	O
the	O
development	O
process	O
of	O
the	O
Tilde	O
's	O
NMT	S-RESEARCH_PROBLEM
systems	O
that	O
were	O
submitted	O
for	O
the	O
WMT	O
2018	O
shared	O
task	O
on	O
news	B-RESEARCH_PROBLEM
translation	E-RESEARCH_PROBLEM
.	O
We	O
describe	O
the	O
data	O
filtering	O
and	O
pre-processing	O
workflows	O
,	O
the	O
NMT	S-RESEARCH_PROBLEM
system	O
training	O
architectures	O
,	O
and	O
automatic	O
evaluation	O
results	O
.	O
For	O
the	O
WMT	O
2018	O
shared	O
task	O
,	O
we	O
submitted	O
seven	O
systems	O
(	O
both	O
constrained	O
and	O
unconstrained	O
)	O
for	O
English	O
-	O
Estonian	O
and	O
Estonian	O
-	O
English	O
translation	O
directions	O
.	O
The	O
submitted	O
systems	O
were	O
trained	O
using	O
Transformer	O
models	O
.	O

Continuous	O
word	O
representation	O
(	O
aka	O
word	O
embedding	O
)	O
is	O
a	O
basic	O
building	O
block	O
in	O
many	O
neural	O
network	O
-	O
based	O
models	O
used	O
in	O
natural	O
language	O
processing	O
tasks	O
.	O
Although	O
it	O
is	O
widely	O
accepted	O
that	O
words	O
with	O
similar	O
semantics	O
should	O
be	O
close	O
to	O
each	O
other	O
in	O
the	O
embedding	O
space	O
,	O
we	O
find	O
that	O
word	B-RESEARCH_PROBLEM
embeddings	I-RESEARCH_PROBLEM
learned	I-RESEARCH_PROBLEM
in	I-RESEARCH_PROBLEM
several	I-RESEARCH_PROBLEM
tasks	I-RESEARCH_PROBLEM
are	I-RESEARCH_PROBLEM
biased	I-RESEARCH_PROBLEM
towards	I-RESEARCH_PROBLEM
word	I-RESEARCH_PROBLEM
frequency	E-RESEARCH_PROBLEM
:	O
the	O
embeddings	O
of	O
highfrequency	O
and	O
low	O
-	O
frequency	O
words	O
lie	O
in	O
different	O
subregions	O
of	O
the	O
embedding	O
space	O
,	O
and	O
the	O
embedding	O
of	O
a	O
rare	O
word	O
and	O
a	O
popular	O
word	O
can	O
be	O
far	O
from	O
each	O
other	O
even	O
if	O
they	O
are	O
semantically	O
similar	O
.	O
This	O
makes	O
learned	O
word	O
embeddings	O
ineffective	O
,	O
especially	O
for	O
rare	O
words	O
,	O
and	O
consequently	O
limits	O
the	O
performance	O
of	O
these	O
neural	O
network	O
models	O
.	O
In	O
this	O
paper	O
,	O
we	O
develop	O
a	O
neat	O
,	O
simple	O
yet	O
effective	O
way	O
to	O
learn	O
FRequency	O
-	O
AGnostic	O
word	O
Embedding	O
(	O
FRAGE	O
)	O
using	O
adversarial	O
training	O
.	O
We	O
conducted	O
comprehensive	O
studies	O
on	O
ten	O
datasets	O
across	O
four	O
natural	O
language	O
processing	O
tasks	O
,	O
including	O
word	O
similarity	O
,	O
language	O
modeling	O
,	O
machine	O
translation	O
and	O
text	O
classification	O
.	O
Results	O
show	O
that	O
with	O
FRAGE	O
,	O
we	O
achieve	O
higher	O
performance	O
than	O
the	O
baselines	O
in	O
all	O
tasks	O
.	O

The	O
capacity	O
of	O
a	O
neural	O
network	O
to	O
absorb	O
information	O
is	O
limited	O
by	O
its	O
number	O
of	O
parameters	O
.	O
Conditional	B-RESEARCH_PROBLEM
computation	E-RESEARCH_PROBLEM
,	O
where	O
parts	O
of	O
the	O
network	O
are	O
active	O
on	O
a	O
per-example	O
basis	O
,	O
has	O
been	O
proposed	O
in	O
theory	O
as	O
away	O
of	O
dramatically	O
increasing	B-RESEARCH_PROBLEM
model	I-RESEARCH_PROBLEM
capacity	I-RESEARCH_PROBLEM
without	I-RESEARCH_PROBLEM
a	I-RESEARCH_PROBLEM
proportional	I-RESEARCH_PROBLEM
increase	I-RESEARCH_PROBLEM
in	I-RESEARCH_PROBLEM
computation	E-RESEARCH_PROBLEM
.	O
In	O
practice	O
,	O
however	O
,	O
there	O
are	O
significant	O
algorithmic	O
and	O
performance	O
challenges	O
.	O
In	O
this	O
work	O
,	O
we	O
address	O
these	O
challenges	O
and	O
finally	O
realize	O
the	O
promise	O
of	O
conditional	O
computation	O
,	O
achieving	O
greater	O
than	O
1000x	O
improvements	O
in	O
model	O
capacity	O
with	O
only	O
minor	O
losses	O
in	O
computational	O
efficiency	O
on	O
modern	O
GPU	O
clusters	O
.	O
We	O
introduce	O
a	O
Sparsely	O
-	O
Gated	O
Mixture	O
-	O
of	O
-	O
Experts	O
layer	O
(	O
MoE	O
)	O
,	O
consisting	O
of	O
up	O
to	O
thousands	O
of	O
feed	O
-	O
forward	O
sub	O
-	O
networks	O
.	O
A	O
trainable	O
gating	O
network	O
determines	O
a	O
sparse	O
combination	O
of	O
these	O
experts	O
to	O
use	O
for	O
each	O
example	O
.	O
We	O
apply	O
the	O
MoE	O
to	O
the	O
tasks	O
of	O
language	O
modeling	O
and	O
machine	O
translation	O
,	O
where	O
model	O
capacity	O
is	O
critical	O
for	O
absorbing	O
the	O
vast	O
quantities	O
of	O
knowledge	O
available	O
in	O
the	O
training	O
corpora	O
.	O

Neural	O
machine	B-RESEARCH_PROBLEM
translation	E-RESEARCH_PROBLEM
is	O
a	O
recently	O
proposed	O
approach	O
to	O
machine	B-RESEARCH_PROBLEM
translation	E-RESEARCH_PROBLEM
.	O
Unlike	O
the	O
traditional	O
statistical	O
machine	B-RESEARCH_PROBLEM
translation	E-RESEARCH_PROBLEM
,	O
the	O
neural	O
machine	B-RESEARCH_PROBLEM
translation	E-RESEARCH_PROBLEM
aims	O
at	O
building	O
a	O
single	O
neural	O
network	O
that	O
can	O
be	O
jointly	O
tuned	O
to	O
maximize	O
the	O
translation	O
performance	O
.	O
The	O
models	O
proposed	O
recently	O
for	O
neural	O
machine	B-RESEARCH_PROBLEM
translation	E-RESEARCH_PROBLEM
often	O
belong	O
to	O
a	O
family	O
of	O
encoder	O
-	O
decoders	O
and	O
encode	O
a	O
source	O
sentence	O
into	O
a	O
fixed	O
-	O
length	O
vector	O
from	O
which	O
a	O
decoder	O
generates	O
a	O
translation	O
.	O
In	O
this	O
paper	O
,	O
we	O
conjecture	O
that	O
the	O
use	B-RESEARCH_PROBLEM
of	I-RESEARCH_PROBLEM
a	I-RESEARCH_PROBLEM
fixed	I-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
length	I-RESEARCH_PROBLEM
vector	I-RESEARCH_PROBLEM
is	I-RESEARCH_PROBLEM
a	I-RESEARCH_PROBLEM
bottleneck	I-RESEARCH_PROBLEM
in	I-RESEARCH_PROBLEM
improving	I-RESEARCH_PROBLEM
the	I-RESEARCH_PROBLEM
performance	I-RESEARCH_PROBLEM
of	I-RESEARCH_PROBLEM
this	I-RESEARCH_PROBLEM
basic	I-RESEARCH_PROBLEM
encoder	I-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
decoder	I-RESEARCH_PROBLEM
architecture	E-RESEARCH_PROBLEM
,	O
and	O
propose	O
to	O
extend	O
this	O
by	O
allowing	O
a	O
model	O
to	O
automatically	O
(	O
soft	O
-	O
)	O
search	O
for	O
parts	O
of	O
a	O
source	O
sentence	O
that	O
are	O
relevant	O
to	O
predicting	O
a	O
target	O
word	O
,	O
without	O
having	O
to	O
form	O
these	O
parts	O
as	O
a	O
hard	O
segment	O
explicitly	O
.	O
With	O
this	O
new	O
approach	O
,	O
we	O
achieve	O
a	O
translation	O
performance	O
comparable	O
to	O
the	O
existing	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
phrase	O
-	O
based	O
system	O
on	O
the	O
task	O
of	O
English	O
-	O
to	O
-	O
French	O
translation	O
.	O
Furthermore	O
,	O
qualitative	O
analysis	O
reveals	O
that	O
the	O
(	O
soft	O
-	O
)	O
alignments	O
found	O
by	O
the	O
model	O
agree	O
well	O
with	O
our	O
intuition	O
.	O
INTRODUCTION	O

Natural	O
language	O
processing	O
(	O
NLP	O
)	O
models	O
often	O
require	O
a	O
massive	O
number	O
of	O
parameters	O
for	O
word	O
embeddings	O
,	O
resulting	O
in	O
a	O
large	O
storage	O
or	O
memory	O
footprint	O
.	O
Deploying	O
neural	O
NLP	O
models	O
to	O
mobile	O
devices	O
requires	O
compressing	B-RESEARCH_PROBLEM
the	I-RESEARCH_PROBLEM
word	I-RESEARCH_PROBLEM
embeddings	I-RESEARCH_PROBLEM
without	I-RESEARCH_PROBLEM
any	I-RESEARCH_PROBLEM
significant	I-RESEARCH_PROBLEM
sacrifices	I-RESEARCH_PROBLEM
in	I-RESEARCH_PROBLEM
performance	E-RESEARCH_PROBLEM
.	O
For	O
this	O
purpose	O
,	O
we	O
propose	O
to	O
construct	O
the	O
embeddings	O
with	O
few	O
basis	O
vectors	O
.	O
For	O
each	O
word	O
,	O
the	O
composition	O
of	O
basis	O
vectors	O
is	O
determined	O
by	O
a	O
hash	O
code	O
.	O
To	O
maximize	O
the	O
compression	O
rate	O
,	O
we	O
adopt	O
the	O
multi-codebook	O
quantization	O
approach	O
instead	O
of	O
binary	O
coding	O
scheme	O
.	O
Each	O
code	O
is	O
composed	O
of	O
multiple	O
discrete	O
numbers	O
,	O
such	O
as	O
(	O
3	O
,	O
2	O
,	O
1	O
,	O
8	O
)	O
,	O
where	O
the	O
value	O
of	O
each	O
component	O
is	O
limited	O
to	O
a	O
fixed	O
range	O
.	O
We	O
propose	O
to	O
directly	O
learn	O
the	O
discrete	O
codes	O
in	O
an	O
end	O
-	O
to	O
-	O
end	O
neural	O
network	O
by	O
applying	O
the	O
Gumbel	O
-	O
softmax	O
trick	O
.	O

Subset	B-RESEARCH_PROBLEM
selection	I-RESEARCH_PROBLEM
from	I-RESEARCH_PROBLEM
massive	I-RESEARCH_PROBLEM
data	E-RESEARCH_PROBLEM
with	O
noised	O
information	O
is	O
increasingly	O
popular	O
for	O
various	O
applications	O
.	O
This	O
problem	O
is	O
still	O
highly	O
challenging	O
as	O
current	O
methods	O
are	O
generally	O
slow	O
in	O
speed	O
and	O
sensitive	O
to	O
outliers	O
.	O
To	O
address	O
the	O
above	O
two	O
issues	O
,	O
we	O
propose	O
an	O
accelerated	O
robust	O
subset	B-RESEARCH_PROBLEM
selection	E-RESEARCH_PROBLEM
(	O
ARSS	O
)	O
method	O
.	O
Specifically	O
in	O
the	O
subset	B-RESEARCH_PROBLEM
selection	E-RESEARCH_PROBLEM
area	O
,	O
this	O
is	O
the	O
first	O
attempt	O
to	O
employ	O
the	O
p	O
(	O
0	O
<	O
p	O
?	O
1	O
)	O
-	O
norm	O
based	O
measure	O
for	O
the	O
representation	O
loss	O
,	O
preventing	O
large	O
errors	O
from	O
dominating	O
our	O
objective	O
.	O
As	O
a	O
result	O
,	O
the	O
robustness	O
against	O
outlier	O
elements	O
is	O
greatly	O
enhanced	O
.	O
Actually	O
,	O
data	O
size	O
is	O
generally	O
much	O
larger	O
than	O
feature	O
length	O
,	O
i.e.	O
N	O
L.	O
Based	O
on	O
this	O
observation	O
,	O
we	O
propose	O
a	O
speedup	O
solver	O
(	O
via	O
ALM	O
and	O
equivalent	O
derivations	O
)	O
to	O
highly	O
reduce	O
the	O
computational	O
cost	O
,	O
theoretically	O
from	O
ON	O
4	O
to	O
ON	O
2	O
L	O
.	O
Extensive	O
experiments	O
on	O
ten	O
benchmark	O
datasets	O
verify	O
that	O
our	O
method	O
not	O
only	O
outperforms	O
state	O
of	O
the	O
art	O
methods	O
,	O
but	O
also	O
runs	O
10,000	O
+	O
times	O
faster	O
than	O
the	O
most	O
related	O
method	O
.	O

State	O
-	O
of	O
-	O
the	O
-	O
art	O
named	O
entity	O
recognition	O
systems	O
rely	O
heavily	O
on	O
hand	O
-	O
crafted	O
features	O
and	O
domain	O
-	O
specific	O
knowledge	O
in	O
order	O
to	O
learn	O
effectively	O
from	O
the	O
small	O
,	O
supervised	O
training	O
corpora	O
thatare	O
available	O
.	O
In	O
this	O
paper	O
,	O
we	O
introduce	O
two	O
new	O
neural	O
architectures	O
-	O
one	O
based	O
on	O
bidirectional	O
LSTMs	O
and	O
conditional	O
random	O
fields	O
,	O
and	O
the	O
other	O
that	O
constructs	O
and	O
labels	O
segments	O
using	O
a	O
transition	O
-	O
based	O
approach	O
inspired	O
by	O
shift	O
-	O
reduce	O
parsers	O
.	O
Our	O
models	O
rely	O
on	O
two	O
sources	O
of	O
information	O
about	O
words	O
:	O
character	O
-	O
based	O
word	O
representations	O
learned	O
from	O
the	O
supervised	O
corpus	O
and	O
unsupervised	O
word	O
representations	O
learned	O
from	O
unannotated	O
corpora	O
.	O
Our	O
models	O
obtain	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
in	O
NER	S-RESEARCH_PROBLEM
in	O
four	O
languages	O
without	O
resorting	O
to	O
any	O
language	O
-	O
specific	O
knowledge	O
or	O
resources	O
such	O
as	O
gazetteers	O
.	O
1	O

Today	O
when	O
many	O
practitioners	O
run	O
basic	O
NLP	O
on	O
the	O
entire	O
web	O
and	O
large	O
-	O
volume	O
traffic	O
,	O
faster	O
methods	O
are	O
paramount	O
to	O
saving	O
time	O
and	O
energy	O
costs	O
.	O
Recent	O
advances	O
in	O
GPU	O
hardware	O
have	O
led	O
to	O
the	O
emergence	O
of	O
bi-directional	O
LSTMs	O
as	O
a	O
standard	O
method	O
for	O
obtaining	O
pertoken	O
vector	O
representations	O
serving	O
as	O
input	O
to	O
labeling	O
tasks	O
such	O
as	O
NER	O
(	O
often	O
followed	O
by	O
prediction	O
in	O
a	O
linear	O
-	O
chain	O
CRF	O
)	O
.	O
Though	O
expressive	O
and	O
accurate	O
,	O
these	O
models	O
fail	O
to	O
fully	O
exploit	O
GPU	O
parallelism	O
,	O
limiting	O
their	O
computational	O
efficiency	O
.	O
This	O
paper	O
proposes	O
a	O
faster	B-RESEARCH_PROBLEM
alternative	I-RESEARCH_PROBLEM
to	I-RESEARCH_PROBLEM
Bi	I-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
LSTMs	I-RESEARCH_PROBLEM
for	I-RESEARCH_PROBLEM
NER	E-RESEARCH_PROBLEM
:	O
Iterated	O
Dilated	O
Convolutional	O
Neural	O
Networks	O
(	O
ID	O
-	O
CNNs	O
)	O
,	O
which	O
have	O
better	O
capacity	O
than	O
traditional	O
CNNs	O
for	O
large	O
context	O
and	O
structured	O
prediction	O
.	O
Unlike	O
LSTMs	O
whose	O
sequential	O
processing	O
on	O
sentences	O
of	O
length	O
N	O
requires	O
O(N	O
)	O
time	O
even	O
in	O
the	O
face	O
of	O
parallelism	O
,	O
ID	O
-	O
CNNs	O
permit	O
fixed	O
-	O
depth	O
convolutions	O
to	O
run	O
in	O
parallel	O
across	O
entire	O
documents	O
.	O
We	O
describe	O
a	O
distinct	O
combination	O
of	O
network	O
structure	O
,	O
parameter	O
sharing	O
and	O
training	O
procedures	O
that	O
enable	O
dramatic	O
14	O
-	O
20x	O
testtime	O
speedups	O
while	O
retaining	O
accuracy	O
comparable	O
to	O
the	O
Bi	O
-	O
LSTM	O
-	O
CRF	O
.	O
Moreover	O
,	O
ID	O
-	O
CNNs	O
trained	O
to	O
aggregate	O
context	O
from	O
the	O
entire	O
document	O
are	O
even	O
more	O
accurate	O
while	O
maintaining	O
8	O
x	O
faster	O
test	O
time	O
speeds	O
.	O

Pre-trained	O
word	O
embeddings	O
learned	O
from	O
unlabeled	O
text	O
have	O
become	O
a	O
standard	O
component	O
of	O
neural	O
network	O
architectures	O
for	O
NLP	O
tasks	O
.	O
However	O
,	O
in	O
most	O
cases	O
,	O
the	O
recurrent	O
network	O
that	O
operates	O
on	O
word	O
-	O
level	O
representations	O
to	O
produce	O
context	O
sensitive	O
representations	O
is	O
trained	O
on	O
relatively	O
little	O
labeled	O
data	O
.	O
In	O
this	O
paper	O
,	O
we	O
demonstrate	O
a	O
general	B-RESEARCH_PROBLEM
semi-supervised	I-RESEARCH_PROBLEM
approach	I-RESEARCH_PROBLEM
for	I-RESEARCH_PROBLEM
adding	I-RESEARCH_PROBLEM
pretrained	I-RESEARCH_PROBLEM
context	I-RESEARCH_PROBLEM
embeddings	I-RESEARCH_PROBLEM
from	I-RESEARCH_PROBLEM
bidirectional	I-RESEARCH_PROBLEM
language	I-RESEARCH_PROBLEM
models	I-RESEARCH_PROBLEM
to	I-RESEARCH_PROBLEM
NLP	I-RESEARCH_PROBLEM
systems	E-RESEARCH_PROBLEM
and	O
apply	O
it	O
to	O
sequence	O
labeling	O
tasks	O
.	O
We	O
evaluate	O
our	O
model	O
on	O
two	O
standard	O
datasets	O
for	O
named	O
entity	O
recognition	O
(	O
NER	O
)	O
and	O
chunking	O
,	O
and	O
in	O
both	O
cases	O
achieve	O
state	O
of	O
the	O
art	O
results	O
,	O
surpassing	O
previous	O
systems	O
that	O
use	O
other	O
forms	O
of	O
transfer	O
or	O
joint	O
learning	O
with	O
additional	O
labeled	O
data	O
and	O
task	O
specific	O
gazetteers	O
.	O

Bi-directional	O
LSTMs	O
are	O
a	O
powerful	O
tool	O
for	O
text	O
representation	O
.	O
On	O
the	O
other	O
hand	O
,	O
they	O
have	O
been	O
shown	O
to	O
suffer	O
various	O
limitations	O
due	O
to	O
their	O
sequential	O
nature	O
.	O
We	O
investigate	O
an	O
alternative	B-RESEARCH_PROBLEM
LSTM	I-RESEARCH_PROBLEM
structure	I-RESEARCH_PROBLEM
for	I-RESEARCH_PROBLEM
encoding	I-RESEARCH_PROBLEM
text	E-RESEARCH_PROBLEM
,	O
which	O
consists	O
of	O
a	O
parallel	O
state	O
for	O
each	O
word	O
.	O
Recurrent	O
steps	O
are	O
used	O
to	O
perform	O
local	O
and	O
global	O
information	O
exchange	O
between	O
words	O
simultaneously	O
,	O
rather	O
than	O
incremental	O
reading	O
of	O
a	O
sequence	O
of	O
words	O
.	O
Results	O
on	O
various	O
classification	O
and	O
sequence	O
labelling	O
benchmarks	O
show	O
that	O
the	O
proposed	O
model	O
has	O
strong	O
representation	O
power	O
,	O
giving	O
highly	O
competitive	O
performances	O
compared	O
to	O
stacked	O
BiLSTM	O
models	O
with	O
similar	O
parameter	O
numbers	O
.	O

Neural	B-RESEARCH_PROBLEM
network	I-RESEARCH_PROBLEM
approaches	I-RESEARCH_PROBLEM
to	I-RESEARCH_PROBLEM
Named	I-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
Entity	I-RESEARCH_PROBLEM
Recognition	E-RESEARCH_PROBLEM
reduce	O
the	O
need	O
for	O
carefully	O
handcrafted	O
features	O
.	O
While	O
some	O
features	O
do	O
remain	O
in	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
systems	O
,	O
lexical	O
features	O
have	O
been	O
mostly	O
discarded	O
,	O
with	O
the	O
exception	O
of	O
gazetteers	O
.	O
In	O
this	O
work	O
,	O
we	O
show	O
that	O
this	O
is	O
unfair	O
:	O
lexical	O
features	O
are	O
actually	O
quite	O
useful	O
.	O
We	O
propose	O
to	O
embed	O
words	O
and	O
entity	O
types	O
into	O
a	O
lowdimensional	O
vector	O
space	O
we	O
train	O
from	O
annotated	O
data	O
produced	O
by	O
distant	O
supervision	O
thanks	O
to	O
Wikipedia	O
.	O
From	O
this	O
,	O
we	O
compute	O
-	O
offline	O
-	O
a	O
feature	O
vector	O
representing	O
each	O
word	O
.	O
When	O
used	O
with	O
a	O
vanilla	O
recurrent	O
neural	O
network	O
model	O
,	O
this	O
representation	O
yields	O
substantial	O
improvements	O
.	O
We	O
establish	O
a	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
F1	O
score	O
of	O
87.95	O
on	O
ONTONOTES	O
5.0	O
,	O
while	O
matching	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
with	O
a	O
F	O
1	O
score	O
of	O
91.73	O
on	O
the	O
over	O
-	O
studied	O
CONLL	O
-	O
2003	O
dataset	O
.	O

It	O
is	O
common	O
that	O
entity	O
mentions	O
can	O
contain	O
other	O
mentions	O
recursively	O
.	O
This	O
paper	O
introduces	O
a	O
scalable	O
transition	O
-	O
based	O
method	O
to	O
model	B-RESEARCH_PROBLEM
the	I-RESEARCH_PROBLEM
nested	I-RESEARCH_PROBLEM
structure	I-RESEARCH_PROBLEM
of	I-RESEARCH_PROBLEM
mentions	E-RESEARCH_PROBLEM
.	O
We	O
first	O
map	O
a	O
sentence	O
with	O
nested	O
mentions	O
to	O
a	O
designated	O
forest	O
where	O
each	O
mention	O
corresponds	O
to	O
a	O
constituent	O
of	O
the	O
forest	O
.	O
Our	O
shiftreduce	O
based	O
system	O
then	O
learns	O
to	O
construct	O
the	O
forest	O
structure	O
in	O
a	O
bottom	O
-	O
up	O
manner	O
through	O
an	O
action	O
sequence	O
whose	O
maximal	O
length	O
is	O
guaranteed	O
to	O
be	O
three	O
times	O
of	O
the	O
sentence	O
length	O
.	O
Based	O
on	O
Stack	O
-	O
LSTM	O
which	O
is	O
employed	O
to	O
efficiently	O
and	O
effectively	O
represent	O
the	O
states	O
of	O
the	O
system	O
in	O
a	O
continuous	O
space	O
,	O
our	O
system	O
is	O
further	O
incorporated	O
with	O
a	O
character	O
-	O
based	O
component	O
to	O
capture	O
letterlevel	O
patterns	O
.	O
Our	O
model	O
achieves	O
the	O
stateof	O
-	O
the	O
-	O
art	O
results	O
on	O
ACE	O
datasets	O
,	O
showing	O
its	O
effectiveness	O
in	O
detecting	O
nested	O
mentions	O
.	O
1	O

We	O
introduce	O
a	O
new	O
language	B-RESEARCH_PROBLEM
representation	I-RESEARCH_PROBLEM
model	E-RESEARCH_PROBLEM
called	O
BERT	O
,	O
which	O
stands	O
for	O
Bidirectional	O
Encoder	O
Representations	O
from	O
Transformers	O
.	O
Unlike	O
recent	O
(	O
Peters	O
et	O
al.	O
,	O
2018	O
a	O
;	O
Radford	O
et	O
al.	O
,	O
2018	O
)	O
,	O
BERT	O
is	O
designed	O
to	O
pretrain	O
deep	O
bidirectional	O
representations	O
from	O
unlabeled	O
text	O
by	O
jointly	O
conditioning	O
on	O
both	O
left	O
and	O
right	O
context	O
in	O
all	O
layers	O
.	O
As	O
a	O
result	O
,	O
the	O
pre-trained	O
BERT	O
model	O
can	O
be	O
finetuned	O
with	O
just	O
one	O
additional	O
output	O
layer	O
to	O
create	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
for	O
a	O
wide	O
range	O
of	O
tasks	O
,	O
such	O
as	O
question	O
answering	O
and	O
language	O
inference	O
,	O
without	O
substantial	O
taskspecific	O
architecture	O
modifications	O
.	O
BERT	O
is	O
conceptually	O
simple	O
and	O
empirically	O
powerful	O
.	O
It	O
obtains	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
eleven	O
natural	O
language	O
processing	O
tasks	O
,	O
including	O
pushing	O
the	O
GLUE	O
score	O
to	O
80.5	O
%	O
(	O
7.7	O
%	O
point	O
absolute	O
improvement	O
)	O
,	O
MultiNLI	O
accuracy	O
to	O
86.7	O
%	O
(	O
4.6	O
%	O
absolute	O
improvement	O
)	O
,	O
SQ	O
u	O
AD	O
v	O
1.1	O
question	O
answering	O
Test	O
F1	O
to	O
93.2	O
(	O
1.5	O
point	O
absolute	O
improvement	O
)	O
and	O
SQ	O
u	O
AD	O
v2.0	O
Test	O
F1	O
to	O
83.1	O
(	O
5.1	O
point	O
absolute	O
improvement	O
)	O
.	O
Jeremy	O
Howard	O
and	O
Sebastian	O
Ruder	O
.	O
2018	O
.	O
Universal	O
language	O
model	O
fine	O
-	O
tuning	O
for	O
text	O
classification	O
.	O

Motivation	O
:	O
Biomedical	O
text	O
mining	O
is	O
becoming	O
increasingly	O
important	O
as	O
the	O
number	O
of	O
biomedical	O
documents	O
rapidly	O
grows	O
.	O
With	O
the	O
progress	O
in	O
natural	O
language	O
processing	O
(	O
NLP	O
)	O
,	O
extracting	B-RESEARCH_PROBLEM
valuable	I-RESEARCH_PROBLEM
information	I-RESEARCH_PROBLEM
from	I-RESEARCH_PROBLEM
biomedical	I-RESEARCH_PROBLEM
literature	E-RESEARCH_PROBLEM
has	O
gained	O
popularity	O
among	O
researchers	O
,	O
and	O
deep	O
learning	O
has	O
boosted	O
the	O
development	O
of	O
effective	O
biomedical	O
text	O
mining	O
models	O
.	O
However	O
,	O
directly	O
applying	O
the	O
advancements	O
in	O
NLP	O
to	O
biomedical	B-RESEARCH_PROBLEM
text	I-RESEARCH_PROBLEM
mining	I-RESEARCH_PROBLEM
often	I-RESEARCH_PROBLEM
yields	I-RESEARCH_PROBLEM
unsatisfactory	I-RESEARCH_PROBLEM
results	I-RESEARCH_PROBLEM
due	I-RESEARCH_PROBLEM
to	I-RESEARCH_PROBLEM
a	I-RESEARCH_PROBLEM
word	I-RESEARCH_PROBLEM
distribution	I-RESEARCH_PROBLEM
shift	I-RESEARCH_PROBLEM
from	I-RESEARCH_PROBLEM
general	I-RESEARCH_PROBLEM
domain	I-RESEARCH_PROBLEM
corpora	I-RESEARCH_PROBLEM
to	I-RESEARCH_PROBLEM
biomedical	I-RESEARCH_PROBLEM
corpora	E-RESEARCH_PROBLEM
.	O
In	O
this	O
article	O
,	O
we	O
investigate	O
how	O
the	O
recently	O
introduced	O
pre-trained	B-RESEARCH_PROBLEM
language	I-RESEARCH_PROBLEM
model	I-RESEARCH_PROBLEM
BERT	I-RESEARCH_PROBLEM
can	I-RESEARCH_PROBLEM
be	I-RESEARCH_PROBLEM
adapted	I-RESEARCH_PROBLEM
for	I-RESEARCH_PROBLEM
biomedical	I-RESEARCH_PROBLEM
corpora	E-RESEARCH_PROBLEM
.	O
Results	O
:	O
We	O
introduce	O
BioBERT	O
(	O
Bidirectional	O
Encoder	O
Representations	O
from	O
Transformers	O
for	O
Biomedical	O
Text	O
Mining	O
)	O
,	O
which	O
is	O
a	O
domain	O
-	O
specific	O
language	O
representation	O
model	O
pre-trained	O
on	O
large	O
-	O
scale	O
biomedical	O
corpora	O
.	O

Building	B-RESEARCH_PROBLEM
computers	I-RESEARCH_PROBLEM
able	I-RESEARCH_PROBLEM
to	I-RESEARCH_PROBLEM
answer	I-RESEARCH_PROBLEM
questions	I-RESEARCH_PROBLEM
on	I-RESEARCH_PROBLEM
any	I-RESEARCH_PROBLEM
subject	E-RESEARCH_PROBLEM
is	O
along	O
standing	O
goal	O
of	O
artificial	O
intelligence	O
.	O
Promising	O
progress	O
has	O
recently	O
been	O
achieved	O
by	O
methods	O
that	O
learn	O
to	O
map	O
questions	O
to	O
logical	O
forms	O
or	O
data	O
base	O
queries	O
.	O
Such	O
approaches	O
can	O
be	O
effective	O
but	O
at	O
the	O
cost	O
of	O
either	O
large	O
amounts	O
of	O
human	O
-	O
labeled	O
data	O
or	O
by	O
defining	O
lexicons	O
and	O
grammars	O
tailored	O
by	O
practitioners	O
.	O
In	O
this	O
paper	O
,	O
we	O
instead	O
take	O
the	O
radical	O
approach	O
of	O
learning	O
to	O
map	O
questions	O
to	O
vectorial	O
feature	O
representations	O
.	O
By	O
mapping	O
answers	O
into	O
the	O
same	O
space	O
one	O
can	O
query	O
any	O
knowledge	O
base	O
independent	O
of	O
its	O
schema	O
,	O
without	O
requiring	O
any	O
grammar	O
or	O
lexicon	O
.	O
Our	O
method	O
is	O
trained	O
with	O
a	O
new	O
optimization	O
procedure	O
combining	O
stochastic	O
gradient	O
descent	O
followed	O
by	O
a	O
fine	O
-	O
tuning	O
step	O
using	O
the	O
weak	O
supervision	O
provided	O
by	O
blending	O
automatically	O
and	O
collaboratively	O
generated	O
resources	O
.	O
We	O
empirically	O
demonstrate	O
that	O
our	O
model	O
can	O
capture	O
meaningful	O
signals	O
from	O
its	O
noisy	O
supervision	O
leading	O
to	O
major	O
improvements	O
over	O
paralex	O
,	O
the	O
only	O
existing	O
method	O
able	O
to	O
be	O
trained	O
on	O
similar	O
weakly	O
labeled	O
data	O
.	O

Semantic	B-RESEARCH_PROBLEM
matching	E-RESEARCH_PROBLEM
is	O
of	O
central	O
importance	O
to	O
many	O
natural	O
language	O
tasks	O
[	O
2,28	O
]	O
.	O
A	O
successful	O
matching	O
algorithm	O
needs	O
to	O
adequately	O
model	O
the	O
internal	O
structures	O
of	O
language	O
objects	O
and	O
the	O
interaction	O
between	O
them	O
.	O
As	O
a	O
step	O
toward	O
this	O
goal	O
,	O
we	O
propose	O
convolutional	O
neural	O
network	O
models	O
for	O
matching	O
two	O
sentences	O
,	O
by	O
adapting	O
the	O
convolutional	O
strategy	O
in	O
vision	O
and	O
speech	O
.	O
The	O
proposed	O
models	O
not	O
only	O
nicely	O
represent	O
the	O
hierarchical	O
structures	O
of	O
sentences	O
with	O
their	O
layerby	O
-	O
layer	O
composition	O
and	O
pooling	O
,	O
but	O
also	O
capture	O
the	O
rich	O
matching	O
patterns	O
at	O
different	O
levels	O
.	O
Our	O
models	O
are	O
rather	O
generic	O
,	O
requiring	O
no	O
prior	O
knowledge	O
on	O
language	O
,	O
and	O
can	O
hence	O
be	O
applied	O
to	O
matching	O
tasks	O
of	O
different	O
nature	O
and	O
in	O
different	O
languages	O
.	O
The	O
empirical	O
study	O
on	O
a	O
variety	O
of	O
matching	O
tasks	O
demonstrates	O
the	O
efficacy	O
of	O
the	O
proposed	O
model	O
on	O
a	O
variety	O
of	O
matching	O
tasks	O
and	O
its	O
superiority	O
to	O
competitor	O
models	O
.	O

Training	B-RESEARCH_PROBLEM
large	I-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
scale	I-RESEARCH_PROBLEM
question	I-RESEARCH_PROBLEM
answering	I-RESEARCH_PROBLEM
systems	E-RESEARCH_PROBLEM
is	O
complicated	O
because	O
training	O
sources	O
usually	O
cover	O
a	O
small	O
portion	O
of	O
the	O
range	O
of	O
possible	O
questions	O
.	O
This	O
paper	O
studies	O
the	O
impact	O
of	O
multitask	O
and	O
transfer	O
learning	O
for	O
simple	B-RESEARCH_PROBLEM
question	I-RESEARCH_PROBLEM
answering	E-RESEARCH_PROBLEM
;	O
a	O
setting	O
for	O
which	O
the	O
reasoning	O
required	O
to	O
answer	O
is	O
quite	O
easy	O
,	O
as	O
long	O
as	O
one	O
can	O
retrieve	O
the	O
correct	O
evidence	O
given	O
a	O
question	O
,	O
which	O
can	O
be	O
difficult	O
in	O
large	O
-	O
scale	O
conditions	O
.	O
To	O
this	O
end	O
,	O
we	O
introduce	O
a	O
new	O
dataset	O
of	O
100	O
k	O
questions	O
that	O
we	O
use	O
in	O
conjunction	O
with	O
existing	O
benchmarks	O
.	O
We	O
conduct	O
our	O
study	O
within	O
the	O
framework	O
of	O
Memory	O
Networks	O
(	O
Weston	O
et	O
al.	O
,	O
2015	O
)	O
because	O
this	O
perspective	O
allows	O
us	O
to	O
eventually	O
scale	O
up	O
to	O
more	O
complex	O
reasoning	O
,	O
and	O
show	O
that	O
Memory	O
Networks	O
can	O
be	O
successfully	O
trained	O
to	O
achieve	O
excellent	O
performance	O
.	O

Most	O
conventional	O
sentence	B-RESEARCH_PROBLEM
similarity	E-RESEARCH_PROBLEM
methods	O
only	O
focus	O
on	O
similar	O
parts	O
of	O
two	O
input	O
sentences	O
,	O
and	O
simply	O
ignore	O
the	O
dissimilar	O
parts	O
,	O
which	O
usually	O
give	O
us	O
some	O
clues	O
and	O
semantic	O
meanings	O
about	O
the	O
sentences	O
.	O
In	O
this	O
work	O
,	O
we	O
propose	O
a	O
model	O
to	O
take	O
into	O
account	O
both	O
the	O
similarities	O
and	O
dissimilarities	O
by	O
decomposing	O
and	O
composing	O
lexical	O
semantics	O
over	O
sentences	O
.	O
The	O
model	O
represents	O
each	O
word	O
as	O
a	O
vector	O
,	O
and	O
calculates	O
a	O
semantic	O
matching	O
vector	O
for	O
each	O
word	O
based	O
on	O
all	O
words	O
in	O
the	O
other	O
sentence	O
.	O
Then	O
,	O
each	O
word	O
vector	O
is	O
decomposed	O
into	O
a	O
similar	O
component	O
and	O
a	O
dissimilar	O
component	O
based	O
on	O
the	O
semantic	O
matching	O
vector	O
.	O
After	O
this	O
,	O
a	O
two	O
-	O
channel	O
CNN	O
model	O
is	O
employed	O
to	O
capture	O
features	O
by	O
composing	O
the	O
similar	O
and	O
dissimilar	O
components	O
.	O
Finally	O
,	O
a	O
similarity	O
score	O
is	O
estimated	O
over	O
the	O
composed	O
feature	O
vectors	O
.	O
Experimental	O
results	O
show	O
that	O
our	O
model	O
gets	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
the	O
answer	O
sentence	O
selection	O
task	O
,	O
and	O
achieves	O
a	O
comparable	O
result	O
on	O
the	O
paraphrase	O
identification	O
task	O
.	O

Understanding	B-RESEARCH_PROBLEM
unstructured	I-RESEARCH_PROBLEM
text	E-RESEARCH_PROBLEM
is	O
a	O
major	O
goal	O
within	O
natural	O
language	O
processing	O
.	O
Comprehension	O
tests	O
pose	O
questions	O
based	O
on	O
short	O
text	O
passages	O
to	O
evaluate	O
such	O
understanding	O
.	O
In	O
this	O
work	O
,	O
we	O
investigate	O
machine	O
comprehension	O
on	O
the	O
challenging	O
benchmark	O
.	O
Partly	O
because	O
of	O
its	O
limited	O
size	O
,	O
prior	O
work	O
on	O
MCTest	O
has	O
focused	O
mainly	O
on	O
engineering	O
better	O
features	O
.	O
We	O
tackle	O
the	O
dataset	O
with	O
a	O
neural	O
approach	O
,	O
harnessing	O
simple	O
neural	O
networks	O
arranged	O
in	O
a	O
parallel	O
hierarchy	O
.	O
The	O
parallel	O
hierarchy	O
enables	O
our	O
model	O
to	O
compare	O
the	O
passage	O
,	O
question	O
,	O
and	O
answer	O
from	O
a	O
variety	O
of	O
trainable	O
perspectives	O
,	O
as	O
opposed	O
to	O
using	O
a	O
manually	O
designed	O
,	O
rigid	O
feature	O
set	O
.	O
Perspectives	O
range	O
from	O
the	O
word	O
level	O
to	O
sentence	O
fragments	O
to	O
sequences	O
of	O
sentences	O
;	O
the	O
networks	O
operate	O
only	O
on	O
word	O
-	O
embedding	O
representations	O
of	O
text	O
.	O

We	O
propose	O
a	O
novel	O
neural	O
attention	O
architecture	O
to	O
tackle	O
machine	B-RESEARCH_PROBLEM
comprehension	E-RESEARCH_PROBLEM
tasks	O
,	O
such	O
as	O
answering	B-RESEARCH_PROBLEM
Cloze	I-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
style	I-RESEARCH_PROBLEM
queries	I-RESEARCH_PROBLEM
with	I-RESEARCH_PROBLEM
respect	I-RESEARCH_PROBLEM
to	I-RESEARCH_PROBLEM
a	I-RESEARCH_PROBLEM
document	E-RESEARCH_PROBLEM
.	O
Unlike	O
previous	O
models	O
,	O
we	O
do	O
not	O
collapse	O
the	O
query	O
into	O
a	O
single	O
vector	O
,	O
instead	O
we	O
deploy	O
an	O
iterative	O
alternating	O
attention	O
mechanism	O
that	O
allows	O
a	O
fine	O
-	O
grained	O
exploration	O
of	O
both	O
the	O
query	O
and	O
the	O
document	O
.	O
Our	O
model	O
outperforms	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
baselines	O
in	O
standard	O
machine	B-RESEARCH_PROBLEM
comprehension	E-RESEARCH_PROBLEM
benchmarks	O
such	O
as	O
CNN	O
news	O
articles	O
and	O
the	O
Children	O
's	O
Book	O
Test	O
(	O
CBT	O
)	O
dataset	O
.	O

In	O
this	O
paper	O
,	O
we	O
study	O
the	O
problem	O
of	O
question	B-RESEARCH_PROBLEM
answering	I-RESEARCH_PROBLEM
when	I-RESEARCH_PROBLEM
reasoning	I-RESEARCH_PROBLEM
over	I-RESEARCH_PROBLEM
multiple	I-RESEARCH_PROBLEM
facts	I-RESEARCH_PROBLEM
is	I-RESEARCH_PROBLEM
required	E-RESEARCH_PROBLEM
.	O
We	O
propose	O
Query	O
-	O
Reduction	O
Network	O
(	O
QRN	O
)	O
,	O
a	O
variant	O
of	O
Recurrent	O
Neural	O
Network	O
(	O
RNN	O
)	O
that	O
effectively	O
handles	O
both	O
short	O
-	O
term	O
(	O
local	O
)	O
and	O
long	O
-	O
term	O
(	O
global	O
)	O
sequential	O
dependencies	O
to	O
reason	O
over	O
multiple	O
facts	O
.	O
QRN	O
considers	O
the	O
context	O
sentences	O
as	O
a	O
sequence	O
of	O
state	O
-	O
changing	O
triggers	O
,	O
and	O
reduces	O
the	O
original	O
query	O
to	O
a	O
more	O
informed	O
query	O
as	O
it	O
observes	O
each	O
trigger	O
(	O
context	O
sentence	O
)	O
through	O
time	O
.	O
Our	O
experiments	O
show	O
that	O
QRN	O
produces	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
in	O
bAbI	O
QA	O
and	O
dialog	O
tasks	O
,	O
and	O
in	O
are	O
al	O
goal	O
-	O
oriented	O
dialog	O
dataset	O
.	O
In	O
addition	O
,	O
QRN	O
formulation	O
allows	O
parallelization	O
on	O
RNN	O
's	O
time	O
axis	O
,	O
saving	O
an	O
order	O
of	O
magnitude	O
in	O
time	O
complexity	O
for	O
training	O
and	O
inference	O
.	O
INTRODUCTION	O
In	O
this	O
paper	O
,	O
we	O
address	O
the	O
problem	O
of	O
question	O
answering	O
(	O
QA	O
)	O
when	O
reasoning	O
over	O
multiple	O
facts	O
is	O
required	O
.	O

We	O
present	O
a	O
memory	O
augmented	O
neural	O
network	O
for	O
natural	B-RESEARCH_PROBLEM
language	I-RESEARCH_PROBLEM
understanding	E-RESEARCH_PROBLEM
:	O
Neural	O
Semantic	O
Encoders	O
.	O
NSE	O
is	O
equipped	O
with	O
a	O
novel	O
memory	O
update	O
rule	O
and	O
has	O
a	O
variable	O
sized	O
encoding	O
memory	O
that	O
evolves	O
overtime	O
and	O
maintains	O
the	O
understanding	O
of	O
input	O
sequences	O
through	O
read	O
,	O
compose	O
and	O
write	O
operations	O
.	O
NSE	O
can	O
also	O
access	O
1	O
multiple	O
and	O
shared	O
memories	O
.	O
In	O
this	O
paper	O
,	O
we	O
demonstrated	O
the	O
effectiveness	O
and	O
the	O
flexibility	O
of	O
NSE	O
on	O
five	O
different	O
natural	O
language	O
tasks	O
:	O
natural	O
language	O
inference	O
,	O
question	O
answering	O
,	O
sentence	O
classification	O
,	O
document	O
sentiment	O
analysis	O
and	O
machine	O
translation	O
where	O
NSE	O
achieved	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
when	O
evaluated	O
on	O
publically	O
available	O
benchmarks	O
.	O
For	O
example	O
,	O
our	O
shared	O
-	O
memory	O
model	O
showed	O
an	O
encouraging	O
result	O
on	O
neural	O
machine	O
translation	O
,	O
improving	O
an	O
attention	O
-	O
based	O
baseline	O
by	O
approximately	O
1.0	O
BLEU	O
.	O
Recurrent	O
neural	O
networks	O
(	O
RNNs	O
)	O
have	O
been	O
successful	O
for	O
modeling	O
sequences	O
[	O
1	O
]	O
.	O

Machine	B-RESEARCH_PROBLEM
comprehension	I-RESEARCH_PROBLEM
of	I-RESEARCH_PROBLEM
text	E-RESEARCH_PROBLEM
is	O
an	O
important	O
problem	O
in	O
natural	O
language	O
processing	O
.	O
A	O
recently	O
released	O
dataset	O
,	O
the	O
Stanford	O
Question	O
Answering	O
Dataset	O
(	O
SQuAD	O
)	O
,	O
offers	O
a	O
large	O
number	O
of	O
real	O
questions	O
and	O
their	O
answers	O
created	O
by	O
humans	O
through	O
crowdsourcing	O
.	O
SQuAD	O
provides	O
a	O
challenging	O
testbed	O
for	O
evaluating	O
machine	O
comprehension	O
algorithms	O
,	O
partly	O
because	O
compared	O
with	O
previous	O
datasets	O
,	O
in	O
SQuAD	O
the	O
answers	O
do	O
not	O
come	O
from	O
a	O
small	O
set	O
of	O
candidate	O
answers	O
and	O
they	O
have	O
variable	O
lengths	O
.	O
We	O
propose	O
an	O
end	O
-	O
to	O
-	O
end	O
neural	O
architecture	O
for	O
the	O
task	O
.	O
The	O
architecture	O
is	O
based	O
on	O
match	O
-	O
LSTM	O
,	O
a	O
model	O
we	O
proposed	O
previously	O
for	O
textual	O
entailment	O
,	O
and	O
Pointer	O
Net	O
,	O
a	O
sequence	O
-	O
to	O
-	O
sequence	O
model	O
proposed	O
by	O
Vinyals	O
et	O
al.	O
(	O
2015	O
)	O
to	O
constrain	O
the	O
output	O
tokens	O
to	O
be	O
from	O
the	O
input	O
sequences	O
.	O
We	O
propose	O
two	O
ways	O
of	O
using	O
Pointer	O
Net	O
for	O
our	O
task	O
.	O
Our	O
experiments	O
show	O
that	O
both	O
of	O
our	O
two	O
models	O
substantially	O
outperform	O
the	O
best	O
results	O
obtained	O
by	O
Rajpurkar	O
et	O
al.	O
(	O
2016	O
)	O
using	O
logistic	O
regression	O
and	O
manually	O
crafted	O
features	O
.	O

The	O
reading	B-RESEARCH_PROBLEM
comprehension	E-RESEARCH_PROBLEM
task	O
,	O
that	O
asks	O
questions	O
about	O
a	O
given	O
evidence	O
document	O
,	O
is	O
a	O
central	O
problem	O
in	O
natural	O
language	O
understanding	O
.	O
Recent	O
formulations	O
of	O
this	O
task	O
have	O
typically	O
focused	O
on	O
answer	O
selection	O
from	O
a	O
set	O
of	O
candidates	O
pre-defined	O
manually	O
or	O
through	O
the	O
use	O
of	O
an	O
external	O
NLP	O
pipeline	O
.	O
However	O
,	O
Rajpurkar	O
et	O
al	O
.	O
(	O
2016	O
)	O
recently	O
released	O
the	O
SQUAD	O
dataset	O
in	O
which	O
the	O
answers	O
can	O
be	O
arbitrary	O
strings	O
from	O
the	O
supplied	O
text	O
.	O
In	O
this	O
paper	O
,	O
we	O
focus	O
on	O
this	O
answer	O
extraction	O
task	O
,	O
presenting	O
a	O
novel	O
model	O
architecture	O
that	O
efficiently	O
builds	O
fixed	O
length	O
representations	O
of	O
all	O
spans	O
in	O
the	O
evidence	O
document	O
with	O
a	O
recurrent	O
network	O
.	O
We	O
show	O
that	O
scoring	O
explicit	O
span	O
representations	O
significantly	O
improves	O
performance	O
over	O
other	O
approaches	O
that	O
factor	O
the	O
prediction	O
into	O
separate	O
predictions	O
about	O
words	O
or	O
start	O
and	O
end	O
markers	O
.	O
Our	O
approach	O
improves	O
upon	O
the	O
best	O
published	O
results	O
of	O
Wang	O
&	O
Jiang	O
(	O
2016	O
)	O
by	O
5	O
%	O
and	O
decreases	O
the	O
error	O
of	O
Rajpurkar	O
et	O
al.	O
's	O
baseline	O
by	O
>	O
50	O
%.	O
Recently	O
,	O
Rajpurkar	O
et	O
al.	O
(	O
2016	O
)	O
released	O
the	O
less	O
restricted	O
SQUAD	O
dataset	O
1	O
that	O
does	O
not	O
place	O
any	O
constraints	O
on	O
the	O
set	O
of	O
allowed	O
answers	O
,	O
other	O
than	O
that	O
they	O
should	O
be	O
drawn	O
from	O
the	O
evidence	O
document	O
.	O

We	O
present	O
a	O
novel	O
end	B-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
to	I-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
end	I-RESEARCH_PROBLEM
neural	I-RESEARCH_PROBLEM
model	I-RESEARCH_PROBLEM
to	I-RESEARCH_PROBLEM
extract	I-RESEARCH_PROBLEM
entities	I-RESEARCH_PROBLEM
and	I-RESEARCH_PROBLEM
relations	I-RESEARCH_PROBLEM
between	I-RESEARCH_PROBLEM
them	E-RESEARCH_PROBLEM
.	O
Our	O
recurrent	O
neural	O
network	O
based	O
model	O
captures	O
both	O
word	O
sequence	O
and	O
dependency	O
tree	O
substructure	O
information	O
by	O
stacking	O
bidirectional	O
treestructured	O
LSTM	O
-	O
RNNs	O
on	O
bidirectional	O
sequential	O
LSTM	O
-	O
RNNs	O
.	O
This	O
allows	O
our	O
model	O
to	O
jointly	B-RESEARCH_PROBLEM
represent	I-RESEARCH_PROBLEM
both	I-RESEARCH_PROBLEM
entities	I-RESEARCH_PROBLEM
and	I-RESEARCH_PROBLEM
relations	I-RESEARCH_PROBLEM
with	I-RESEARCH_PROBLEM
shared	I-RESEARCH_PROBLEM
parameters	I-RESEARCH_PROBLEM
in	I-RESEARCH_PROBLEM
a	I-RESEARCH_PROBLEM
single	I-RESEARCH_PROBLEM
model	E-RESEARCH_PROBLEM
.	O
We	O
further	O
encourage	O
detection	O
of	O
entities	O
during	O
training	O
and	O
use	O
of	O
entity	O
information	O
in	O
relation	O
extraction	O
via	O
entity	O
pretraining	O
and	O
scheduled	O
sampling	O
.	O
Our	O
model	O
improves	O
over	O
the	O
stateof	O
-	O
the	O
-	O
art	O
feature	O
-	O
based	O
model	O
on	O
end	O
-toend	O
relation	O
extraction	O
,	O
achieving	O
12.1	O
%	O
and	O
5.7	O
%	O
relative	O
error	O
reductions	O
in	O
F1score	O
on	O
ACE2005	O
and	O
ACE2004	O
,	O
respectively	O
.	O
We	O
also	O
show	O
that	O
our	O
LSTM	O
-	O
RNN	O
based	O
model	O
compares	O
favorably	O
to	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
CNN	O
based	O
model	O
(	O
in	O
F1-score	O
)	O
on	O
nominal	O
relation	O
classification	O
(	O
Sem	O
Eval	O
-	O
2010	O
Task	O
8	O
)	O
.	O
Finally	O
,	O
we	O
present	O
an	O
extensive	O
ablation	O
analysis	O
of	O
several	O
model	O
components	O
.	O

State	O
-	O
of	O
-	O
the	O
-	O
art	O
models	O
for	O
joint	O
entity	O
recognition	O
and	O
relation	O
extraction	O
strongly	O
rely	O
on	O
external	O
natural	O
language	O
processing	O
(	O
NLP	O
)	O
tools	O
such	O
as	O
POS	O
(	O
part	O
-	O
of	O
-	O
speech	O
)	O
taggers	O
and	O
dependency	O
parsers	O
.	O
Thus	O
,	O
the	O
performance	O
of	O
such	O
joint	O
models	O
depends	O
on	O
the	O
quality	O
of	O
the	O
features	O
obtained	O
from	O
these	O
NLP	O
tools	O
.	O
However	O
,	O
these	O
features	O
are	O
not	O
always	O
accurate	O
for	O
various	O
languages	O
and	O
contexts	O
.	O
In	O
this	O
paper	O
,	O
we	O
propose	O
a	O
joint	O
neural	O
model	O
which	O
performs	O
entity	B-RESEARCH_PROBLEM
recognition	I-RESEARCH_PROBLEM
and	I-RESEARCH_PROBLEM
relation	I-RESEARCH_PROBLEM
extraction	I-RESEARCH_PROBLEM
simultaneously	E-RESEARCH_PROBLEM
,	O
without	O
the	O
need	O
of	O
any	O
manually	O
extracted	O
features	O
or	O
the	O
use	O
of	O
any	O
external	O
tool	O
.	O
Specifically	O
,	O
we	O
model	O
the	O
entity	O
recognition	O
task	O
using	O
a	O
CRF	O
(	O
Conditional	O
Random	O
Fields	O
)	O
layer	O
and	O
the	O
relation	O
extraction	O
task	O
as	O
a	O
multi-head	O
selection	O
problem	O
(	O
i.e.	O
,	O
potentially	O
identify	O
multiple	O
relations	O
for	O
each	O
entity	O
)	O
.	O
We	O
present	O
an	O
extensive	O
experimental	O
setup	O
,	O
to	O
demonstrate	O
the	O
effectiveness	O
of	O
our	O
method	O
using	O
datasets	O
from	O
various	O
contexts	O
(	O
i.e.	O
,	O
news	O
,	O
biomedical	O
,	O
real	O
estate	O
)	O
and	O
languages	O
(	O
i.e.	O
,	O
English	O
,	O
Dutch	O
)	O
.	O
Our	O
model	O
outperforms	O
the	O
previous	O
neural	O
models	O
that	O
use	O
automatically	O
extracted	O
features	O
,	O
while	O
it	O
performs	O
within	O
a	O
reasonable	O
margin	O
of	O
feature	O
-	O
based	O
neural	O
models	O
,	O
or	O
even	O
beats	O
them	O
.	O

Adversarial	O
training	O
(	O
AT	O
)	O
is	O
a	O
regularization	O
method	O
that	O
can	O
be	O
used	O
to	O
improve	O
the	O
robustness	O
of	O
neural	O
network	O
methods	O
by	O
adding	O
small	O
perturbations	O
in	O
the	O
training	O
data	O
.	O
We	O
show	O
how	O
to	O
use	O
AT	O
for	O
the	O
tasks	O
of	O
entity	B-RESEARCH_PROBLEM
recognition	I-RESEARCH_PROBLEM
and	I-RESEARCH_PROBLEM
relation	I-RESEARCH_PROBLEM
extraction	E-RESEARCH_PROBLEM
.	O
In	O
particular	O
,	O
we	O
demonstrate	O
that	O
applying	O
AT	O
to	O
a	O
general	O
purpose	O
baseline	O
model	O
for	O
jointly	B-RESEARCH_PROBLEM
extracting	I-RESEARCH_PROBLEM
entities	I-RESEARCH_PROBLEM
and	I-RESEARCH_PROBLEM
relations	E-RESEARCH_PROBLEM
,	O
allows	O
improving	O
the	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
effectiveness	O
on	O
several	O
datasets	O
in	O
different	O
contexts	O
(	O
i.e.	O
,	O
news	O
,	O
biomedical	O
,	O
and	O
real	O
estate	O
data	O
)	O
and	O
for	O
different	O
languages	O
(	O
English	O
and	O
Dutch	O
)	O
.	O

Dependency	O
trees	O
help	O
relation	O
extraction	O
models	O
capture	B-RESEARCH_PROBLEM
long	I-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
range	I-RESEARCH_PROBLEM
relations	I-RESEARCH_PROBLEM
between	I-RESEARCH_PROBLEM
words	E-RESEARCH_PROBLEM
.	O
However	O
,	O
existing	O
dependency	O
-	O
based	O
models	O
either	O
neglect	O
crucial	O
information	O
(	O
e.g.	O
,	O
negation	O
)	O
by	O
pruning	O
the	O
dependency	O
trees	O
too	O
aggressively	O
,	O
or	O
are	O
computationally	O
inefficient	O
because	O
it	O
is	O
difficult	O
to	O
parallelize	O
over	O
different	O
tree	O
structures	O
.	O
We	O
propose	O
an	O
extension	O
of	O
graph	O
convolutional	O
networks	O
that	O
is	O
tailored	O
for	O
relation	O
extraction	O
,	O
which	O
pools	O
information	O
over	O
arbitrary	O
dependency	O
structures	O
efficiently	O
in	O
parallel	O
.	O
To	O
incorporate	O
relevant	O
information	O
while	O
maximally	O
removing	O
irrelevant	O
content	O
,	O
we	O
further	O
apply	O
a	O
novel	O
pruning	O
strategy	O
to	O
the	O
input	O
trees	O
by	O
keeping	O
words	O
immediately	O
around	O
the	O
shortest	O
path	O
between	O
the	O
two	O
entities	O
among	O
which	O
a	O
relation	O
might	O
hold	O
.	O
The	O
resulting	O
model	O
achieves	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
the	O
large	O
-	O
scale	O
TACRED	O
dataset	O
,	O
outperforming	O
existing	O
sequence	O
and	O
dependency	O
-	O
based	O
neural	O
models	O
.	O
We	O
also	O
show	O
through	O
detailed	O
analysis	O
that	O
this	O
model	O
has	O
complementary	O
strengths	O
to	O
sequence	O
models	O
,	O
and	O
combining	O
them	O
further	O
improves	O
the	O
state	O
of	O
the	O
art	O
.	O
*	O
Equal	O
contribution	O
.	O

We	O
propose	O
a	O
neural	O
network	O
model	O
for	O
joint	B-RESEARCH_PROBLEM
extraction	I-RESEARCH_PROBLEM
of	I-RESEARCH_PROBLEM
named	I-RESEARCH_PROBLEM
entities	I-RESEARCH_PROBLEM
and	I-RESEARCH_PROBLEM
relations	I-RESEARCH_PROBLEM
between	I-RESEARCH_PROBLEM
them	E-RESEARCH_PROBLEM
,	O
without	O
any	O
hand	O
-	O
crafted	O
features	O
.	O
The	O
key	O
contribution	O
of	O
our	O
model	O
is	O
to	O
extend	O
a	O
BiLSTM	O
-	O
CRF	O
-	O
based	O
entity	O
recognition	O
model	O
with	O
a	O
deep	O
biaffine	O
attention	O
layer	O
to	O
model	O
second	O
-	O
order	O
interactions	O
between	O
latent	O
features	O
for	O
relation	O
classification	O
,	O
specifically	O
attending	O
to	O
the	O
role	O
of	O
an	O
entity	O
in	O
a	O
directional	O
relationship	O
.	O
On	O
the	O
benchmark	O
"	O
relation	O
and	O
entity	O
recognition	O
"	O
dataset	O
CoNLL04	O
,	O
experimental	O
results	O
show	O
that	O
our	O
model	O
outperforms	O
previous	O
models	O
,	O
producing	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performances	O
.	O

Classifying	B-RESEARCH_PROBLEM
semantic	I-RESEARCH_PROBLEM
relations	I-RESEARCH_PROBLEM
between	I-RESEARCH_PROBLEM
entity	I-RESEARCH_PROBLEM
pairs	I-RESEARCH_PROBLEM
in	I-RESEARCH_PROBLEM
sentences	E-RESEARCH_PROBLEM
is	O
an	O
important	O
task	O
in	O
Natural	O
Language	O
Processing	O
(	O
NLP	O
)	O
.	O
Most	O
previous	O
models	O
for	O
relation	O
classification	O
rely	O
on	O
the	O
high	O
-	O
level	O
lexical	O
and	O
syntatic	O
features	O
obtained	O
by	O
NLP	O
tools	O
such	O
as	O
WordNet	O
,	O
dependency	O
parser	O
,	O
part	O
-	O
ofspeech	O
(	O
POS	O
)	O
tagger	O
,	O
and	O
named	O
entity	O
recognizers	O
(	O
NER	O
)	O
.	O
In	O
addition	O
,	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
neural	O
models	O
based	O
on	O
attention	O
mechanisms	O
do	O
not	O
fully	O
utilize	O
information	O
of	O
entity	O
that	O
maybe	O
the	O
most	O
crucial	O
features	O
for	O
relation	O
classification	O
.	O
To	O
address	O
these	O
issues	O
,	O
we	O
propose	O
a	O
novel	O
end	O
-	O
to	O
-	O
end	O
recurrent	O
neural	O
model	O
which	O
incorporates	O
an	O
entity	O
-	O
aware	O
attention	O
mechanism	O
with	O
a	O
latent	O
entity	O
typing	O
(	O
LET	O
)	O
method	O
.	O
Our	O
model	O
not	O
only	O
utilizes	O
entities	O
and	O
their	O
latent	O
types	O
as	O
features	O
effectively	O
but	O
also	O
is	O
more	O
interpretable	O
by	O
visualizing	O
attention	O
mechanisms	O
applied	O
to	O
our	O
model	O
and	O
results	O
of	O
LET	O
.	O
Experimental	O
results	O
on	O
the	O
SemEval	O
-	O
2010	O
Task	O
8	O
,	O
one	O
of	O
the	O
most	O
popular	O
relation	O
classification	O
task	O
,	O
demonstrate	O
that	O
our	O
model	O
outperforms	O
existing	O
state	O
-	O
of	O
the	O
-	O
art	O
models	O
without	O
any	O
high	O
-	O
level	O
features	O
.	O

Motivation	O
:	O
Biomedical	O
text	O
mining	O
is	O
becoming	O
increasingly	O
important	O
as	O
the	O
number	O
of	O
biomedical	O
documents	O
rapidly	O
grows	O
.	O
With	O
the	O
progress	O
in	O
natural	O
language	O
processing	O
(	O
NLP	O
)	O
,	O
extracting	B-RESEARCH_PROBLEM
valuable	I-RESEARCH_PROBLEM
information	I-RESEARCH_PROBLEM
from	I-RESEARCH_PROBLEM
biomedical	I-RESEARCH_PROBLEM
literature	E-RESEARCH_PROBLEM
has	O
gained	O
popularity	O
among	O
researchers	O
,	O
and	O
deep	O
learning	O
has	O
boosted	O
the	O
development	O
of	O
effective	O
biomedical	O
text	O
mining	O
models	O
.	O
However	O
,	O
directly	O
applying	O
the	O
advancements	O
in	O
NLP	O
to	O
biomedical	B-RESEARCH_PROBLEM
text	I-RESEARCH_PROBLEM
mining	I-RESEARCH_PROBLEM
often	I-RESEARCH_PROBLEM
yields	I-RESEARCH_PROBLEM
unsatisfactory	I-RESEARCH_PROBLEM
results	I-RESEARCH_PROBLEM
due	I-RESEARCH_PROBLEM
to	I-RESEARCH_PROBLEM
a	I-RESEARCH_PROBLEM
word	I-RESEARCH_PROBLEM
distribution	I-RESEARCH_PROBLEM
shift	I-RESEARCH_PROBLEM
from	I-RESEARCH_PROBLEM
general	I-RESEARCH_PROBLEM
domain	I-RESEARCH_PROBLEM
corpora	I-RESEARCH_PROBLEM
to	I-RESEARCH_PROBLEM
biomedical	I-RESEARCH_PROBLEM
corpora	E-RESEARCH_PROBLEM
.	O
In	O
this	O
article	O
,	O
we	O
investigate	O
how	O
the	O
recently	O
introduced	O
pre-trained	B-RESEARCH_PROBLEM
language	I-RESEARCH_PROBLEM
model	I-RESEARCH_PROBLEM
BERT	I-RESEARCH_PROBLEM
can	I-RESEARCH_PROBLEM
be	I-RESEARCH_PROBLEM
adapted	I-RESEARCH_PROBLEM
for	I-RESEARCH_PROBLEM
biomedical	I-RESEARCH_PROBLEM
corpora	E-RESEARCH_PROBLEM
.	O
Results	O
:	O
We	O
introduce	O
BioBERT	O
(	O
Bidirectional	O
Encoder	O
Representations	O
from	O
Transformers	O
for	O
Biomedical	O
Text	O
Mining	O
)	O
,	O
which	O
is	O
a	O
domain	O
-	O
specific	O
language	O
representation	O
model	O
pre-trained	O
on	O
large	O
-	O
scale	O
biomedical	O
corpora	O
.	O

The	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
solutions	O
for	O
extracting	B-RESEARCH_PROBLEM
multiple	I-RESEARCH_PROBLEM
entity	I-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
relations	I-RESEARCH_PROBLEM
from	I-RESEARCH_PROBLEM
an	I-RESEARCH_PROBLEM
input	I-RESEARCH_PROBLEM
paragraph	E-RESEARCH_PROBLEM
always	O
require	O
a	O
multiple	O
-	O
pass	O
encoding	O
on	O
the	O
input	O
.	O
This	O
paper	O
proposes	O
a	O
new	O
solution	O
that	O
can	O
complete	O
the	O
multiple	B-RESEARCH_PROBLEM
entityrelations	I-RESEARCH_PROBLEM
extraction	I-RESEARCH_PROBLEM
task	I-RESEARCH_PROBLEM
with	I-RESEARCH_PROBLEM
only	I-RESEARCH_PROBLEM
one	I-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
pass	E-RESEARCH_PROBLEM
encoding	O
on	O
the	O
input	O
corpus	O
,	O
and	O
achieve	O
a	O
new	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
accuracy	O
performance	O
,	O
as	O
demonstrated	O
in	O
the	O
ACE	O
2005	O
benchmark	O
.	O
Our	O
solution	O
is	O
built	O
on	O
top	O
of	O
the	O
pre-trained	O
self	O
-	O
attentive	O
models	O
(	O
Transformer	O
)	O
.	O
Since	O
our	O
method	O
uses	O
a	O
single	O
-	O
pass	O
to	O
compute	O
all	O
relations	O
at	O
once	O
,	O
it	O
scales	O
to	O
larger	O
datasets	O
easily	O
;	O
which	O
makes	O
it	O
more	O
usable	O
in	O
real	O
-	O
world	O
applications	O
.	O
1	O

Obtaining	B-RESEARCH_PROBLEM
large	I-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
scale	I-RESEARCH_PROBLEM
annotated	I-RESEARCH_PROBLEM
data	I-RESEARCH_PROBLEM
for	I-RESEARCH_PROBLEM
NLP	I-RESEARCH_PROBLEM
tasks	I-RESEARCH_PROBLEM
in	I-RESEARCH_PROBLEM
the	I-RESEARCH_PROBLEM
scientific	I-RESEARCH_PROBLEM
domain	E-RESEARCH_PROBLEM
is	O
challenging	O
and	O
expensive	O
.	O
We	O
release	O
SCIBERT	O
,	O
a	O
pretrained	O
language	O
model	O
based	O
on	O
BERT	O
(	O
Devlin	O
et	O
al.	O
,	O
2019	O
)	O
to	O
address	B-RESEARCH_PROBLEM
the	I-RESEARCH_PROBLEM
lack	I-RESEARCH_PROBLEM
of	I-RESEARCH_PROBLEM
high	I-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
quality	I-RESEARCH_PROBLEM
,	I-RESEARCH_PROBLEM
large	I-RESEARCH_PROBLEM
-	I-RESEARCH_PROBLEM
scale	I-RESEARCH_PROBLEM
labeled	I-RESEARCH_PROBLEM
scientific	I-RESEARCH_PROBLEM
data	E-RESEARCH_PROBLEM
.	O
SCIBERT	O
leverages	O
unsupervised	O
pretraining	O
on	O
a	O
large	O
multi-domain	O
corpus	O
of	O
scientific	O
publications	O
to	O
improve	O
performance	O
on	O
downstream	O
scientific	O
NLP	O
tasks	O
.	O
We	O
evaluate	O
on	O
a	O
suite	O
of	O
tasks	O
including	O
sequence	O
tagging	O
,	O
sentence	O
classification	O
and	O
dependency	O
parsing	O
,	O
with	O
datasets	O
from	O
a	O
variety	O
of	O
scientific	O
domains	O
.	O
We	O
demonstrate	O
statistically	O
significant	O
improvements	O
over	O
BERT	O
and	O
achieve	O
new	O
state	O
-	O
of	O
-	O
theart	O
results	O
on	O
several	O
of	O
these	O
tasks	O
.	O
The	O
code	O
and	O
pretrained	O
models	O
are	O
available	O
at	O
https://github.com/allenai/scibert/.	O

One	O
-	O
hot	O
CNN	O
(	O
convolutional	O
neural	O
network	O
)	O
has	O
been	O
shown	O
to	O
be	O
effective	O
for	O
text	B-RESEARCH_PROBLEM
categorization	E-RESEARCH_PROBLEM
(	O
Johnson	O
&	O
Zhang	O
,	O
2015a	O
;	O
b	O
)	O
.	O
We	O
view	O
it	O
as	O
a	O
special	O
case	O
of	O
a	O
general	O
framework	O
which	O
jointly	O
trains	O
a	O
linear	O
model	O
with	O
a	O
non-linear	O
feature	O
generator	O
consisting	O
of	O
'	O
text	O
region	O
embedding	O
+	O
pooling	O
'	O
.	O
Under	O
this	O
framework	O
,	O
we	O
explore	O
a	O
more	O
sophisticated	O
region	O
embedding	O
method	O
using	O
Long	O
Short	O
-	O
Term	O
Memory	O
(	O
LSTM	O
)	O
.	O
LSTM	O
can	O
embed	O
text	O
regions	O
of	O
variable	O
(	O
and	O
possibly	O
large	O
)	O
sizes	O
,	O
whereas	O
the	O
region	O
size	O
needs	O
to	O
be	O
fixed	O
in	O
a	O
CNN	O
.	O
We	O
seek	O
effective	O
and	O
efficient	O
use	O
of	O
LSTM	O
for	O
this	O
purpose	O
in	O
the	O
supervised	O
and	O
semi-supervised	O
settings	O
.	O
The	O
best	O
results	O
were	O
obtained	O
by	O
combining	O
region	O
embeddings	O
in	O
the	O
form	O
of	O
LSTM	O
and	O
convolution	O
layers	O
trained	O
on	O
unlabeled	O
data	O
.	O
The	O
results	O
indicate	O
that	O
on	O
this	O
task	O
,	O
embeddings	O
of	O
text	O
regions	O
,	O
which	O
can	O
convey	O
complex	O
concepts	O
,	O
are	O
more	O
useful	O
than	O
embeddings	O
of	O
single	O
words	O
in	O
isolation	O
.	O

This	O
paper	O
explores	O
a	O
simple	O
and	O
efficient	O
baseline	O
for	O
text	B-RESEARCH_PROBLEM
classification	E-RESEARCH_PROBLEM
.	O
Our	O
experiments	O
show	O
that	O
our	O
fast	O
text	O
classifier	O
fastText	O
is	O
often	O
on	O
par	O
with	O
deep	O
learning	O
classifiers	O
in	O
terms	O
of	O
accuracy	O
,	O
and	O
many	O
orders	O
of	O
magnitude	O
faster	O
for	O
training	O
and	O
evaluation	O
.	O
We	O
can	O
train	O
fastText	O
on	O
more	O
than	O
one	O
billion	O
words	O
in	O
less	O
than	O
ten	O
minutes	O
using	O
a	O
standard	O
multicore	O
CPU	O
,	O
and	O
classify	O
half	O
a	O
million	O
sentences	O
among	O
312K	O
classes	O
in	O
less	O
than	O
a	O
minute	O
.	O

Text	B-RESEARCH_PROBLEM
preprocessing	E-RESEARCH_PROBLEM
is	O
often	O
the	O
first	O
step	O
in	O
the	O
pipeline	O
of	O
a	O
Natural	O
Language	O
Processing	O
(	O
NLP	O
)	O
system	O
,	O
with	O
potential	O
impact	O
in	O
its	O
final	O
performance	O
.	O
Despite	O
its	O
importance	O
,	O
text	O
preprocessing	O
has	O
not	O
received	O
much	O
attention	O
in	O
the	O
deep	O
learning	O
literature	O
.	O
In	O
this	O
paper	O
we	O
investigate	O
the	O
impact	O
of	O
simple	O
text	O
preprocessing	O
decisions	O
(	O
particularly	O
tokenizing	O
,	O
lemmatizing	O
,	O
lowercasing	O
and	O
multiword	O
grouping	O
)	O
on	O
the	O
performance	O
of	O
a	O
standard	O
neural	O
text	O
classifier	O
.	O
We	O
perform	O
an	O
extensive	O
evaluation	O
on	O
standard	O
benchmarks	O
from	O
text	O
categorization	O
and	O
sentiment	O
analysis	O
.	O
While	O
our	O
experiments	O
show	O
that	O
a	O
simple	O
tokenization	O
of	O
input	O
text	O
is	O
generally	O
adequate	O
,	O
they	O
also	O
highlight	O
significant	O
degrees	O
of	O
variability	O
across	O
preprocessing	O
techniques	O
.	O
This	O
reveals	O
the	O
importance	O
of	O
paying	O
attention	O
to	O
this	O
usually	O
-	O
overlooked	O
step	O
in	O
the	O
pipeline	O
,	O
particularly	O
when	O
comparing	O
different	O
models	O
.	O
Finally	O
,	O
our	O
evaluation	O
provides	O
insights	O
into	O
the	O
best	O
preprocessing	O
practices	O
for	O
training	O
word	O
embeddings	O
.	O

Convolutional	O
neural	O
networks	O
(	O
CNNs	O
)	O
have	O
recently	O
emerged	O
as	O
a	O
popular	O
building	O
block	O
for	O
natural	O
language	O
processing	O
(	O
NLP	O
)	O
.	O
Despite	O
their	O
success	O
,	O
most	O
existing	O
CNN	O
models	O
employed	O
in	O
NLP	O
share	O
the	O
same	O
learned	O
(	O
and	O
static	O
)	O
set	O
of	O
filters	O
for	O
all	O
input	O
sentences	O
.	O
In	O
this	O
paper	O
,	O
we	O
consider	O
an	O
approach	O
of	O
using	O
a	O
small	O
meta	O
network	O
to	O
learn	B-RESEARCH_PROBLEM
contextsensitive	I-RESEARCH_PROBLEM
convolutional	I-RESEARCH_PROBLEM
filters	I-RESEARCH_PROBLEM
for	I-RESEARCH_PROBLEM
text	I-RESEARCH_PROBLEM
processing	E-RESEARCH_PROBLEM
.	O
The	O
role	O
of	O
meta	O
network	O
is	O
to	O
abstract	O
the	O
contextual	O
information	O
of	O
a	O
sentence	O
or	O
document	O
into	O
a	O
set	O
of	O
input	O
-aware	O
filters	O
.	O
We	O
further	O
generalize	O
this	O
framework	O
to	O
model	O
sentence	O
pairs	O
,	O
where	O
a	O
bidirectional	O
filter	O
generation	O
mechanism	O
is	O
introduced	O
to	O
encapsulate	O
co-dependent	O
sentence	O
representations	O
.	O
In	O
our	O
benchmarks	O
on	O
four	O
different	O
tasks	O
,	O
including	O
ontology	O
classification	O
,	O
sentiment	O
analysis	O
,	O
answer	O
sentence	O
selection	O
,	O
and	O
paraphrase	O
identification	O
,	O
our	O
proposed	O
model	O
,	O
a	O
modified	O
CNN	O
with	O
context	O
-	O
sensitive	O
filters	O
,	O
consistently	O
outperforms	O
the	O
standard	O
CNN	O
and	O
attention	O
-	O
based	O
CNN	O
baselines	O
.	O
By	O
visualizing	O
the	O
learned	O
context	O
-	O
sensitive	O
filters	O
,	O
we	O
further	O
validate	O
and	O
rationalize	O
the	O
effectiveness	O
of	O
proposed	O
framework	O
.	O

We	O
present	O
models	O
for	O
encoding	B-RESEARCH_PROBLEM
sentences	I-RESEARCH_PROBLEM
into	I-RESEARCH_PROBLEM
embedding	I-RESEARCH_PROBLEM
vectors	I-RESEARCH_PROBLEM
that	I-RESEARCH_PROBLEM
specifically	I-RESEARCH_PROBLEM
target	I-RESEARCH_PROBLEM
transfer	I-RESEARCH_PROBLEM
learning	E-RESEARCH_PROBLEM
to	O
other	O
NLP	O
tasks	O
.	O
The	O
models	O
are	O
efficient	O
and	O
result	O
in	O
accurate	O
performance	O
on	O
diverse	O
transfer	O
tasks	O
.	O
Two	O
variants	O
of	O
the	O
encoding	O
models	O
allow	O
for	O
trade	O
-	O
offs	O
between	O
accuracy	O
and	O
compute	O
resources	O
.	O
For	O
both	O
variants	O
,	O
we	O
investigate	O
and	O
report	O
the	O
relationship	O
between	O
model	O
complexity	O
,	O
resource	O
consumption	O
,	O
the	O
availability	O
of	O
transfer	O
task	O
training	O
data	O
,	O
and	O
task	O
performance	O
.	O
Comparisons	O
are	O
made	O
with	O
baselines	O
that	O
use	O
word	O
level	O
transfer	O
learning	O
via	O
pretrained	O
word	O
embeddings	O
as	O
well	O
as	O
baselines	O
do	O
not	O
use	O
any	O
transfer	O
learning	O
.	O
We	O
find	O
that	O
transfer	B-RESEARCH_PROBLEM
learning	I-RESEARCH_PROBLEM
using	I-RESEARCH_PROBLEM
sentence	I-RESEARCH_PROBLEM
embeddings	E-RESEARCH_PROBLEM
tends	O
to	O
outperform	O
word	O
level	O
transfer	O
.	O
With	O
transfer	B-RESEARCH_PROBLEM
learning	I-RESEARCH_PROBLEM
via	I-RESEARCH_PROBLEM
sentence	I-RESEARCH_PROBLEM
embeddings	E-RESEARCH_PROBLEM
,	O
we	O
observe	O
surprisingly	O
good	O
performance	O
with	O
minimal	O
amounts	O
of	O
supervised	O
training	O
data	O
for	O
a	O
transfer	O
task	O
.	O

Many	O
deep	O
learning	O
architectures	O
have	O
been	O
proposed	O
to	O
model	B-RESEARCH_PROBLEM
the	I-RESEARCH_PROBLEM
compositionality	I-RESEARCH_PROBLEM
in	I-RESEARCH_PROBLEM
text	I-RESEARCH_PROBLEM
sequences	E-RESEARCH_PROBLEM
,	O
requiring	O
a	O
substantial	O
number	O
of	O
parameters	O
and	O
expensive	O
computations	O
.	O
However	O
,	O
there	O
has	O
not	O
been	O
a	O
rigorous	O
evaluation	O
regarding	O
the	O
added	O
value	O
of	O
sophisticated	O
compositional	O
functions	O
.	O
In	O
this	O
paper	O
,	O
we	O
conduct	O
a	O
point	O
-	O
by	O
-	O
point	O
comparative	O
study	O
between	O
Simple	O
Word	O
-	O
Embeddingbased	O
Models	O
(	O
SWEMs	O
)	O
,	O
consisting	O
of	O
parameter	O
-	O
free	O
pooling	O
operations	O
,	O
relative	O
to	O
word	O
-	O
embedding	O
-	O
based	O
RNN	O
/	O
CNN	O
models	O
.	O
Surprisingly	O
,	O
SWEMs	O
exhibit	O
comparable	O
or	O
even	O
superior	O
performance	O
in	O
the	O
majority	O
of	O
cases	O
considered	O
.	O
Based	O
upon	O
this	O
understanding	O
,	O
we	O
propose	O
two	O
additional	O
pooling	O
strategies	O
over	O
learned	O
word	O
embeddings	O
:	O
(	O
i	O
)	O
a	O
max	O
-	O
pooling	O
operation	O
for	O
improved	O
interpretability	O
;	O
and	O
(	O
ii	O
)	O
a	O
hierarchical	O
pooling	O
operation	O
,	O
which	O
preserves	O
spatial	O
(	O
n	O
-	O
gram	O
)	O
information	O
within	O
text	O
sequences	O
.	O
We	O
present	O
experiments	O
on	O
17	O
datasets	O
encompassing	O
three	O
tasks	O
:	O
(	O
i	O
)	O
(	O
long	O
)	O
document	O
classification	O
;	O
(	O
ii	O
)	O
text	O
sequence	O
matching	O
;	O
and	O
(	O
iii	O
)	O
short	O
text	O
tasks	O
,	O
including	O
classification	O
and	O
tagging	O
.	O
The	O
source	O
code	O
and	O
datasets	O
can	O
be	O
obtained	O
from	O
https://github.com/dinghanshen/SWEM	O
.	O

